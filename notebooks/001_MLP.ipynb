{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdbf63e2-f450-4718-b1b7-4a529b5f2c74",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ee56e45-1184-4105-a917-22ce99ee58c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T07:21:31.412543Z",
     "iopub.status.busy": "2024-02-18T07:21:31.412382Z",
     "iopub.status.idle": "2024-02-18T07:21:31.417796Z",
     "shell.execute_reply": "2024-02-18T07:21:31.417110Z",
     "shell.execute_reply.started": "2024-02-18T07:21:31.412530Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cabb7217-b646-47e5-9830-ae33c71b457f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T07:21:31.418761Z",
     "iopub.status.busy": "2024-02-18T07:21:31.418573Z",
     "iopub.status.idle": "2024-02-18T07:21:33.959957Z",
     "shell.execute_reply": "2024-02-18T07:21:33.959465Z",
     "shell.execute_reply.started": "2024-02-18T07:21:31.418734Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 12:51:32.422068: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from scripts.constants import X_TRAIN\n",
    "from scripts.constants import X_TEST\n",
    "from scripts.constants import Y_TRAIN\n",
    "from scripts.constants import Y_TEST\n",
    "\n",
    "from scripts.data import scaffold_dataloaders\n",
    "\n",
    "from scripts.training import train_test_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3f061b9-1cc0-4491-a2e1-22ef301a2060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T07:21:33.960940Z",
     "iopub.status.busy": "2024-02-18T07:21:33.960647Z",
     "iopub.status.idle": "2024-02-18T07:21:34.084247Z",
     "shell.execute_reply": "2024-02-18T07:21:34.083238Z",
     "shell.execute_reply.started": "2024-02-18T07:21:33.960925Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = scaffold_dataloaders(X_TRAIN, X_TEST, Y_TRAIN, Y_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b95638f1-5767-4478-8962-96e49160466a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T07:21:34.087640Z",
     "iopub.status.busy": "2024-02-18T07:21:34.087307Z",
     "iopub.status.idle": "2024-02-18T07:21:34.105111Z",
     "shell.execute_reply": "2024-02-18T07:21:34.097978Z",
     "shell.execute_reply.started": "2024-02-18T07:21:34.087608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2051,  1.2021,  1.1673,  ...,  0.9636,  0.9794,  0.9781],\n",
      "        [ 1.0533,  1.0573,  1.0304,  ...,  0.7042,  0.7231,  0.7181],\n",
      "        [-0.0200, -0.0700, -0.1400,  ..., -0.3122, -0.2807, -0.3011],\n",
      "        ...,\n",
      "        [ 0.3804,  0.3328,  0.2615,  ...,  0.2941,  0.3447,  0.3615],\n",
      "        [-0.4490, -0.4413, -0.4667,  ..., -0.5019, -0.4541, -0.4507],\n",
      "        [-0.5691, -0.5921, -0.6341,  ..., -0.4179, -0.4033, -0.4432]])\n",
      "tensor([[3],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0],\n",
      "        [0]])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_dataloader))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d72c0cf8-8a8e-4023-97eb-e444a6abf458",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T07:21:34.107259Z",
     "iopub.status.busy": "2024-02-18T07:21:34.106592Z",
     "iopub.status.idle": "2024-02-18T07:21:34.112677Z",
     "shell.execute_reply": "2024-02-18T07:21:34.111548Z",
     "shell.execute_reply.started": "2024-02-18T07:21:34.107227Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.dense = nn.Linear(1440, 4)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae52b9f9-0069-4a45-bb37-f0f7608e9b86",
   "metadata": {},
   "source": [
    "## Using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba8e89c4-38c7-42d0-b5dc-690ab663f21a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T07:21:34.114511Z",
     "iopub.status.busy": "2024-02-18T07:21:34.114137Z",
     "iopub.status.idle": "2024-02-18T07:21:34.124244Z",
     "shell.execute_reply": "2024-02-18T07:21:34.123396Z",
     "shell.execute_reply.started": "2024-02-18T07:21:34.114477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (dense): Linear(in_features=1440, out_features=4, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16fc5880-70ff-4c01-93c0-53486ec32a30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T07:21:34.125586Z",
     "iopub.status.busy": "2024-02-18T07:21:34.125286Z",
     "iopub.status.idle": "2024-02-18T07:21:34.764355Z",
     "shell.execute_reply": "2024-02-18T07:21:34.763590Z",
     "shell.execute_reply.started": "2024-02-18T07:21:34.125564Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42ffc5ae-5a3f-408a-a66e-637d885595d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T07:21:34.765960Z",
     "iopub.status.busy": "2024-02-18T07:21:34.765334Z",
     "iopub.status.idle": "2024-02-18T07:21:34.771773Z",
     "shell.execute_reply": "2024-02-18T07:21:34.770915Z",
     "shell.execute_reply.started": "2024-02-18T07:21:34.765914Z"
    }
   },
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/mlp_sgd_{}'.format(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a5514f7-5c41-4789-b39c-d4a36f83aef0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T07:21:34.773309Z",
     "iopub.status.busy": "2024-02-18T07:21:34.772728Z",
     "iopub.status.idle": "2024-02-18T07:21:56.946456Z",
     "shell.execute_reply": "2024-02-18T07:21:56.945880Z",
     "shell.execute_reply.started": "2024-02-18T07:21:34.773277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0:\n",
      "  batch 1 loss: 1.3572397232055664\n",
      "  batch 2 loss: 1.438297986984253\n",
      "  batch 3 loss: 1.3376415967941284\n",
      "  batch 4 loss: 1.4353207349777222\n",
      "  batch 5 loss: 1.3565753698349\n",
      "  batch 6 loss: 1.4245274066925049\n",
      "  batch 7 loss: 1.4130030870437622\n",
      "  batch 8 loss: 1.40128493309021\n",
      "  batch 9 loss: 1.427922248840332\n",
      "  batch 10 loss: 1.4664177894592285\n",
      "  batch 11 loss: 1.4026010036468506\n",
      "  batch 12 loss: 1.3651753664016724\n",
      "  batch 13 loss: 1.3559849262237549\n",
      "  batch 14 loss: 1.404484748840332\n",
      "  batch 15 loss: 1.415113925933838\n",
      "LOSS train 1.415113925933838 valid 1.371384620666504\n",
      "ACCURACY train: 0.375 valid 0.5\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 1.349901556968689\n",
      "  batch 2 loss: 1.405714988708496\n",
      "  batch 3 loss: 1.3836859464645386\n",
      "  batch 4 loss: 1.297317385673523\n",
      "  batch 5 loss: 1.3329689502716064\n",
      "  batch 6 loss: 1.4291081428527832\n",
      "  batch 7 loss: 1.3856496810913086\n",
      "  batch 8 loss: 1.3550256490707397\n",
      "  batch 9 loss: 1.3006168603897095\n",
      "  batch 10 loss: 1.3847801685333252\n",
      "  batch 11 loss: 1.2984025478363037\n",
      "  batch 12 loss: 1.4464178085327148\n",
      "  batch 13 loss: 1.3598964214324951\n",
      "  batch 14 loss: 1.4248162508010864\n",
      "  batch 15 loss: 1.4475336074829102\n",
      "LOSS train 1.4475336074829102 valid 1.3783700466156006\n",
      "ACCURACY train: 0.125 valid 0.0\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.290039300918579\n",
      "  batch 2 loss: 1.3771967887878418\n",
      "  batch 3 loss: 1.297295331954956\n",
      "  batch 4 loss: 1.4168708324432373\n",
      "  batch 5 loss: 1.406451940536499\n",
      "  batch 6 loss: 1.2825231552124023\n",
      "  batch 7 loss: 1.2337239980697632\n",
      "  batch 8 loss: 1.3140946626663208\n",
      "  batch 9 loss: 1.4003686904907227\n",
      "  batch 10 loss: 1.2478749752044678\n",
      "  batch 11 loss: 1.4292452335357666\n",
      "  batch 12 loss: 1.2320071458816528\n",
      "  batch 13 loss: 1.328842043876648\n",
      "  batch 14 loss: 1.436688780784607\n",
      "  batch 15 loss: 1.390255331993103\n",
      "LOSS train 1.390255331993103 valid 1.3752115964889526\n",
      "ACCURACY train: 0.25 valid 0.0\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.2252848148345947\n",
      "  batch 2 loss: 1.3885748386383057\n",
      "  batch 3 loss: 1.500324010848999\n",
      "  batch 4 loss: 1.4111353158950806\n",
      "  batch 5 loss: 1.3586081266403198\n",
      "  batch 6 loss: 1.0630959272384644\n",
      "  batch 7 loss: 1.387139916419983\n",
      "  batch 8 loss: 1.3535338640213013\n",
      "  batch 9 loss: 1.298247218132019\n",
      "  batch 10 loss: 1.4192732572555542\n",
      "  batch 11 loss: 1.141923189163208\n",
      "  batch 12 loss: 1.3010811805725098\n",
      "  batch 13 loss: 1.1984608173370361\n",
      "  batch 14 loss: 1.3405429124832153\n",
      "  batch 15 loss: 1.3906673192977905\n",
      "LOSS train 1.3906673192977905 valid 1.3519388437271118\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.501103162765503\n",
      "  batch 2 loss: 1.2737159729003906\n",
      "  batch 3 loss: 1.106809377670288\n",
      "  batch 4 loss: 1.1850008964538574\n",
      "  batch 5 loss: 1.191299557685852\n",
      "  batch 6 loss: 1.3207653760910034\n",
      "  batch 7 loss: 1.347588062286377\n",
      "  batch 8 loss: 1.3317281007766724\n",
      "  batch 9 loss: 1.3849236965179443\n",
      "  batch 10 loss: 1.3160455226898193\n",
      "  batch 11 loss: 1.3259412050247192\n",
      "  batch 12 loss: 1.3409687280654907\n",
      "  batch 13 loss: 1.2137396335601807\n",
      "  batch 14 loss: 1.3056931495666504\n",
      "  batch 15 loss: 1.383940577507019\n",
      "LOSS train 1.383940577507019 valid 1.3395287990570068\n",
      "ACCURACY train: 0.125 valid 0.5\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.1993992328643799\n",
      "  batch 2 loss: 1.3450489044189453\n",
      "  batch 3 loss: 1.3858028650283813\n",
      "  batch 4 loss: 1.2738243341445923\n",
      "  batch 5 loss: 1.313011646270752\n",
      "  batch 6 loss: 1.2390210628509521\n",
      "  batch 7 loss: 1.3965308666229248\n",
      "  batch 8 loss: 1.2442479133605957\n",
      "  batch 9 loss: 1.3164769411087036\n",
      "  batch 10 loss: 1.3426936864852905\n",
      "  batch 11 loss: 1.3967498540878296\n",
      "  batch 12 loss: 1.3244317770004272\n",
      "  batch 13 loss: 1.2308584451675415\n",
      "  batch 14 loss: 1.1420778036117554\n",
      "  batch 15 loss: 1.1868301630020142\n",
      "LOSS train 1.1868301630020142 valid 1.3187404870986938\n",
      "ACCURACY train: 0.625 valid 0.0\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.2230489253997803\n",
      "  batch 2 loss: 1.1567342281341553\n",
      "  batch 3 loss: 1.380347490310669\n",
      "  batch 4 loss: 1.2138577699661255\n",
      "  batch 5 loss: 1.3304640054702759\n",
      "  batch 6 loss: 1.2203056812286377\n",
      "  batch 7 loss: 1.3530290126800537\n",
      "  batch 8 loss: 1.302573800086975\n",
      "  batch 9 loss: 1.2108125686645508\n",
      "  batch 10 loss: 1.3130931854248047\n",
      "  batch 11 loss: 1.2806925773620605\n",
      "  batch 12 loss: 1.2969539165496826\n",
      "  batch 13 loss: 1.3103760480880737\n",
      "  batch 14 loss: 1.3859220743179321\n",
      "  batch 15 loss: 1.1324182748794556\n",
      "LOSS train 1.1324182748794556 valid 1.313611626625061\n",
      "ACCURACY train: 0.375 valid 0.5\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.3236368894577026\n",
      "  batch 2 loss: 1.0324597358703613\n",
      "  batch 3 loss: 1.319287896156311\n",
      "  batch 4 loss: 1.296731948852539\n",
      "  batch 5 loss: 1.2877881526947021\n",
      "  batch 6 loss: 1.222907304763794\n",
      "  batch 7 loss: 1.1911072731018066\n",
      "  batch 8 loss: 1.1411798000335693\n",
      "  batch 9 loss: 1.2858973741531372\n",
      "  batch 10 loss: 1.399800181388855\n",
      "  batch 11 loss: 1.3286221027374268\n",
      "  batch 12 loss: 1.2667558193206787\n",
      "  batch 13 loss: 1.3071447610855103\n",
      "  batch 14 loss: 1.4173526763916016\n",
      "  batch 15 loss: 1.4787381887435913\n",
      "LOSS train 1.4787381887435913 valid 1.3120520114898682\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.3108155727386475\n",
      "  batch 2 loss: 1.1597925424575806\n",
      "  batch 3 loss: 1.4534598588943481\n",
      "  batch 4 loss: 1.3895525932312012\n",
      "  batch 5 loss: 1.378902554512024\n",
      "  batch 6 loss: 1.1931382417678833\n",
      "  batch 7 loss: 1.2015608549118042\n",
      "  batch 8 loss: 1.2058329582214355\n",
      "  batch 9 loss: 0.9689279198646545\n",
      "  batch 10 loss: 1.0855618715286255\n",
      "  batch 11 loss: 1.3627928495407104\n",
      "  batch 12 loss: 1.1307973861694336\n",
      "  batch 13 loss: 1.2963454723358154\n",
      "  batch 14 loss: 1.4678373336791992\n",
      "  batch 15 loss: 1.486702561378479\n",
      "LOSS train 1.486702561378479 valid 1.3115777969360352\n",
      "ACCURACY train: 0.5 valid 0.0\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 1.3453407287597656\n",
      "  batch 2 loss: 1.1719975471496582\n",
      "  batch 3 loss: 1.1764216423034668\n",
      "  batch 4 loss: 1.2491285800933838\n",
      "  batch 5 loss: 1.0183519124984741\n",
      "  batch 6 loss: 1.1880865097045898\n",
      "  batch 7 loss: 1.3065297603607178\n",
      "  batch 8 loss: 1.2667258977890015\n",
      "  batch 9 loss: 1.3222553730010986\n",
      "  batch 10 loss: 1.2376078367233276\n",
      "  batch 11 loss: 1.2742443084716797\n",
      "  batch 12 loss: 1.278712272644043\n",
      "  batch 13 loss: 1.3866021633148193\n",
      "  batch 14 loss: 1.433452844619751\n",
      "  batch 15 loss: 1.15632963180542\n",
      "LOSS train 1.15632963180542 valid 1.3136229515075684\n",
      "ACCURACY train: 0.625 valid 1.0\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.4070501327514648\n",
      "  batch 2 loss: 1.4364241361618042\n",
      "  batch 3 loss: 1.1821990013122559\n",
      "  batch 4 loss: 1.269484519958496\n",
      "  batch 5 loss: 1.117741584777832\n",
      "  batch 6 loss: 1.0501561164855957\n",
      "  batch 7 loss: 1.3910655975341797\n",
      "  batch 8 loss: 1.2219314575195312\n",
      "  batch 9 loss: 1.1987301111221313\n",
      "  batch 10 loss: 1.3802518844604492\n",
      "  batch 11 loss: 1.1678509712219238\n",
      "  batch 12 loss: 1.3733835220336914\n",
      "  batch 13 loss: 1.1238025426864624\n",
      "  batch 14 loss: 1.2892494201660156\n",
      "  batch 15 loss: 1.2678977251052856\n",
      "LOSS train 1.2678977251052856 valid 1.28288733959198\n",
      "ACCURACY train: 0.625 valid 0.0\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 1.2709994316101074\n",
      "  batch 2 loss: 1.130557894706726\n",
      "  batch 3 loss: 1.1812576055526733\n",
      "  batch 4 loss: 1.2030044794082642\n",
      "  batch 5 loss: 1.1052221059799194\n",
      "  batch 6 loss: 1.3069190979003906\n",
      "  batch 7 loss: 1.276308298110962\n",
      "  batch 8 loss: 1.2118773460388184\n",
      "  batch 9 loss: 1.4133708477020264\n",
      "  batch 10 loss: 1.2540733814239502\n",
      "  batch 11 loss: 1.1745469570159912\n",
      "  batch 12 loss: 1.4374144077301025\n",
      "  batch 13 loss: 1.3303824663162231\n",
      "  batch 14 loss: 1.2075716257095337\n",
      "  batch 15 loss: 0.9219823479652405\n",
      "LOSS train 0.9219823479652405 valid 1.3134630918502808\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 1.110990285873413\n",
      "  batch 2 loss: 1.3293559551239014\n",
      "  batch 3 loss: 1.2117706537246704\n",
      "  batch 4 loss: 1.570194959640503\n",
      "  batch 5 loss: 1.2056466341018677\n",
      "  batch 6 loss: 1.257709264755249\n",
      "  batch 7 loss: 1.344056487083435\n",
      "  batch 8 loss: 1.2854830026626587\n",
      "  batch 9 loss: 1.099013090133667\n",
      "  batch 10 loss: 1.1304023265838623\n",
      "  batch 11 loss: 1.2752282619476318\n",
      "  batch 12 loss: 1.1341503858566284\n",
      "  batch 13 loss: 1.258104681968689\n",
      "  batch 14 loss: 1.0740864276885986\n",
      "  batch 15 loss: 1.3905388116836548\n",
      "LOSS train 1.3905388116836548 valid 1.3178937435150146\n",
      "ACCURACY train: 0.625 valid 1.0\n",
      "EPOCH 13:\n",
      "  batch 1 loss: 1.4028780460357666\n",
      "  batch 2 loss: 1.2703408002853394\n",
      "  batch 3 loss: 1.1942729949951172\n",
      "  batch 4 loss: 1.1755168437957764\n",
      "  batch 5 loss: 1.1604773998260498\n",
      "  batch 6 loss: 1.303992509841919\n",
      "  batch 7 loss: 1.2717853784561157\n",
      "  batch 8 loss: 1.0433491468429565\n",
      "  batch 9 loss: 1.340219259262085\n",
      "  batch 10 loss: 1.068176031112671\n",
      "  batch 11 loss: 1.3329590559005737\n",
      "  batch 12 loss: 1.2292698621749878\n",
      "  batch 13 loss: 1.2400994300842285\n",
      "  batch 14 loss: 1.1530756950378418\n",
      "  batch 15 loss: 1.362026333808899\n",
      "LOSS train 1.362026333808899 valid 1.3268263339996338\n",
      "ACCURACY train: 0.375 valid 0.5\n",
      "EPOCH 14:\n",
      "  batch 1 loss: 1.1417824029922485\n",
      "  batch 2 loss: 1.134509801864624\n",
      "  batch 3 loss: 1.2690502405166626\n",
      "  batch 4 loss: 1.3767271041870117\n",
      "  batch 5 loss: 1.1397614479064941\n",
      "  batch 6 loss: 1.2930479049682617\n",
      "  batch 7 loss: 1.1648521423339844\n",
      "  batch 8 loss: 1.0466279983520508\n",
      "  batch 9 loss: 1.1262905597686768\n",
      "  batch 10 loss: 1.1364433765411377\n",
      "  batch 11 loss: 1.170332908630371\n",
      "  batch 12 loss: 1.3362677097320557\n",
      "  batch 13 loss: 1.249737024307251\n",
      "  batch 14 loss: 1.4802700281143188\n",
      "  batch 15 loss: 1.4154853820800781\n",
      "LOSS train 1.4154853820800781 valid 1.3034772872924805\n",
      "ACCURACY train: 0.625 valid 1.0\n",
      "EPOCH 15:\n",
      "  batch 1 loss: 1.1872774362564087\n",
      "  batch 2 loss: 1.1204124689102173\n",
      "  batch 3 loss: 1.0952351093292236\n",
      "  batch 4 loss: 1.1666789054870605\n",
      "  batch 5 loss: 1.2369332313537598\n",
      "  batch 6 loss: 1.2325282096862793\n",
      "  batch 7 loss: 1.2231656312942505\n",
      "  batch 8 loss: 1.3366752862930298\n",
      "  batch 9 loss: 1.2418044805526733\n",
      "  batch 10 loss: 1.2521922588348389\n",
      "  batch 11 loss: 1.2290151119232178\n",
      "  batch 12 loss: 1.3518927097320557\n",
      "  batch 13 loss: 1.1069244146347046\n",
      "  batch 14 loss: 1.284607172012329\n",
      "  batch 15 loss: 1.227129340171814\n",
      "LOSS train 1.227129340171814 valid 1.31073796749115\n",
      "ACCURACY train: nan valid 0.5\n",
      "EPOCH 16:\n",
      "  batch 1 loss: 1.2065629959106445\n",
      "  batch 2 loss: 1.170179009437561\n",
      "  batch 3 loss: 1.1013625860214233\n",
      "  batch 4 loss: 1.2277590036392212\n",
      "  batch 5 loss: 1.3542633056640625\n",
      "  batch 6 loss: 1.1405398845672607\n",
      "  batch 7 loss: 1.3161176443099976\n",
      "  batch 8 loss: 1.1465041637420654\n",
      "  batch 9 loss: 1.4354171752929688\n",
      "  batch 10 loss: 1.1117308139801025\n",
      "  batch 11 loss: 1.2389899492263794\n",
      "  batch 12 loss: 1.0456836223602295\n",
      "  batch 13 loss: 1.2509262561798096\n",
      "  batch 14 loss: 1.1732358932495117\n",
      "  batch 15 loss: 1.4310970306396484\n",
      "LOSS train 1.4310970306396484 valid 1.3118188381195068\n",
      "ACCURACY train: 0.625 valid 1.0\n",
      "EPOCH 17:\n",
      "  batch 1 loss: 1.2297844886779785\n",
      "  batch 2 loss: 1.238991141319275\n",
      "  batch 3 loss: 1.0809786319732666\n",
      "  batch 4 loss: 1.128447413444519\n",
      "  batch 5 loss: 1.3253991603851318\n",
      "  batch 6 loss: 1.2249313592910767\n",
      "  batch 7 loss: 1.2772932052612305\n",
      "  batch 8 loss: 1.1109566688537598\n",
      "  batch 9 loss: 1.234031081199646\n",
      "  batch 10 loss: 1.2388907670974731\n",
      "  batch 11 loss: 1.1933379173278809\n",
      "  batch 12 loss: 1.2235242128372192\n",
      "  batch 13 loss: 1.1786426305770874\n",
      "  batch 14 loss: 1.377985954284668\n",
      "  batch 15 loss: 0.9737817645072937\n",
      "LOSS train 0.9737817645072937 valid 1.2983819246292114\n",
      "ACCURACY train: 0.5 valid 0.5\n",
      "EPOCH 18:\n",
      "  batch 1 loss: 1.152806043624878\n",
      "  batch 2 loss: 1.1293797492980957\n",
      "  batch 3 loss: 1.390995740890503\n",
      "  batch 4 loss: 1.2701056003570557\n",
      "  batch 5 loss: 1.2484676837921143\n",
      "  batch 6 loss: 1.3263975381851196\n",
      "  batch 7 loss: 1.214417815208435\n",
      "  batch 8 loss: 1.3828818798065186\n",
      "  batch 9 loss: 1.3242026567459106\n",
      "  batch 10 loss: 1.0909501314163208\n",
      "  batch 11 loss: 1.0434448719024658\n",
      "  batch 12 loss: 1.0940240621566772\n",
      "  batch 13 loss: 1.2866885662078857\n",
      "  batch 14 loss: 1.1370211839675903\n",
      "  batch 15 loss: 0.9027299284934998\n",
      "LOSS train 0.9027299284934998 valid 1.3076740503311157\n",
      "ACCURACY train: 0.875 valid 0.0\n",
      "EPOCH 19:\n",
      "  batch 1 loss: 1.1609971523284912\n",
      "  batch 2 loss: 1.2312132120132446\n",
      "  batch 3 loss: 1.2418930530548096\n",
      "  batch 4 loss: 1.227188229560852\n",
      "  batch 5 loss: 1.3169105052947998\n",
      "  batch 6 loss: 1.1442649364471436\n",
      "  batch 7 loss: 1.148572564125061\n",
      "  batch 8 loss: 1.1408095359802246\n",
      "  batch 9 loss: 1.353242039680481\n",
      "  batch 10 loss: 1.0854865312576294\n",
      "  batch 11 loss: 1.0720319747924805\n",
      "  batch 12 loss: 1.095357894897461\n",
      "  batch 13 loss: 1.2916375398635864\n",
      "  batch 14 loss: 1.2975363731384277\n",
      "  batch 15 loss: 1.4826651811599731\n",
      "LOSS train 1.4826651811599731 valid 1.3298404216766357\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 20:\n",
      "  batch 1 loss: 1.0571260452270508\n",
      "  batch 2 loss: 1.3854295015335083\n",
      "  batch 3 loss: 1.239546775817871\n",
      "  batch 4 loss: 1.0923646688461304\n",
      "  batch 5 loss: 1.024726152420044\n",
      "  batch 6 loss: 1.2182371616363525\n",
      "  batch 7 loss: 1.2230570316314697\n",
      "  batch 8 loss: 1.182433843612671\n",
      "  batch 9 loss: 1.2500710487365723\n",
      "  batch 10 loss: 1.2502772808074951\n",
      "  batch 11 loss: 1.2924925088882446\n",
      "  batch 12 loss: 1.3884401321411133\n",
      "  batch 13 loss: 1.2133301496505737\n",
      "  batch 14 loss: 1.0102812051773071\n",
      "  batch 15 loss: 1.4523817300796509\n",
      "LOSS train 1.4523817300796509 valid 1.309891939163208\n",
      "ACCURACY train: 0.75 valid 0.0\n",
      "EPOCH 21:\n",
      "  batch 1 loss: 1.1881803274154663\n",
      "  batch 2 loss: 1.2916549444198608\n",
      "  batch 3 loss: 1.2391386032104492\n",
      "  batch 4 loss: 1.2056876420974731\n",
      "  batch 5 loss: 1.2900481224060059\n",
      "  batch 6 loss: 1.099434494972229\n",
      "  batch 7 loss: 1.0377328395843506\n",
      "  batch 8 loss: 1.0170507431030273\n",
      "  batch 9 loss: 1.4325716495513916\n",
      "  batch 10 loss: 1.2554291486740112\n",
      "  batch 11 loss: 1.2579127550125122\n",
      "  batch 12 loss: 1.0827491283416748\n",
      "  batch 13 loss: 1.194757103919983\n",
      "  batch 14 loss: 1.220088005065918\n",
      "  batch 15 loss: 1.1089168787002563\n",
      "LOSS train 1.1089168787002563 valid 1.3206853866577148\n",
      "ACCURACY train: 0.75 valid 0.0\n",
      "EPOCH 22:\n",
      "  batch 1 loss: 1.2408738136291504\n",
      "  batch 2 loss: 1.201975703239441\n",
      "  batch 3 loss: 1.1878855228424072\n",
      "  batch 4 loss: 1.1668647527694702\n",
      "  batch 5 loss: 1.0075408220291138\n",
      "  batch 6 loss: 1.2127927541732788\n",
      "  batch 7 loss: 1.2133651971817017\n",
      "  batch 8 loss: 1.4591084718704224\n",
      "  batch 9 loss: 1.1164041757583618\n",
      "  batch 10 loss: 0.9941939115524292\n",
      "  batch 11 loss: 1.321824312210083\n",
      "  batch 12 loss: 1.0572022199630737\n",
      "  batch 13 loss: 1.2309621572494507\n",
      "  batch 14 loss: 1.3430970907211304\n",
      "  batch 15 loss: 1.2933751344680786\n",
      "LOSS train 1.2933751344680786 valid 1.316713571548462\n",
      "ACCURACY train: 0.375 valid 0.0\n",
      "EPOCH 23:\n",
      "  batch 1 loss: 1.2966115474700928\n",
      "  batch 2 loss: 1.1622891426086426\n",
      "  batch 3 loss: 1.165114164352417\n",
      "  batch 4 loss: 0.9691546559333801\n",
      "  batch 5 loss: 1.104638695716858\n",
      "  batch 6 loss: 1.2326791286468506\n",
      "  batch 7 loss: 1.270477056503296\n",
      "  batch 8 loss: 1.19699227809906\n",
      "  batch 9 loss: 1.1685655117034912\n",
      "  batch 10 loss: 1.294217824935913\n",
      "  batch 11 loss: 1.362647294998169\n",
      "  batch 12 loss: 1.096083164215088\n",
      "  batch 13 loss: 1.2701551914215088\n",
      "  batch 14 loss: 1.251192331314087\n",
      "  batch 15 loss: 0.8727496266365051\n",
      "LOSS train 0.8727496266365051 valid 1.3024359941482544\n",
      "ACCURACY train: nan valid 1.0\n",
      "EPOCH 24:\n",
      "  batch 1 loss: 1.133766531944275\n",
      "  batch 2 loss: 1.1903715133666992\n",
      "  batch 3 loss: 1.115156888961792\n",
      "  batch 4 loss: 0.9926353693008423\n",
      "  batch 5 loss: 1.3102052211761475\n",
      "  batch 6 loss: 1.228715419769287\n",
      "  batch 7 loss: 1.1036376953125\n",
      "  batch 8 loss: 1.016711711883545\n",
      "  batch 9 loss: 1.5491224527359009\n",
      "  batch 10 loss: 1.069885015487671\n",
      "  batch 11 loss: 1.2244365215301514\n",
      "  batch 12 loss: 1.3243811130523682\n",
      "  batch 13 loss: 1.0723726749420166\n",
      "  batch 14 loss: 1.2694807052612305\n",
      "  batch 15 loss: 1.410656452178955\n",
      "LOSS train 1.410656452178955 valid 1.3148332834243774\n",
      "ACCURACY train: 0.625 valid 0.5\n",
      "EPOCH 25:\n",
      "  batch 1 loss: 1.1240085363388062\n",
      "  batch 2 loss: 1.2482616901397705\n",
      "  batch 3 loss: 1.340457558631897\n",
      "  batch 4 loss: 1.1086435317993164\n",
      "  batch 5 loss: 1.2612398862838745\n",
      "  batch 6 loss: 0.9741493463516235\n",
      "  batch 7 loss: 1.3615262508392334\n",
      "  batch 8 loss: 1.309003233909607\n",
      "  batch 9 loss: 1.068587064743042\n",
      "  batch 10 loss: 0.9853206276893616\n",
      "  batch 11 loss: 1.0979909896850586\n",
      "  batch 12 loss: 1.1235215663909912\n",
      "  batch 13 loss: 1.2620928287506104\n",
      "  batch 14 loss: 1.3764848709106445\n",
      "  batch 15 loss: 1.2310324907302856\n",
      "LOSS train 1.2310324907302856 valid 1.3171086311340332\n",
      "ACCURACY train: 0.875 valid 1.0\n",
      "EPOCH 26:\n",
      "  batch 1 loss: 1.4154881238937378\n",
      "  batch 2 loss: 1.084736704826355\n",
      "  batch 3 loss: 1.366542100906372\n",
      "  batch 4 loss: 1.0600923299789429\n",
      "  batch 5 loss: 1.0804619789123535\n",
      "  batch 6 loss: 1.10019850730896\n",
      "  batch 7 loss: 1.3559982776641846\n",
      "  batch 8 loss: 1.129088282585144\n",
      "  batch 9 loss: 1.0411347150802612\n",
      "  batch 10 loss: 1.2129261493682861\n",
      "  batch 11 loss: 1.231337308883667\n",
      "  batch 12 loss: 1.2261123657226562\n",
      "  batch 13 loss: 1.0833097696304321\n",
      "  batch 14 loss: 1.239295482635498\n",
      "  batch 15 loss: 1.4145952463150024\n",
      "LOSS train 1.4145952463150024 valid 1.3142114877700806\n",
      "ACCURACY train: 0.5 valid 0.0\n",
      "EPOCH 27:\n",
      "  batch 1 loss: 1.1706466674804688\n",
      "  batch 2 loss: 1.3727474212646484\n",
      "  batch 3 loss: 1.1221561431884766\n",
      "  batch 4 loss: 0.9961929321289062\n",
      "  batch 5 loss: 1.4727282524108887\n",
      "  batch 6 loss: 1.1835105419158936\n",
      "  batch 7 loss: 1.1456526517868042\n",
      "  batch 8 loss: 1.1507688760757446\n",
      "  batch 9 loss: 1.0474090576171875\n",
      "  batch 10 loss: 1.1115387678146362\n",
      "  batch 11 loss: 1.1316457986831665\n",
      "  batch 12 loss: 1.146608591079712\n",
      "  batch 13 loss: 1.5104870796203613\n",
      "  batch 14 loss: 1.1618647575378418\n",
      "  batch 15 loss: 1.103864073753357\n",
      "LOSS train 1.103864073753357 valid 1.3210716247558594\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 28:\n",
      "  batch 1 loss: 1.2159459590911865\n",
      "  batch 2 loss: 1.3421341180801392\n",
      "  batch 3 loss: 1.5384595394134521\n",
      "  batch 4 loss: 1.0599517822265625\n",
      "  batch 5 loss: 1.2663049697875977\n",
      "  batch 6 loss: 1.120687484741211\n",
      "  batch 7 loss: 1.2534847259521484\n",
      "  batch 8 loss: 1.0031450986862183\n",
      "  batch 9 loss: 1.236173391342163\n",
      "  batch 10 loss: 1.074453353881836\n",
      "  batch 11 loss: 1.0504684448242188\n",
      "  batch 12 loss: 1.0189591646194458\n",
      "  batch 13 loss: 1.1785553693771362\n",
      "  batch 14 loss: 1.0943527221679688\n",
      "  batch 15 loss: 1.4974974393844604\n",
      "LOSS train 1.4974974393844604 valid 1.3066719770431519\n",
      "ACCURACY train: 0.5 valid 0.5\n",
      "EPOCH 29:\n",
      "  batch 1 loss: 1.0808460712432861\n",
      "  batch 2 loss: 1.2940515279769897\n",
      "  batch 3 loss: 0.9941906332969666\n",
      "  batch 4 loss: 1.123030185699463\n",
      "  batch 5 loss: 1.3534709215164185\n",
      "  batch 6 loss: 1.046884536743164\n",
      "  batch 7 loss: 1.3914878368377686\n",
      "  batch 8 loss: 1.2757036685943604\n",
      "  batch 9 loss: 1.0625038146972656\n",
      "  batch 10 loss: 1.276785969734192\n",
      "  batch 11 loss: 1.20766282081604\n",
      "  batch 12 loss: 1.130036473274231\n",
      "  batch 13 loss: 1.1439435482025146\n",
      "  batch 14 loss: 1.169170618057251\n",
      "  batch 15 loss: 1.2428683042526245\n",
      "LOSS train 1.2428683042526245 valid 1.3267384767532349\n",
      "ACCURACY train: 0.75 valid 1.0\n",
      "EPOCH 30:\n",
      "  batch 1 loss: 1.1441147327423096\n",
      "  batch 2 loss: 1.1386890411376953\n",
      "  batch 3 loss: 1.3911150693893433\n",
      "  batch 4 loss: 1.2352184057235718\n",
      "  batch 5 loss: 1.023691177368164\n",
      "  batch 6 loss: 1.2912025451660156\n",
      "  batch 7 loss: 1.24591064453125\n",
      "  batch 8 loss: 1.2639214992523193\n",
      "  batch 9 loss: 1.1117726564407349\n",
      "  batch 10 loss: 1.1269891262054443\n",
      "  batch 11 loss: 1.2747536897659302\n",
      "  batch 12 loss: 1.0758823156356812\n",
      "  batch 13 loss: 1.1762109994888306\n",
      "  batch 14 loss: 1.0614614486694336\n",
      "  batch 15 loss: 1.0741859674453735\n",
      "LOSS train 1.0741859674453735 valid 1.314314603805542\n",
      "ACCURACY train: 0.5 valid 0.0\n",
      "EPOCH 31:\n",
      "  batch 1 loss: 1.0370724201202393\n",
      "  batch 2 loss: 1.0960458517074585\n",
      "  batch 3 loss: 1.103447437286377\n",
      "  batch 4 loss: 1.2152338027954102\n",
      "  batch 5 loss: 1.2507829666137695\n",
      "  batch 6 loss: 1.0114707946777344\n",
      "  batch 7 loss: 1.1506755352020264\n",
      "  batch 8 loss: 1.062648057937622\n",
      "  batch 9 loss: 1.2974718809127808\n",
      "  batch 10 loss: 1.4259860515594482\n",
      "  batch 11 loss: 1.2874330282211304\n",
      "  batch 12 loss: 1.2704651355743408\n",
      "  batch 13 loss: 1.3288137912750244\n",
      "  batch 14 loss: 1.1633703708648682\n",
      "  batch 15 loss: 0.9057657122612\n",
      "LOSS train 0.9057657122612 valid 1.3043427467346191\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 32:\n",
      "  batch 1 loss: 1.3052408695220947\n",
      "  batch 2 loss: 0.9722505807876587\n",
      "  batch 3 loss: 1.36661696434021\n",
      "  batch 4 loss: 1.0848400592803955\n",
      "  batch 5 loss: 1.1730140447616577\n",
      "  batch 6 loss: 1.212322473526001\n",
      "  batch 7 loss: 1.1121959686279297\n",
      "  batch 8 loss: 1.4588981866836548\n",
      "  batch 9 loss: 1.0984350442886353\n",
      "  batch 10 loss: 1.0838013887405396\n",
      "  batch 11 loss: 1.0489897727966309\n",
      "  batch 12 loss: 1.1579662561416626\n",
      "  batch 13 loss: 1.1475203037261963\n",
      "  batch 14 loss: 1.1827152967453003\n",
      "  batch 15 loss: 1.3474150896072388\n",
      "LOSS train 1.3474150896072388 valid 1.322340726852417\n",
      "ACCURACY train: 0.5 valid 1.0\n",
      "EPOCH 33:\n",
      "  batch 1 loss: 1.0439389944076538\n",
      "  batch 2 loss: 1.1796294450759888\n",
      "  batch 3 loss: 1.3068472146987915\n",
      "  batch 4 loss: 1.242831826210022\n",
      "  batch 5 loss: 1.129599690437317\n",
      "  batch 6 loss: 1.4017001390457153\n",
      "  batch 7 loss: 1.2589191198349\n",
      "  batch 8 loss: 1.104101300239563\n",
      "  batch 9 loss: 1.1779392957687378\n",
      "  batch 10 loss: 1.0878567695617676\n",
      "  batch 11 loss: 1.2687300443649292\n",
      "  batch 12 loss: 1.1259759664535522\n",
      "  batch 13 loss: 1.0394155979156494\n",
      "  batch 14 loss: 1.0976519584655762\n",
      "  batch 15 loss: 0.9967686533927917\n",
      "LOSS train 0.9967686533927917 valid 1.29708993434906\n",
      "ACCURACY train: 0.375 valid 0.5\n",
      "EPOCH 34:\n",
      "  batch 1 loss: 1.2965211868286133\n",
      "  batch 2 loss: 1.327711582183838\n",
      "  batch 3 loss: 0.9624625444412231\n",
      "  batch 4 loss: 1.0645989179611206\n",
      "  batch 5 loss: 1.0121421813964844\n",
      "  batch 6 loss: 1.1350151300430298\n",
      "  batch 7 loss: 1.3272758722305298\n",
      "  batch 8 loss: 0.9639111757278442\n",
      "  batch 9 loss: 1.0884528160095215\n",
      "  batch 10 loss: 1.3133130073547363\n",
      "  batch 11 loss: 1.1000292301177979\n",
      "  batch 12 loss: 1.2203760147094727\n",
      "  batch 13 loss: 1.2898513078689575\n",
      "  batch 14 loss: 1.1847896575927734\n",
      "  batch 15 loss: 1.4795507192611694\n",
      "LOSS train 1.4795507192611694 valid 1.3188214302062988\n",
      "ACCURACY train: 0.625 valid 1.0\n",
      "EPOCH 35:\n",
      "  batch 1 loss: 1.2583255767822266\n",
      "  batch 2 loss: 1.1165724992752075\n",
      "  batch 3 loss: 1.083097219467163\n",
      "  batch 4 loss: 1.3598172664642334\n",
      "  batch 5 loss: 1.0062828063964844\n",
      "  batch 6 loss: 1.2816110849380493\n",
      "  batch 7 loss: 1.1778348684310913\n",
      "  batch 8 loss: 1.0675559043884277\n",
      "  batch 9 loss: 1.15964937210083\n",
      "  batch 10 loss: 1.0293734073638916\n",
      "  batch 11 loss: 1.4441654682159424\n",
      "  batch 12 loss: 1.0825374126434326\n",
      "  batch 13 loss: 1.0669143199920654\n",
      "  batch 14 loss: 1.0722179412841797\n",
      "  batch 15 loss: 1.6643415689468384\n",
      "LOSS train 1.6643415689468384 valid 1.3262107372283936\n",
      "ACCURACY train: nan valid 0.5\n",
      "EPOCH 36:\n",
      "  batch 1 loss: 1.0382804870605469\n",
      "  batch 2 loss: 1.1113309860229492\n",
      "  batch 3 loss: 0.9927374720573425\n",
      "  batch 4 loss: 1.3930199146270752\n",
      "  batch 5 loss: 1.2944270372390747\n",
      "  batch 6 loss: 1.0412571430206299\n",
      "  batch 7 loss: 1.0873841047286987\n",
      "  batch 8 loss: 1.1914126873016357\n",
      "  batch 9 loss: 1.2707734107971191\n",
      "  batch 10 loss: 1.071207046508789\n",
      "  batch 11 loss: 1.120119571685791\n",
      "  batch 12 loss: 1.3095452785491943\n",
      "  batch 13 loss: 1.210240125656128\n",
      "  batch 14 loss: 1.275489091873169\n",
      "  batch 15 loss: 0.96775883436203\n",
      "LOSS train 0.96775883436203 valid 1.3221355676651\n",
      "ACCURACY train: 0.875 valid 1.0\n",
      "EPOCH 37:\n",
      "  batch 1 loss: 0.9394174814224243\n",
      "  batch 2 loss: 1.1464656591415405\n",
      "  batch 3 loss: 1.19078528881073\n",
      "  batch 4 loss: 1.2299200296401978\n",
      "  batch 5 loss: 1.3046245574951172\n",
      "  batch 6 loss: 1.1493606567382812\n",
      "  batch 7 loss: 1.1838281154632568\n",
      "  batch 8 loss: 1.2095245122909546\n",
      "  batch 9 loss: 1.1955721378326416\n",
      "  batch 10 loss: 1.095314860343933\n",
      "  batch 11 loss: 1.2030158042907715\n",
      "  batch 12 loss: 1.2548116445541382\n",
      "  batch 13 loss: 0.9894253611564636\n",
      "  batch 14 loss: 1.218613624572754\n",
      "  batch 15 loss: 1.1624432802200317\n",
      "LOSS train 1.1624432802200317 valid 1.3053303956985474\n",
      "ACCURACY train: 0.625 valid 0.5\n",
      "EPOCH 38:\n",
      "  batch 1 loss: 1.0003423690795898\n",
      "  batch 2 loss: 0.9951567053794861\n",
      "  batch 3 loss: 1.1567116975784302\n",
      "  batch 4 loss: 1.2705613374710083\n",
      "  batch 5 loss: 1.2564035654067993\n",
      "  batch 6 loss: 0.9601290225982666\n",
      "  batch 7 loss: 1.4081193208694458\n",
      "  batch 8 loss: 1.0584369897842407\n",
      "  batch 9 loss: 1.0400992631912231\n",
      "  batch 10 loss: 1.1518038511276245\n",
      "  batch 11 loss: 1.159009575843811\n",
      "  batch 12 loss: 1.3358094692230225\n",
      "  batch 13 loss: 1.1847267150878906\n",
      "  batch 14 loss: 1.2390758991241455\n",
      "  batch 15 loss: 1.426639437675476\n",
      "LOSS train 1.426639437675476 valid 1.3167530298233032\n",
      "ACCURACY train: 0.625 valid 0.0\n",
      "EPOCH 39:\n",
      "  batch 1 loss: 1.3019813299179077\n",
      "  batch 2 loss: 1.1170263290405273\n",
      "  batch 3 loss: 1.1282976865768433\n",
      "  batch 4 loss: 1.141890048980713\n",
      "  batch 5 loss: 1.0669705867767334\n",
      "  batch 6 loss: 1.0745950937271118\n",
      "  batch 7 loss: 1.1923441886901855\n",
      "  batch 8 loss: 1.2602306604385376\n",
      "  batch 9 loss: 1.1594364643096924\n",
      "  batch 10 loss: 1.0839701890945435\n",
      "  batch 11 loss: 1.2891231775283813\n",
      "  batch 12 loss: 1.0269062519073486\n",
      "  batch 13 loss: 1.3031913042068481\n",
      "  batch 14 loss: 1.2585582733154297\n",
      "  batch 15 loss: 0.805462658405304\n",
      "LOSS train 0.805462658405304 valid 1.3180347681045532\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 40:\n",
      "  batch 1 loss: 0.9696438908576965\n",
      "  batch 2 loss: 1.1505764722824097\n",
      "  batch 3 loss: 1.2674496173858643\n",
      "  batch 4 loss: 1.425140619277954\n",
      "  batch 5 loss: 1.0640208721160889\n",
      "  batch 6 loss: 1.2801021337509155\n",
      "  batch 7 loss: 1.3238533735275269\n",
      "  batch 8 loss: 0.9029638171195984\n",
      "  batch 9 loss: 1.205904245376587\n",
      "  batch 10 loss: 1.2581617832183838\n",
      "  batch 11 loss: 1.2052714824676514\n",
      "  batch 12 loss: 1.0407721996307373\n",
      "  batch 13 loss: 1.0808844566345215\n",
      "  batch 14 loss: 1.0347607135772705\n",
      "  batch 15 loss: 1.2033082246780396\n",
      "LOSS train 1.2033082246780396 valid 1.3123198747634888\n",
      "ACCURACY train: 0.875 valid 0.0\n",
      "EPOCH 41:\n",
      "  batch 1 loss: 1.052616000175476\n",
      "  batch 2 loss: 1.413920521736145\n",
      "  batch 3 loss: 1.0270922183990479\n",
      "  batch 4 loss: 1.1451375484466553\n",
      "  batch 5 loss: 1.3601170778274536\n",
      "  batch 6 loss: 0.9960662126541138\n",
      "  batch 7 loss: 1.0471220016479492\n",
      "  batch 8 loss: 1.3388941287994385\n",
      "  batch 9 loss: 1.23685884475708\n",
      "  batch 10 loss: 1.2908129692077637\n",
      "  batch 11 loss: 1.0443885326385498\n",
      "  batch 12 loss: 1.0879168510437012\n",
      "  batch 13 loss: 1.2721574306488037\n",
      "  batch 14 loss: 1.091369867324829\n",
      "  batch 15 loss: 0.9329007267951965\n",
      "LOSS train 0.9329007267951965 valid 1.3193941116333008\n",
      "ACCURACY train: 0.875 valid 0.0\n",
      "EPOCH 42:\n",
      "  batch 1 loss: 1.4329180717468262\n",
      "  batch 2 loss: 1.0365711450576782\n",
      "  batch 3 loss: 1.1337218284606934\n",
      "  batch 4 loss: 1.190250277519226\n",
      "  batch 5 loss: 1.0498653650283813\n",
      "  batch 6 loss: 1.0405758619308472\n",
      "  batch 7 loss: 1.0891549587249756\n",
      "  batch 8 loss: 1.0938137769699097\n",
      "  batch 9 loss: 1.1041104793548584\n",
      "  batch 10 loss: 1.1316273212432861\n",
      "  batch 11 loss: 1.3563846349716187\n",
      "  batch 12 loss: 1.2028543949127197\n",
      "  batch 13 loss: 1.1848645210266113\n",
      "  batch 14 loss: 1.2584706544876099\n",
      "  batch 15 loss: 0.990659236907959\n",
      "LOSS train 0.990659236907959 valid 1.316838264465332\n",
      "ACCURACY train: 0.375 valid 0.0\n",
      "EPOCH 43:\n",
      "  batch 1 loss: 1.2377619743347168\n",
      "  batch 2 loss: 1.153172254562378\n",
      "  batch 3 loss: 1.3386168479919434\n",
      "  batch 4 loss: 1.125632643699646\n",
      "  batch 5 loss: 0.985607385635376\n",
      "  batch 6 loss: 1.334027886390686\n",
      "  batch 7 loss: 0.9943669438362122\n",
      "  batch 8 loss: 1.2677363157272339\n",
      "  batch 9 loss: 1.1941386461257935\n",
      "  batch 10 loss: 0.9280300140380859\n",
      "  batch 11 loss: 1.1850712299346924\n",
      "  batch 12 loss: 1.3093894720077515\n",
      "  batch 13 loss: 1.075350046157837\n",
      "  batch 14 loss: 1.0888502597808838\n",
      "  batch 15 loss: 1.0935957431793213\n",
      "LOSS train 1.0935957431793213 valid 1.3256781101226807\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 44:\n",
      "  batch 1 loss: 1.2772297859191895\n",
      "  batch 2 loss: 1.3698501586914062\n",
      "  batch 3 loss: 1.2939414978027344\n",
      "  batch 4 loss: 1.1946468353271484\n",
      "  batch 5 loss: 1.092514991760254\n",
      "  batch 6 loss: 1.109788179397583\n",
      "  batch 7 loss: 1.0826866626739502\n",
      "  batch 8 loss: 1.1003086566925049\n",
      "  batch 9 loss: 1.230568528175354\n",
      "  batch 10 loss: 1.124367117881775\n",
      "  batch 11 loss: 1.0115563869476318\n",
      "  batch 12 loss: 0.9025641679763794\n",
      "  batch 13 loss: 1.277830719947815\n",
      "  batch 14 loss: 1.0733137130737305\n",
      "  batch 15 loss: 1.379759430885315\n",
      "LOSS train 1.379759430885315 valid 1.3082208633422852\n",
      "ACCURACY train: 0.5 valid 0.5\n",
      "EPOCH 45:\n",
      "  batch 1 loss: 1.1384810209274292\n",
      "  batch 2 loss: 1.1605474948883057\n",
      "  batch 3 loss: 1.2064937353134155\n",
      "  batch 4 loss: 1.234156847000122\n",
      "  batch 5 loss: 1.1108869314193726\n",
      "  batch 6 loss: 0.9941745400428772\n",
      "  batch 7 loss: 1.1384657621383667\n",
      "  batch 8 loss: 1.163907527923584\n",
      "  batch 9 loss: 1.0064245462417603\n",
      "  batch 10 loss: 1.2142271995544434\n",
      "  batch 11 loss: 1.1327217817306519\n",
      "  batch 12 loss: 1.0499709844589233\n",
      "  batch 13 loss: 1.3882654905319214\n",
      "  batch 14 loss: 1.1557596921920776\n",
      "  batch 15 loss: 1.210025668144226\n",
      "LOSS train 1.210025668144226 valid 1.325527310371399\n",
      "ACCURACY train: 0.875 valid 1.0\n",
      "EPOCH 46:\n",
      "  batch 1 loss: 1.257620096206665\n",
      "  batch 2 loss: 0.9825423359870911\n",
      "  batch 3 loss: 1.118119478225708\n",
      "  batch 4 loss: 1.1690278053283691\n",
      "  batch 5 loss: 1.2492694854736328\n",
      "  batch 6 loss: 1.1191669702529907\n",
      "  batch 7 loss: 1.1387524604797363\n",
      "  batch 8 loss: 0.9265062212944031\n",
      "  batch 9 loss: 1.2891592979431152\n",
      "  batch 10 loss: 1.2712153196334839\n",
      "  batch 11 loss: 1.286528468132019\n",
      "  batch 12 loss: 1.1572444438934326\n",
      "  batch 13 loss: 1.1752350330352783\n",
      "  batch 14 loss: 1.0790427923202515\n",
      "  batch 15 loss: 0.8134990334510803\n",
      "LOSS train 0.8134990334510803 valid 1.3172240257263184\n",
      "ACCURACY train: 0.5 valid 0.0\n",
      "EPOCH 47:\n",
      "  batch 1 loss: 1.131466269493103\n",
      "  batch 2 loss: 1.2455580234527588\n",
      "  batch 3 loss: 1.0364956855773926\n",
      "  batch 4 loss: 0.9274497032165527\n",
      "  batch 5 loss: 1.2422738075256348\n",
      "  batch 6 loss: 1.183932900428772\n",
      "  batch 7 loss: 1.1577541828155518\n",
      "  batch 8 loss: 1.2908318042755127\n",
      "  batch 9 loss: 1.231448769569397\n",
      "  batch 10 loss: 1.1572169065475464\n",
      "  batch 11 loss: 1.184557557106018\n",
      "  batch 12 loss: 1.2320075035095215\n",
      "  batch 13 loss: 1.2467105388641357\n",
      "  batch 14 loss: 0.9367493391036987\n",
      "  batch 15 loss: 0.9454554915428162\n",
      "LOSS train 0.9454554915428162 valid 1.313763976097107\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 48:\n",
      "  batch 1 loss: 1.299985408782959\n",
      "  batch 2 loss: 1.1944447755813599\n",
      "  batch 3 loss: 1.1206049919128418\n",
      "  batch 4 loss: 1.2545411586761475\n",
      "  batch 5 loss: 1.1570167541503906\n",
      "  batch 6 loss: 1.2373168468475342\n",
      "  batch 7 loss: 0.9034525156021118\n",
      "  batch 8 loss: 0.9636127948760986\n",
      "  batch 9 loss: 1.110421895980835\n",
      "  batch 10 loss: 1.1465778350830078\n",
      "  batch 11 loss: 0.9996275901794434\n",
      "  batch 12 loss: 1.3393622636795044\n",
      "  batch 13 loss: 1.214390516281128\n",
      "  batch 14 loss: 1.1311771869659424\n",
      "  batch 15 loss: 1.1931716203689575\n",
      "LOSS train 1.1931716203689575 valid 1.3218388557434082\n",
      "ACCURACY train: 0.5 valid 0.0\n",
      "EPOCH 49:\n",
      "  batch 1 loss: 1.131140947341919\n",
      "  batch 2 loss: 1.0049902200698853\n",
      "  batch 3 loss: 1.1536004543304443\n",
      "  batch 4 loss: 0.9158426523208618\n",
      "  batch 5 loss: 1.1291881799697876\n",
      "  batch 6 loss: 1.268057107925415\n",
      "  batch 7 loss: 1.0024515390396118\n",
      "  batch 8 loss: 1.1613750457763672\n",
      "  batch 9 loss: 1.2274090051651\n",
      "  batch 10 loss: 1.272993803024292\n",
      "  batch 11 loss: 1.2242316007614136\n",
      "  batch 12 loss: 0.9728264212608337\n",
      "  batch 13 loss: 1.2573069334030151\n",
      "  batch 14 loss: 1.34975004196167\n",
      "  batch 15 loss: 1.1540493965148926\n",
      "LOSS train 1.1540493965148926 valid 1.3070052862167358\n",
      "ACCURACY train: 0.5 valid 1.0\n",
      "EPOCH 50:\n",
      "  batch 1 loss: 1.1368134021759033\n",
      "  batch 2 loss: 1.0988420248031616\n",
      "  batch 3 loss: 1.2257546186447144\n",
      "  batch 4 loss: 1.1555986404418945\n",
      "  batch 5 loss: 1.0831080675125122\n",
      "  batch 6 loss: 0.9270002841949463\n",
      "  batch 7 loss: 1.0213137865066528\n",
      "  batch 8 loss: 1.3892524242401123\n",
      "  batch 9 loss: 1.3381808996200562\n",
      "  batch 10 loss: 1.2498515844345093\n",
      "  batch 11 loss: 1.4031612873077393\n",
      "  batch 12 loss: 1.085118293762207\n",
      "  batch 13 loss: 0.9571341872215271\n",
      "  batch 14 loss: 1.0894675254821777\n",
      "  batch 15 loss: 0.9208549857139587\n",
      "LOSS train 0.9208549857139587 valid 1.3247110843658447\n",
      "ACCURACY train: 0.375 valid 0.0\n",
      "EPOCH 51:\n",
      "  batch 1 loss: 1.265197515487671\n",
      "  batch 2 loss: 1.2986838817596436\n",
      "  batch 3 loss: 0.9126423597335815\n",
      "  batch 4 loss: 0.9751383066177368\n",
      "  batch 5 loss: 0.9771673679351807\n",
      "  batch 6 loss: 1.31144118309021\n",
      "  batch 7 loss: 1.1015068292617798\n",
      "  batch 8 loss: 1.256380558013916\n",
      "  batch 9 loss: 1.1340819597244263\n",
      "  batch 10 loss: 1.3143916130065918\n",
      "  batch 11 loss: 1.224618911743164\n",
      "  batch 12 loss: 1.0255951881408691\n",
      "  batch 13 loss: 1.1497079133987427\n",
      "  batch 14 loss: 1.1322680711746216\n",
      "  batch 15 loss: 0.9452703595161438\n",
      "LOSS train 0.9452703595161438 valid 1.3159884214401245\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 52:\n",
      "  batch 1 loss: 0.9891427159309387\n",
      "  batch 2 loss: 0.9451495409011841\n",
      "  batch 3 loss: 1.3798269033432007\n",
      "  batch 4 loss: 1.2206908464431763\n",
      "  batch 5 loss: 0.9554201364517212\n",
      "  batch 6 loss: 1.2933080196380615\n",
      "  batch 7 loss: 0.9032031297683716\n",
      "  batch 8 loss: 1.1323236227035522\n",
      "  batch 9 loss: 1.3304604291915894\n",
      "  batch 10 loss: 1.5510132312774658\n",
      "  batch 11 loss: 1.165194034576416\n",
      "  batch 12 loss: 1.194319725036621\n",
      "  batch 13 loss: 1.03644859790802\n",
      "  batch 14 loss: 0.9114556908607483\n",
      "  batch 15 loss: 1.1809468269348145\n",
      "LOSS train 1.1809468269348145 valid 1.303625464439392\n",
      "ACCURACY train: 0.875 valid 1.0\n",
      "EPOCH 53:\n",
      "  batch 1 loss: 1.108351469039917\n",
      "  batch 2 loss: 1.3157668113708496\n",
      "  batch 3 loss: 1.3766754865646362\n",
      "  batch 4 loss: 1.1830254793167114\n",
      "  batch 5 loss: 1.1415807008743286\n",
      "  batch 6 loss: 0.8893195986747742\n",
      "  batch 7 loss: 0.9056848883628845\n",
      "  batch 8 loss: 1.1266642808914185\n",
      "  batch 9 loss: 1.0861515998840332\n",
      "  batch 10 loss: 1.2927253246307373\n",
      "  batch 11 loss: 1.1978545188903809\n",
      "  batch 12 loss: 1.3386350870132446\n",
      "  batch 13 loss: 1.029287338256836\n",
      "  batch 14 loss: 1.0169674158096313\n",
      "  batch 15 loss: 1.0322941541671753\n",
      "LOSS train 1.0322941541671753 valid 1.3188782930374146\n",
      "ACCURACY train: 1.0 valid 0.5\n",
      "EPOCH 54:\n",
      "  batch 1 loss: 1.1518354415893555\n",
      "  batch 2 loss: 1.2318520545959473\n",
      "  batch 3 loss: 1.254353404045105\n",
      "  batch 4 loss: 1.0105031728744507\n",
      "  batch 5 loss: 1.0778915882110596\n",
      "  batch 6 loss: 1.1230206489562988\n",
      "  batch 7 loss: 0.9579280614852905\n",
      "  batch 8 loss: 1.1523003578186035\n",
      "  batch 9 loss: 1.1075310707092285\n",
      "  batch 10 loss: 1.238818645477295\n",
      "  batch 11 loss: 1.0409789085388184\n",
      "  batch 12 loss: 1.0580320358276367\n",
      "  batch 13 loss: 1.3728619813919067\n",
      "  batch 14 loss: 1.1900216341018677\n",
      "  batch 15 loss: 1.1261606216430664\n",
      "LOSS train 1.1261606216430664 valid 1.3106436729431152\n",
      "ACCURACY train: 0.75 valid 1.0\n",
      "EPOCH 55:\n",
      "  batch 1 loss: 1.1581079959869385\n",
      "  batch 2 loss: 1.0060745477676392\n",
      "  batch 3 loss: 1.0893967151641846\n",
      "  batch 4 loss: 1.177351713180542\n",
      "  batch 5 loss: 1.0873538255691528\n",
      "  batch 6 loss: 1.2426162958145142\n",
      "  batch 7 loss: 1.0049259662628174\n",
      "  batch 8 loss: 1.1787699460983276\n",
      "  batch 9 loss: 1.334658145904541\n",
      "  batch 10 loss: 1.0223798751831055\n",
      "  batch 11 loss: 1.1391996145248413\n",
      "  batch 12 loss: 1.174001932144165\n",
      "  batch 13 loss: 1.1411317586898804\n",
      "  batch 14 loss: 1.1410828828811646\n",
      "  batch 15 loss: 1.3579939603805542\n",
      "LOSS train 1.3579939603805542 valid 1.3195124864578247\n",
      "ACCURACY train: nan valid 0.5\n",
      "EPOCH 56:\n",
      "  batch 1 loss: 0.9669255018234253\n",
      "  batch 2 loss: 1.3303167819976807\n",
      "  batch 3 loss: 1.064714789390564\n",
      "  batch 4 loss: 1.0261317491531372\n",
      "  batch 5 loss: 1.2424733638763428\n",
      "  batch 6 loss: 1.0820975303649902\n",
      "  batch 7 loss: 1.0498201847076416\n",
      "  batch 8 loss: 1.1089282035827637\n",
      "  batch 9 loss: 1.2774477005004883\n",
      "  batch 10 loss: 1.050705909729004\n",
      "  batch 11 loss: 1.2864621877670288\n",
      "  batch 12 loss: 1.2273294925689697\n",
      "  batch 13 loss: 1.230595350265503\n",
      "  batch 14 loss: 1.0328081846237183\n",
      "  batch 15 loss: 1.0916279554367065\n",
      "LOSS train 1.0916279554367065 valid 1.322859525680542\n",
      "ACCURACY train: 0.875 valid 1.0\n",
      "EPOCH 57:\n",
      "  batch 1 loss: 1.2411216497421265\n",
      "  batch 2 loss: 1.178077220916748\n",
      "  batch 3 loss: 1.2688390016555786\n",
      "  batch 4 loss: 1.0860798358917236\n",
      "  batch 5 loss: 0.9421943426132202\n",
      "  batch 6 loss: 1.108720064163208\n",
      "  batch 7 loss: 1.0520849227905273\n",
      "  batch 8 loss: 1.226611614227295\n",
      "  batch 9 loss: 1.2909226417541504\n",
      "  batch 10 loss: 1.2042700052261353\n",
      "  batch 11 loss: 1.0675997734069824\n",
      "  batch 12 loss: 1.1601303815841675\n",
      "  batch 13 loss: 1.310088872909546\n",
      "  batch 14 loss: 0.8721438050270081\n",
      "  batch 15 loss: 0.86769038438797\n",
      "LOSS train 0.86769038438797 valid 1.3116942644119263\n",
      "ACCURACY train: 0.625 valid 0.5\n",
      "EPOCH 58:\n",
      "  batch 1 loss: 1.146897554397583\n",
      "  batch 2 loss: 1.2217276096343994\n",
      "  batch 3 loss: 0.9657325148582458\n",
      "  batch 4 loss: 1.2542825937271118\n",
      "  batch 5 loss: 1.2464070320129395\n",
      "  batch 6 loss: 0.9044087529182434\n",
      "  batch 7 loss: 0.9589757919311523\n",
      "  batch 8 loss: 1.0725891590118408\n",
      "  batch 9 loss: 1.1539899110794067\n",
      "  batch 10 loss: 1.1493076086044312\n",
      "  batch 11 loss: 1.2513364553451538\n",
      "  batch 12 loss: 1.2786837816238403\n",
      "  batch 13 loss: 1.285971999168396\n",
      "  batch 14 loss: 1.285760521888733\n",
      "  batch 15 loss: 0.7844560146331787\n",
      "LOSS train 0.7844560146331787 valid 1.3330237865447998\n",
      "ACCURACY train: 0.5 valid 0.0\n",
      "EPOCH 59:\n",
      "  batch 1 loss: 1.1574949026107788\n",
      "  batch 2 loss: 0.9130290746688843\n",
      "  batch 3 loss: 1.156544804573059\n",
      "  batch 4 loss: 1.252626895904541\n",
      "  batch 5 loss: 1.2788716554641724\n",
      "  batch 6 loss: 1.0377566814422607\n",
      "  batch 7 loss: 1.0789806842803955\n",
      "  batch 8 loss: 1.2533578872680664\n",
      "  batch 9 loss: 1.138533115386963\n",
      "  batch 10 loss: 1.016758680343628\n",
      "  batch 11 loss: 1.1892226934432983\n",
      "  batch 12 loss: 1.0741859674453735\n",
      "  batch 13 loss: 1.26451575756073\n",
      "  batch 14 loss: 1.0860589742660522\n",
      "  batch 15 loss: 1.132137656211853\n",
      "LOSS train 1.132137656211853 valid 1.3150080442428589\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 60:\n",
      "  batch 1 loss: 1.0413265228271484\n",
      "  batch 2 loss: 1.0578526258468628\n",
      "  batch 3 loss: 0.9964398145675659\n",
      "  batch 4 loss: 1.0119439363479614\n",
      "  batch 5 loss: 1.1258819103240967\n",
      "  batch 6 loss: 1.2640659809112549\n",
      "  batch 7 loss: 1.3627899885177612\n",
      "  batch 8 loss: 1.1843760013580322\n",
      "  batch 9 loss: 1.0174766778945923\n",
      "  batch 10 loss: 1.04106605052948\n",
      "  batch 11 loss: 1.156693458557129\n",
      "  batch 12 loss: 1.329081654548645\n",
      "  batch 13 loss: 1.1587159633636475\n",
      "  batch 14 loss: 1.1693651676177979\n",
      "  batch 15 loss: 0.97152179479599\n",
      "LOSS train 0.97152179479599 valid 1.31260347366333\n",
      "ACCURACY train: 0.875 valid 0.0\n",
      "EPOCH 61:\n",
      "  batch 1 loss: 1.167313575744629\n",
      "  batch 2 loss: 1.1538419723510742\n",
      "  batch 3 loss: 1.051300048828125\n",
      "  batch 4 loss: 1.071366310119629\n",
      "  batch 5 loss: 1.1213021278381348\n",
      "  batch 6 loss: 1.1256763935089111\n",
      "  batch 7 loss: 1.2264280319213867\n",
      "  batch 8 loss: 1.0123405456542969\n",
      "  batch 9 loss: 1.105799674987793\n",
      "  batch 10 loss: 1.2117213010787964\n",
      "  batch 11 loss: 1.241689920425415\n",
      "  batch 12 loss: 1.1214014291763306\n",
      "  batch 13 loss: 1.0532951354980469\n",
      "  batch 14 loss: 1.258693814277649\n",
      "  batch 15 loss: 0.9478461146354675\n",
      "LOSS train 0.9478461146354675 valid 1.3114880323410034\n",
      "ACCURACY train: 0.625 valid 0.0\n",
      "EPOCH 62:\n",
      "  batch 1 loss: 1.2714860439300537\n",
      "  batch 2 loss: 1.3067599534988403\n",
      "  batch 3 loss: 1.2680104970932007\n",
      "  batch 4 loss: 1.2180439233779907\n",
      "  batch 5 loss: 0.9230866432189941\n",
      "  batch 6 loss: 0.9722294807434082\n",
      "  batch 7 loss: 0.938613772392273\n",
      "  batch 8 loss: 0.9556130766868591\n",
      "  batch 9 loss: 0.9845104217529297\n",
      "  batch 10 loss: 1.0113437175750732\n",
      "  batch 11 loss: 1.2075824737548828\n",
      "  batch 12 loss: 1.0992356538772583\n",
      "  batch 13 loss: 1.42092764377594\n",
      "  batch 14 loss: 1.2740809917449951\n",
      "  batch 15 loss: 1.10356867313385\n",
      "LOSS train 1.10356867313385 valid 1.3210904598236084\n",
      "ACCURACY train: 0.625 valid 0.0\n",
      "EPOCH 63:\n",
      "  batch 1 loss: 1.250732660293579\n",
      "  batch 2 loss: 0.9416486024856567\n",
      "  batch 3 loss: 1.1920498609542847\n",
      "  batch 4 loss: 1.2353922128677368\n",
      "  batch 5 loss: 1.138289451599121\n",
      "  batch 6 loss: 1.1410799026489258\n",
      "  batch 7 loss: 1.4003748893737793\n",
      "  batch 8 loss: 0.8632048964500427\n",
      "  batch 9 loss: 1.1986947059631348\n",
      "  batch 10 loss: 1.2824448347091675\n",
      "  batch 11 loss: 1.13206148147583\n",
      "  batch 12 loss: 1.1626664400100708\n",
      "  batch 13 loss: 0.9531362056732178\n",
      "  batch 14 loss: 0.8939765691757202\n",
      "  batch 15 loss: 1.1734992265701294\n",
      "LOSS train 1.1734992265701294 valid 1.3200589418411255\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 64:\n",
      "  batch 1 loss: 1.001832365989685\n",
      "  batch 2 loss: 1.2738521099090576\n",
      "  batch 3 loss: 1.1946263313293457\n",
      "  batch 4 loss: 1.058435320854187\n",
      "  batch 5 loss: 0.9928261637687683\n",
      "  batch 6 loss: 1.1738204956054688\n",
      "  batch 7 loss: 1.0021239519119263\n",
      "  batch 8 loss: 0.9684126377105713\n",
      "  batch 9 loss: 1.1242560148239136\n",
      "  batch 10 loss: 1.3033250570297241\n",
      "  batch 11 loss: 1.3106834888458252\n",
      "  batch 12 loss: 0.9983946084976196\n",
      "  batch 13 loss: 1.0385743379592896\n",
      "  batch 14 loss: 1.3759653568267822\n",
      "  batch 15 loss: 1.0582953691482544\n",
      "LOSS train 1.0582953691482544 valid 1.3180350065231323\n",
      "ACCURACY train: 0.875 valid 0.5\n",
      "EPOCH 65:\n",
      "  batch 1 loss: 0.7823196649551392\n",
      "  batch 2 loss: 1.0473679304122925\n",
      "  batch 3 loss: 1.0547395944595337\n",
      "  batch 4 loss: 1.3179106712341309\n",
      "  batch 5 loss: 0.9433578848838806\n",
      "  batch 6 loss: 0.978194534778595\n",
      "  batch 7 loss: 1.3255516290664673\n",
      "  batch 8 loss: 1.3861422538757324\n",
      "  batch 9 loss: 1.2727906703948975\n",
      "  batch 10 loss: 1.310989499092102\n",
      "  batch 11 loss: 1.0423794984817505\n",
      "  batch 12 loss: 1.0410677194595337\n",
      "  batch 13 loss: 1.0571930408477783\n",
      "  batch 14 loss: 1.2385854721069336\n",
      "  batch 15 loss: 1.0541976690292358\n",
      "LOSS train 1.0541976690292358 valid 1.313838005065918\n",
      "ACCURACY train: 0.875 valid 1.0\n",
      "EPOCH 66:\n",
      "  batch 1 loss: 1.0733065605163574\n",
      "  batch 2 loss: 1.087403416633606\n",
      "  batch 3 loss: 1.1053471565246582\n",
      "  batch 4 loss: 1.0386157035827637\n",
      "  batch 5 loss: 1.1408917903900146\n",
      "  batch 6 loss: 1.1815505027770996\n",
      "  batch 7 loss: 1.07377290725708\n",
      "  batch 8 loss: 1.250618577003479\n",
      "  batch 9 loss: 1.2360072135925293\n",
      "  batch 10 loss: 1.0406708717346191\n",
      "  batch 11 loss: 1.1715846061706543\n",
      "  batch 12 loss: 1.3764986991882324\n",
      "  batch 13 loss: 1.018968939781189\n",
      "  batch 14 loss: 1.1001989841461182\n",
      "  batch 15 loss: 0.9444431662559509\n",
      "LOSS train 0.9444431662559509 valid 1.3137449026107788\n",
      "ACCURACY train: 0.625 valid 0.0\n",
      "EPOCH 67:\n",
      "  batch 1 loss: 1.2728424072265625\n",
      "  batch 2 loss: 1.0489121675491333\n",
      "  batch 3 loss: 1.2270622253417969\n",
      "  batch 4 loss: 1.1079272031784058\n",
      "  batch 5 loss: 1.2932887077331543\n",
      "  batch 6 loss: 1.4173805713653564\n",
      "  batch 7 loss: 1.1419689655303955\n",
      "  batch 8 loss: 1.1978310346603394\n",
      "  batch 9 loss: 1.0147051811218262\n",
      "  batch 10 loss: 1.0029098987579346\n",
      "  batch 11 loss: 0.9782842397689819\n",
      "  batch 12 loss: 0.9498192071914673\n",
      "  batch 13 loss: 1.1362314224243164\n",
      "  batch 14 loss: 1.0929721593856812\n",
      "  batch 15 loss: 0.8068463206291199\n",
      "LOSS train 0.8068463206291199 valid 1.3179128170013428\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 68:\n",
      "  batch 1 loss: 1.0857622623443604\n",
      "  batch 2 loss: 1.12852144241333\n",
      "  batch 3 loss: 1.0913394689559937\n",
      "  batch 4 loss: 1.3632248640060425\n",
      "  batch 5 loss: 1.112216830253601\n",
      "  batch 6 loss: 1.2234113216400146\n",
      "  batch 7 loss: 1.0348045825958252\n",
      "  batch 8 loss: 1.044131875038147\n",
      "  batch 9 loss: 1.2486439943313599\n",
      "  batch 10 loss: 1.0570110082626343\n",
      "  batch 11 loss: 1.0818164348602295\n",
      "  batch 12 loss: 1.1495962142944336\n",
      "  batch 13 loss: 1.186648964881897\n",
      "  batch 14 loss: 0.9254842400550842\n",
      "  batch 15 loss: 1.2195876836776733\n",
      "LOSS train 1.2195876836776733 valid 1.3281092643737793\n",
      "ACCURACY train: 0.75 valid 0.5\n",
      "EPOCH 69:\n",
      "  batch 1 loss: 1.1603150367736816\n",
      "  batch 2 loss: 0.8977859020233154\n",
      "  batch 3 loss: 1.2406340837478638\n",
      "  batch 4 loss: 0.9492696523666382\n",
      "  batch 5 loss: 1.0771384239196777\n",
      "  batch 6 loss: 1.2332004308700562\n",
      "  batch 7 loss: 1.4136539697647095\n",
      "  batch 8 loss: 1.1348271369934082\n",
      "  batch 9 loss: 0.9617543816566467\n",
      "  batch 10 loss: 1.1131653785705566\n",
      "  batch 11 loss: 0.977829098701477\n",
      "  batch 12 loss: 1.232500433921814\n",
      "  batch 13 loss: 0.9944413900375366\n",
      "  batch 14 loss: 1.4463295936584473\n",
      "  batch 15 loss: 0.8169838786125183\n",
      "LOSS train 0.8169838786125183 valid 1.3175632953643799\n",
      "ACCURACY train: 0.5 valid 1.0\n",
      "EPOCH 70:\n",
      "  batch 1 loss: 1.3665913343429565\n",
      "  batch 2 loss: 0.9799195528030396\n",
      "  batch 3 loss: 1.1324957609176636\n",
      "  batch 4 loss: 1.130324125289917\n",
      "  batch 5 loss: 1.248900294303894\n",
      "  batch 6 loss: 1.3792085647583008\n",
      "  batch 7 loss: 1.039043664932251\n",
      "  batch 8 loss: 1.0617741346359253\n",
      "  batch 9 loss: 1.002153992652893\n",
      "  batch 10 loss: 1.1522881984710693\n",
      "  batch 11 loss: 0.9686469435691833\n",
      "  batch 12 loss: 1.1891528367996216\n",
      "  batch 13 loss: 1.0214701890945435\n",
      "  batch 14 loss: 1.1172358989715576\n",
      "  batch 15 loss: 0.9133217930793762\n",
      "LOSS train 0.9133217930793762 valid 1.3153736591339111\n",
      "ACCURACY train: 0.75 valid 0.0\n",
      "EPOCH 71:\n",
      "  batch 1 loss: 1.0723141431808472\n",
      "  batch 2 loss: 1.3317371606826782\n",
      "  batch 3 loss: 0.8570250868797302\n",
      "  batch 4 loss: 1.0203969478607178\n",
      "  batch 5 loss: 1.2246201038360596\n",
      "  batch 6 loss: 0.8876650333404541\n",
      "  batch 7 loss: 1.208128809928894\n",
      "  batch 8 loss: 0.979767382144928\n",
      "  batch 9 loss: 1.284440279006958\n",
      "  batch 10 loss: 1.1430437564849854\n",
      "  batch 11 loss: 1.0766863822937012\n",
      "  batch 12 loss: 1.1062607765197754\n",
      "  batch 13 loss: 1.2936036586761475\n",
      "  batch 14 loss: 1.1073083877563477\n",
      "  batch 15 loss: 1.3686412572860718\n",
      "LOSS train 1.3686412572860718 valid 1.3165088891983032\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 72:\n",
      "  batch 1 loss: 1.0937721729278564\n",
      "  batch 2 loss: 1.1845117807388306\n",
      "  batch 3 loss: 1.1289002895355225\n",
      "  batch 4 loss: 1.2428929805755615\n",
      "  batch 5 loss: 0.9881316423416138\n",
      "  batch 6 loss: 1.0222160816192627\n",
      "  batch 7 loss: 0.9417310953140259\n",
      "  batch 8 loss: 1.0803937911987305\n",
      "  batch 9 loss: 1.183019757270813\n",
      "  batch 10 loss: 1.2448756694793701\n",
      "  batch 11 loss: 1.130666732788086\n",
      "  batch 12 loss: 1.130913257598877\n",
      "  batch 13 loss: 0.9060749411582947\n",
      "  batch 14 loss: 1.4025837182998657\n",
      "  batch 15 loss: 1.1194993257522583\n",
      "LOSS train 1.1194993257522583 valid 1.3173502683639526\n",
      "ACCURACY train: 0.75 valid 1.0\n",
      "EPOCH 73:\n",
      "  batch 1 loss: 1.3594557046890259\n",
      "  batch 2 loss: 1.0339618921279907\n",
      "  batch 3 loss: 1.1049909591674805\n",
      "  batch 4 loss: 1.1396410465240479\n",
      "  batch 5 loss: 1.1646134853363037\n",
      "  batch 6 loss: 1.273625373840332\n",
      "  batch 7 loss: 0.9999088644981384\n",
      "  batch 8 loss: 1.1065304279327393\n",
      "  batch 9 loss: 1.2460038661956787\n",
      "  batch 10 loss: 1.1031382083892822\n",
      "  batch 11 loss: 1.05402672290802\n",
      "  batch 12 loss: 0.947196364402771\n",
      "  batch 13 loss: 0.9281938672065735\n",
      "  batch 14 loss: 1.330467700958252\n",
      "  batch 15 loss: 0.8279686570167542\n",
      "LOSS train 0.8279686570167542 valid 1.3204423189163208\n",
      "ACCURACY train: 0.5 valid 0.0\n",
      "EPOCH 74:\n",
      "  batch 1 loss: 0.9118243455886841\n",
      "  batch 2 loss: 1.1912035942077637\n",
      "  batch 3 loss: 1.172407627105713\n",
      "  batch 4 loss: 1.306159257888794\n",
      "  batch 5 loss: 0.9761890172958374\n",
      "  batch 6 loss: 0.934952974319458\n",
      "  batch 7 loss: 1.1340992450714111\n",
      "  batch 8 loss: 1.3367106914520264\n",
      "  batch 9 loss: 1.0519704818725586\n",
      "  batch 10 loss: 1.326353907585144\n",
      "  batch 11 loss: 1.090399146080017\n",
      "  batch 12 loss: 1.2146179676055908\n",
      "  batch 13 loss: 1.008653998374939\n",
      "  batch 14 loss: 0.9896925687789917\n",
      "  batch 15 loss: 1.14260733127594\n",
      "LOSS train 1.14260733127594 valid 1.3187909126281738\n",
      "ACCURACY train: 0.75 valid 1.0\n",
      "EPOCH 75:\n",
      "  batch 1 loss: 1.1967358589172363\n",
      "  batch 2 loss: 1.1459758281707764\n",
      "  batch 3 loss: 1.1144747734069824\n",
      "  batch 4 loss: 1.0780622959136963\n",
      "  batch 5 loss: 0.7875022292137146\n",
      "  batch 6 loss: 1.0560200214385986\n",
      "  batch 7 loss: 1.2841721773147583\n",
      "  batch 8 loss: 1.1538634300231934\n",
      "  batch 9 loss: 1.1443861722946167\n",
      "  batch 10 loss: 1.1181613206863403\n",
      "  batch 11 loss: 1.0903735160827637\n",
      "  batch 12 loss: 1.2343015670776367\n",
      "  batch 13 loss: 0.882672905921936\n",
      "  batch 14 loss: 1.3075145483016968\n",
      "  batch 15 loss: 1.32868230342865\n",
      "LOSS train 1.32868230342865 valid 1.3235801458358765\n",
      "ACCURACY train: nan valid 0.5\n",
      "EPOCH 76:\n",
      "  batch 1 loss: 1.1629605293273926\n",
      "  batch 2 loss: 1.2495181560516357\n",
      "  batch 3 loss: 1.0919828414916992\n",
      "  batch 4 loss: 0.9774645566940308\n",
      "  batch 5 loss: 1.1464201211929321\n",
      "  batch 6 loss: 0.9458162784576416\n",
      "  batch 7 loss: 1.0604254007339478\n",
      "  batch 8 loss: 1.2615118026733398\n",
      "  batch 9 loss: 1.168004035949707\n",
      "  batch 10 loss: 1.1439104080200195\n",
      "  batch 11 loss: 1.1851229667663574\n",
      "  batch 12 loss: 0.9854573011398315\n",
      "  batch 13 loss: 1.1155953407287598\n",
      "  batch 14 loss: 1.1284923553466797\n",
      "  batch 15 loss: 1.1458455324172974\n",
      "LOSS train 1.1458455324172974 valid 1.3114726543426514\n",
      "ACCURACY train: 0.625 valid 1.0\n",
      "EPOCH 77:\n",
      "  batch 1 loss: 1.2120686769485474\n",
      "  batch 2 loss: 1.1673169136047363\n",
      "  batch 3 loss: 1.0147850513458252\n",
      "  batch 4 loss: 1.1184592247009277\n",
      "  batch 5 loss: 1.1331733465194702\n",
      "  batch 6 loss: 0.9646804928779602\n",
      "  batch 7 loss: 1.0528024435043335\n",
      "  batch 8 loss: 1.1682047843933105\n",
      "  batch 9 loss: 1.0313547849655151\n",
      "  batch 10 loss: 1.125750184059143\n",
      "  batch 11 loss: 1.072906255722046\n",
      "  batch 12 loss: 1.3522708415985107\n",
      "  batch 13 loss: 1.0824475288391113\n",
      "  batch 14 loss: 1.1481539011001587\n",
      "  batch 15 loss: 1.0041359663009644\n",
      "LOSS train 1.0041359663009644 valid 1.3192176818847656\n",
      "ACCURACY train: 0.875 valid 0.5\n",
      "EPOCH 78:\n",
      "  batch 1 loss: 1.069985032081604\n",
      "  batch 2 loss: 1.240959882736206\n",
      "  batch 3 loss: 1.1689422130584717\n",
      "  batch 4 loss: 1.1719269752502441\n",
      "  batch 5 loss: 0.9708693623542786\n",
      "  batch 6 loss: 1.0253255367279053\n",
      "  batch 7 loss: 1.1665449142456055\n",
      "  batch 8 loss: 1.1685748100280762\n",
      "  batch 9 loss: 1.3585721254348755\n",
      "  batch 10 loss: 0.9301716685295105\n",
      "  batch 11 loss: 0.9098680019378662\n",
      "  batch 12 loss: 1.1179604530334473\n",
      "  batch 13 loss: 1.0474863052368164\n",
      "  batch 14 loss: 1.324413776397705\n",
      "  batch 15 loss: 0.9210484027862549\n",
      "LOSS train 0.9210484027862549 valid 1.3155863285064697\n",
      "ACCURACY train: 0.875 valid 0.0\n",
      "EPOCH 79:\n",
      "  batch 1 loss: 1.2609498500823975\n",
      "  batch 2 loss: 1.1605854034423828\n",
      "  batch 3 loss: 1.0148087739944458\n",
      "  batch 4 loss: 1.1048707962036133\n",
      "  batch 5 loss: 1.2797882556915283\n",
      "  batch 6 loss: 1.3748871088027954\n",
      "  batch 7 loss: 1.147429347038269\n",
      "  batch 8 loss: 1.1045429706573486\n",
      "  batch 9 loss: 0.9622851610183716\n",
      "  batch 10 loss: 1.0556787252426147\n",
      "  batch 11 loss: 1.0254881381988525\n",
      "  batch 12 loss: 1.004861831665039\n",
      "  batch 13 loss: 0.969039797782898\n",
      "  batch 14 loss: 1.0667662620544434\n",
      "  batch 15 loss: 1.2300753593444824\n",
      "LOSS train 1.2300753593444824 valid 1.3222968578338623\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 80:\n",
      "  batch 1 loss: 1.207344651222229\n",
      "  batch 2 loss: 1.111510157585144\n",
      "  batch 3 loss: 1.3365843296051025\n",
      "  batch 4 loss: 1.037312388420105\n",
      "  batch 5 loss: 1.0248327255249023\n",
      "  batch 6 loss: 1.0758507251739502\n",
      "  batch 7 loss: 0.9613630771636963\n",
      "  batch 8 loss: 1.3453718423843384\n",
      "  batch 9 loss: 0.9409931898117065\n",
      "  batch 10 loss: 1.1290442943572998\n",
      "  batch 11 loss: 1.1387726068496704\n",
      "  batch 12 loss: 1.0896836519241333\n",
      "  batch 13 loss: 1.149242877960205\n",
      "  batch 14 loss: 1.164272427558899\n",
      "  batch 15 loss: 0.9450517296791077\n",
      "LOSS train 0.9450517296791077 valid 1.3235085010528564\n",
      "ACCURACY train: 0.5 valid 0.0\n",
      "EPOCH 81:\n",
      "  batch 1 loss: 1.0657440423965454\n",
      "  batch 2 loss: 1.058435320854187\n",
      "  batch 3 loss: 1.1041964292526245\n",
      "  batch 4 loss: 1.1530060768127441\n",
      "  batch 5 loss: 1.063199758529663\n",
      "  batch 6 loss: 1.2785427570343018\n",
      "  batch 7 loss: 0.9289284944534302\n",
      "  batch 8 loss: 1.056065320968628\n",
      "  batch 9 loss: 1.1466196775436401\n",
      "  batch 10 loss: 0.9937587976455688\n",
      "  batch 11 loss: 1.3067147731781006\n",
      "  batch 12 loss: 1.1079827547073364\n",
      "  batch 13 loss: 1.175642967224121\n",
      "  batch 14 loss: 1.040735125541687\n",
      "  batch 15 loss: 1.3409219980239868\n",
      "LOSS train 1.3409219980239868 valid 1.3265670537948608\n",
      "ACCURACY train: 0.5 valid 0.0\n",
      "EPOCH 82:\n",
      "  batch 1 loss: 1.0436259508132935\n",
      "  batch 2 loss: 0.854651153087616\n",
      "  batch 3 loss: 0.966380774974823\n",
      "  batch 4 loss: 0.9683937430381775\n",
      "  batch 5 loss: 1.1342631578445435\n",
      "  batch 6 loss: 1.0428496599197388\n",
      "  batch 7 loss: 1.0574544668197632\n",
      "  batch 8 loss: 1.1411229372024536\n",
      "  batch 9 loss: 1.2716346979141235\n",
      "  batch 10 loss: 1.0839388370513916\n",
      "  batch 11 loss: 1.2322330474853516\n",
      "  batch 12 loss: 1.3036233186721802\n",
      "  batch 13 loss: 1.2438538074493408\n",
      "  batch 14 loss: 1.2587260007858276\n",
      "  batch 15 loss: 1.020783543586731\n",
      "LOSS train 1.020783543586731 valid 1.3233795166015625\n",
      "ACCURACY train: 0.5 valid 0.0\n",
      "EPOCH 83:\n",
      "  batch 1 loss: 1.00166654586792\n",
      "  batch 2 loss: 1.006064534187317\n",
      "  batch 3 loss: 1.213795781135559\n",
      "  batch 4 loss: 1.1188477277755737\n",
      "  batch 5 loss: 1.20112144947052\n",
      "  batch 6 loss: 1.0016545057296753\n",
      "  batch 7 loss: 1.1519320011138916\n",
      "  batch 8 loss: 1.114008903503418\n",
      "  batch 9 loss: 1.0124136209487915\n",
      "  batch 10 loss: 1.16341233253479\n",
      "  batch 11 loss: 1.2134588956832886\n",
      "  batch 12 loss: 1.242417812347412\n",
      "  batch 13 loss: 0.9168471097946167\n",
      "  batch 14 loss: 1.3453339338302612\n",
      "  batch 15 loss: 0.9083859324455261\n",
      "LOSS train 0.9083859324455261 valid 1.3144278526306152\n",
      "ACCURACY train: nan valid 1.0\n",
      "EPOCH 84:\n",
      "  batch 1 loss: 1.1163370609283447\n",
      "  batch 2 loss: 1.0916579961776733\n",
      "  batch 3 loss: 1.0167711973190308\n",
      "  batch 4 loss: 1.1792173385620117\n",
      "  batch 5 loss: 1.2672007083892822\n",
      "  batch 6 loss: 0.8778358101844788\n",
      "  batch 7 loss: 0.9835079312324524\n",
      "  batch 8 loss: 1.1344645023345947\n",
      "  batch 9 loss: 1.0570069551467896\n",
      "  batch 10 loss: 1.1627236604690552\n",
      "  batch 11 loss: 1.1797341108322144\n",
      "  batch 12 loss: 1.0865603685379028\n",
      "  batch 13 loss: 1.0516377687454224\n",
      "  batch 14 loss: 1.2730515003204346\n",
      "  batch 15 loss: 1.1550856828689575\n",
      "LOSS train 1.1550856828689575 valid 1.3260751962661743\n",
      "ACCURACY train: 0.625 valid 0.5\n",
      "EPOCH 85:\n",
      "  batch 1 loss: 1.0178337097167969\n",
      "  batch 2 loss: 1.2837586402893066\n",
      "  batch 3 loss: 1.2154308557510376\n",
      "  batch 4 loss: 1.0075616836547852\n",
      "  batch 5 loss: 0.9529203176498413\n",
      "  batch 6 loss: 1.1357910633087158\n",
      "  batch 7 loss: 0.9102158546447754\n",
      "  batch 8 loss: 1.0828323364257812\n",
      "  batch 9 loss: 1.3032459020614624\n",
      "  batch 10 loss: 1.078090786933899\n",
      "  batch 11 loss: 1.1354972124099731\n",
      "  batch 12 loss: 1.1564710140228271\n",
      "  batch 13 loss: 1.2495360374450684\n",
      "  batch 14 loss: 0.9594861268997192\n",
      "  batch 15 loss: 1.2561153173446655\n",
      "LOSS train 1.2561153173446655 valid 1.319454312324524\n",
      "ACCURACY train: 0.625 valid 1.0\n",
      "EPOCH 86:\n",
      "  batch 1 loss: 1.1412413120269775\n",
      "  batch 2 loss: 1.2147663831710815\n",
      "  batch 3 loss: 1.1727609634399414\n",
      "  batch 4 loss: 1.1101994514465332\n",
      "  batch 5 loss: 1.230709195137024\n",
      "  batch 6 loss: 1.0459403991699219\n",
      "  batch 7 loss: 1.0704834461212158\n",
      "  batch 8 loss: 1.2239768505096436\n",
      "  batch 9 loss: 1.12106192111969\n",
      "  batch 10 loss: 0.9578321576118469\n",
      "  batch 11 loss: 0.8274761438369751\n",
      "  batch 12 loss: 1.1136223077774048\n",
      "  batch 13 loss: 1.257441520690918\n",
      "  batch 14 loss: 1.000015139579773\n",
      "  batch 15 loss: 1.1281291246414185\n",
      "LOSS train 1.1281291246414185 valid 1.3116475343704224\n",
      "ACCURACY train: 1.0 valid 0.0\n",
      "EPOCH 87:\n",
      "  batch 1 loss: 1.219016671180725\n",
      "  batch 2 loss: 1.1978037357330322\n",
      "  batch 3 loss: 1.1592811346054077\n",
      "  batch 4 loss: 1.033545732498169\n",
      "  batch 5 loss: 1.1544992923736572\n",
      "  batch 6 loss: 1.077650547027588\n",
      "  batch 7 loss: 1.042419672012329\n",
      "  batch 8 loss: 1.0687874555587769\n",
      "  batch 9 loss: 1.1953229904174805\n",
      "  batch 10 loss: 1.0313925743103027\n",
      "  batch 11 loss: 1.0538593530654907\n",
      "  batch 12 loss: 1.1142085790634155\n",
      "  batch 13 loss: 0.8973605632781982\n",
      "  batch 14 loss: 1.2990314960479736\n",
      "  batch 15 loss: 1.0717190504074097\n",
      "LOSS train 1.0717190504074097 valid 1.3195197582244873\n",
      "ACCURACY train: nan valid 1.0\n",
      "EPOCH 88:\n",
      "  batch 1 loss: 1.1231769323349\n",
      "  batch 2 loss: 0.956037163734436\n",
      "  batch 3 loss: 1.1608693599700928\n",
      "  batch 4 loss: 0.9934707283973694\n",
      "  batch 5 loss: 1.2410930395126343\n",
      "  batch 6 loss: 1.0807567834854126\n",
      "  batch 7 loss: 0.9068453311920166\n",
      "  batch 8 loss: 1.1836011409759521\n",
      "  batch 9 loss: 1.2976608276367188\n",
      "  batch 10 loss: 1.3573166131973267\n",
      "  batch 11 loss: 0.8460325002670288\n",
      "  batch 12 loss: 1.0419116020202637\n",
      "  batch 13 loss: 1.0428476333618164\n",
      "  batch 14 loss: 1.203216314315796\n",
      "  batch 15 loss: 1.2307928800582886\n",
      "LOSS train 1.2307928800582886 valid 1.3137143850326538\n",
      "ACCURACY train: 0.625 valid 0.5\n",
      "EPOCH 89:\n",
      "  batch 1 loss: 0.9066692590713501\n",
      "  batch 2 loss: 1.0557395219802856\n",
      "  batch 3 loss: 1.0802338123321533\n",
      "  batch 4 loss: 0.8066103458404541\n",
      "  batch 5 loss: 0.9908931255340576\n",
      "  batch 6 loss: 1.118155598640442\n",
      "  batch 7 loss: 1.1098206043243408\n",
      "  batch 8 loss: 1.2248425483703613\n",
      "  batch 9 loss: 1.126745343208313\n",
      "  batch 10 loss: 1.320460557937622\n",
      "  batch 11 loss: 0.8985270857810974\n",
      "  batch 12 loss: 1.2764341831207275\n",
      "  batch 13 loss: 1.163812518119812\n",
      "  batch 14 loss: 1.249523401260376\n",
      "  batch 15 loss: 1.431440830230713\n",
      "LOSS train 1.431440830230713 valid 1.3081635236740112\n",
      "ACCURACY train: 0.75 valid 1.0\n",
      "EPOCH 90:\n",
      "  batch 1 loss: 1.0101027488708496\n",
      "  batch 2 loss: 0.9617026448249817\n",
      "  batch 3 loss: 1.274379849433899\n",
      "  batch 4 loss: 1.0696829557418823\n",
      "  batch 5 loss: 1.0865741968154907\n",
      "  batch 6 loss: 1.1550419330596924\n",
      "  batch 7 loss: 0.9866082072257996\n",
      "  batch 8 loss: 1.2352628707885742\n",
      "  batch 9 loss: 1.0048627853393555\n",
      "  batch 10 loss: 1.1708728075027466\n",
      "  batch 11 loss: 1.1455790996551514\n",
      "  batch 12 loss: 1.2477093935012817\n",
      "  batch 13 loss: 1.0897718667984009\n",
      "  batch 14 loss: 1.1277170181274414\n",
      "  batch 15 loss: 0.8496903777122498\n",
      "LOSS train 0.8496903777122498 valid 1.3151224851608276\n",
      "ACCURACY train: 0.625 valid 0.0\n",
      "EPOCH 91:\n",
      "  batch 1 loss: 1.0003817081451416\n",
      "  batch 2 loss: 1.1371039152145386\n",
      "  batch 3 loss: 1.0310375690460205\n",
      "  batch 4 loss: 0.9215089082717896\n",
      "  batch 5 loss: 1.2144371271133423\n",
      "  batch 6 loss: 1.1293631792068481\n",
      "  batch 7 loss: 1.3734463453292847\n",
      "  batch 8 loss: 1.0634541511535645\n",
      "  batch 9 loss: 0.9391801357269287\n",
      "  batch 10 loss: 0.9762640595436096\n",
      "  batch 11 loss: 1.3247301578521729\n",
      "  batch 12 loss: 1.165967345237732\n",
      "  batch 13 loss: 1.0970849990844727\n",
      "  batch 14 loss: 1.0536577701568604\n",
      "  batch 15 loss: 1.1836694478988647\n",
      "LOSS train 1.1836694478988647 valid 1.319785237312317\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 92:\n",
      "  batch 1 loss: 1.219591736793518\n",
      "  batch 2 loss: 0.9267294406890869\n",
      "  batch 3 loss: 1.157801866531372\n",
      "  batch 4 loss: 1.0615061521530151\n",
      "  batch 5 loss: 1.0901107788085938\n",
      "  batch 6 loss: 1.1590198278427124\n",
      "  batch 7 loss: 0.918409526348114\n",
      "  batch 8 loss: 1.224229335784912\n",
      "  batch 9 loss: 1.0408169031143188\n",
      "  batch 10 loss: 1.073227882385254\n",
      "  batch 11 loss: 1.2360087633132935\n",
      "  batch 12 loss: 0.9500721096992493\n",
      "  batch 13 loss: 1.2545807361602783\n",
      "  batch 14 loss: 1.080441951751709\n",
      "  batch 15 loss: 1.2951209545135498\n",
      "LOSS train 1.2951209545135498 valid 1.311549186706543\n",
      "ACCURACY train: 0.5 valid 1.0\n",
      "EPOCH 93:\n",
      "  batch 1 loss: 1.0670875310897827\n",
      "  batch 2 loss: 1.1266968250274658\n",
      "  batch 3 loss: 1.1415183544158936\n",
      "  batch 4 loss: 1.101273536682129\n",
      "  batch 5 loss: 1.0423730611801147\n",
      "  batch 6 loss: 1.2649524211883545\n",
      "  batch 7 loss: 1.1076297760009766\n",
      "  batch 8 loss: 1.198574423789978\n",
      "  batch 9 loss: 0.9731502532958984\n",
      "  batch 10 loss: 1.0451009273529053\n",
      "  batch 11 loss: 1.073609709739685\n",
      "  batch 12 loss: 1.1291555166244507\n",
      "  batch 13 loss: 1.2572180032730103\n",
      "  batch 14 loss: 1.0077121257781982\n",
      "  batch 15 loss: 0.8483948111534119\n",
      "LOSS train 0.8483948111534119 valid 1.319105625152588\n",
      "ACCURACY train: 0.5 valid 0.0\n",
      "EPOCH 94:\n",
      "  batch 1 loss: 1.0168259143829346\n",
      "  batch 2 loss: 1.369084358215332\n",
      "  batch 3 loss: 1.12974214553833\n",
      "  batch 4 loss: 1.249807596206665\n",
      "  batch 5 loss: 0.9179732799530029\n",
      "  batch 6 loss: 1.0664010047912598\n",
      "  batch 7 loss: 0.9258295893669128\n",
      "  batch 8 loss: 1.1696984767913818\n",
      "  batch 9 loss: 0.9800235033035278\n",
      "  batch 10 loss: 1.2319386005401611\n",
      "  batch 11 loss: 1.1716363430023193\n",
      "  batch 12 loss: 0.8971078395843506\n",
      "  batch 13 loss: 1.2245889902114868\n",
      "  batch 14 loss: 0.9791490435600281\n",
      "  batch 15 loss: 1.362027645111084\n",
      "LOSS train 1.362027645111084 valid 1.3139524459838867\n",
      "ACCURACY train: 0.625 valid 1.0\n",
      "EPOCH 95:\n",
      "  batch 1 loss: 1.145206093788147\n",
      "  batch 2 loss: 1.137284517288208\n",
      "  batch 3 loss: 1.143050193786621\n",
      "  batch 4 loss: 0.9393695592880249\n",
      "  batch 5 loss: 1.1364516019821167\n",
      "  batch 6 loss: 0.964137077331543\n",
      "  batch 7 loss: 0.8023381233215332\n",
      "  batch 8 loss: 1.2092161178588867\n",
      "  batch 9 loss: 1.1546416282653809\n",
      "  batch 10 loss: 1.0200146436691284\n",
      "  batch 11 loss: 1.327613115310669\n",
      "  batch 12 loss: 1.165724277496338\n",
      "  batch 13 loss: 1.0347832441329956\n",
      "  batch 14 loss: 1.1073551177978516\n",
      "  batch 15 loss: 1.3631668090820312\n",
      "LOSS train 1.3631668090820312 valid 1.3184444904327393\n",
      "ACCURACY train: nan valid 0.5\n",
      "EPOCH 96:\n",
      "  batch 1 loss: 1.0168651342391968\n",
      "  batch 2 loss: 1.1651394367218018\n",
      "  batch 3 loss: 1.0565681457519531\n",
      "  batch 4 loss: 1.1092714071273804\n",
      "  batch 5 loss: 1.0170838832855225\n",
      "  batch 6 loss: 1.0447354316711426\n",
      "  batch 7 loss: 1.2472540140151978\n",
      "  batch 8 loss: 1.08556067943573\n",
      "  batch 9 loss: 0.9555093050003052\n",
      "  batch 10 loss: 1.1991267204284668\n",
      "  batch 11 loss: 1.127020239830017\n",
      "  batch 12 loss: 1.0808112621307373\n",
      "  batch 13 loss: 1.0666977167129517\n",
      "  batch 14 loss: 1.117075800895691\n",
      "  batch 15 loss: 1.3530802726745605\n",
      "LOSS train 1.3530802726745605 valid 1.3191859722137451\n",
      "ACCURACY train: 0.75 valid 1.0\n",
      "EPOCH 97:\n",
      "  batch 1 loss: 0.9053815603256226\n",
      "  batch 2 loss: 1.187904953956604\n",
      "  batch 3 loss: 0.9560765624046326\n",
      "  batch 4 loss: 1.214203119277954\n",
      "  batch 5 loss: 1.0606073141098022\n",
      "  batch 6 loss: 1.2491456270217896\n",
      "  batch 7 loss: 1.2248371839523315\n",
      "  batch 8 loss: 1.000669240951538\n",
      "  batch 9 loss: 1.0232617855072021\n",
      "  batch 10 loss: 1.2829225063323975\n",
      "  batch 11 loss: 1.1342321634292603\n",
      "  batch 12 loss: 0.907779335975647\n",
      "  batch 13 loss: 1.2439665794372559\n",
      "  batch 14 loss: 0.8879252672195435\n",
      "  batch 15 loss: 1.409409523010254\n",
      "LOSS train 1.409409523010254 valid 1.3201359510421753\n",
      "ACCURACY train: 0.5 valid 0.5\n",
      "EPOCH 98:\n",
      "  batch 1 loss: 0.9430288076400757\n",
      "  batch 2 loss: 1.0526326894760132\n",
      "  batch 3 loss: 1.2492587566375732\n",
      "  batch 4 loss: 1.3572399616241455\n",
      "  batch 5 loss: 0.9212595224380493\n",
      "  batch 6 loss: 1.3300284147262573\n",
      "  batch 7 loss: 1.12552809715271\n",
      "  batch 8 loss: 1.1497557163238525\n",
      "  batch 9 loss: 1.131528377532959\n",
      "  batch 10 loss: 0.95052170753479\n",
      "  batch 11 loss: 1.0915430784225464\n",
      "  batch 12 loss: 1.0461924076080322\n",
      "  batch 13 loss: 0.8551578521728516\n",
      "  batch 14 loss: 1.15451979637146\n",
      "  batch 15 loss: 1.0840920209884644\n",
      "LOSS train 1.0840920209884644 valid 1.3148283958435059\n",
      "ACCURACY train: 0.75 valid 0.0\n",
      "EPOCH 99:\n",
      "  batch 1 loss: 1.1999307870864868\n",
      "  batch 2 loss: 1.0293246507644653\n",
      "  batch 3 loss: 0.8855047225952148\n",
      "  batch 4 loss: 1.2233202457427979\n",
      "  batch 5 loss: 1.0341050624847412\n",
      "  batch 6 loss: 0.9751666784286499\n",
      "  batch 7 loss: 1.1657646894454956\n",
      "  batch 8 loss: 1.0781484842300415\n",
      "  batch 9 loss: 1.2810040712356567\n",
      "  batch 10 loss: 1.1455029249191284\n",
      "  batch 11 loss: 1.1292521953582764\n",
      "  batch 12 loss: 0.949924886226654\n",
      "  batch 13 loss: 1.276949167251587\n",
      "  batch 14 loss: 0.841277003288269\n",
      "  batch 15 loss: 1.4582414627075195\n",
      "LOSS train 1.4582414627075195 valid 1.3098845481872559\n",
      "ACCURACY train: nan valid 0.0\n"
     ]
    }
   ],
   "source": [
    "train_test_loop(\"mlp_sgd\", model, writer, train_dataloader, test_dataloader, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e8ccfe-5fb3-456a-b4ae-c198f20d9f92",
   "metadata": {},
   "source": [
    "## Using Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67926bec-e447-4440-a947-551e29594d8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T07:21:56.947654Z",
     "iopub.status.busy": "2024-02-18T07:21:56.947253Z",
     "iopub.status.idle": "2024-02-18T07:21:56.953820Z",
     "shell.execute_reply": "2024-02-18T07:21:56.952978Z",
     "shell.execute_reply.started": "2024-02-18T07:21:56.947631Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (dense): Linear(in_features=1440, out_features=4, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "009adb8e-d0d3-4a63-9640-10548faaccb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T07:21:56.955733Z",
     "iopub.status.busy": "2024-02-18T07:21:56.955060Z",
     "iopub.status.idle": "2024-02-18T07:21:56.958960Z",
     "shell.execute_reply": "2024-02-18T07:21:56.958362Z",
     "shell.execute_reply.started": "2024-02-18T07:21:56.955696Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89b81b25-61b0-48ef-afa2-3a74b0dd6eab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T07:21:56.960062Z",
     "iopub.status.busy": "2024-02-18T07:21:56.959806Z",
     "iopub.status.idle": "2024-02-18T07:21:56.966209Z",
     "shell.execute_reply": "2024-02-18T07:21:56.964911Z",
     "shell.execute_reply.started": "2024-02-18T07:21:56.960037Z"
    }
   },
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/mlp_adam_{}'.format(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0caaa83-2245-4884-8573-6442b8a503b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T07:21:56.968903Z",
     "iopub.status.busy": "2024-02-18T07:21:56.968560Z",
     "iopub.status.idle": "2024-02-18T07:22:16.158289Z",
     "shell.execute_reply": "2024-02-18T07:22:16.157705Z",
     "shell.execute_reply.started": "2024-02-18T07:21:56.968883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0:\n",
      "  batch 1 loss: 1.4202218055725098\n",
      "  batch 2 loss: 1.4354478120803833\n",
      "  batch 3 loss: 1.2075896263122559\n",
      "  batch 4 loss: 1.2415415048599243\n",
      "  batch 5 loss: 1.4462474584579468\n",
      "  batch 6 loss: 1.403373122215271\n",
      "  batch 7 loss: 1.4433950185775757\n",
      "  batch 8 loss: 1.4856009483337402\n",
      "  batch 9 loss: 1.4312715530395508\n",
      "  batch 10 loss: 1.5799949169158936\n",
      "  batch 11 loss: 1.3574556112289429\n",
      "  batch 12 loss: 1.516857385635376\n",
      "  batch 13 loss: 1.500300407409668\n",
      "  batch 14 loss: 1.5955536365509033\n",
      "  batch 15 loss: 1.5199995040893555\n",
      "LOSS train 1.5199995040893555 valid 1.5118473768234253\n",
      "ACCURACY train: 0.125 valid 0.0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 1.207152009010315\n",
      "  batch 2 loss: 1.2318730354309082\n",
      "  batch 3 loss: 1.3591103553771973\n",
      "  batch 4 loss: 1.4807466268539429\n",
      "  batch 5 loss: 1.2847745418548584\n",
      "  batch 6 loss: 1.497134804725647\n",
      "  batch 7 loss: 1.3608430624008179\n",
      "  batch 8 loss: 1.5404229164123535\n",
      "  batch 9 loss: 1.3645076751708984\n",
      "  batch 10 loss: 1.4598866701126099\n",
      "  batch 11 loss: 1.1969292163848877\n",
      "  batch 12 loss: 1.3874834775924683\n",
      "  batch 13 loss: 1.3823163509368896\n",
      "  batch 14 loss: 1.3215258121490479\n",
      "  batch 15 loss: 1.3119388818740845\n",
      "LOSS train 1.3119388818740845 valid 1.514918565750122\n",
      "ACCURACY train: 0.375 valid 0.0\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.4300975799560547\n",
      "  batch 2 loss: 1.4241427183151245\n",
      "  batch 3 loss: 1.1121141910552979\n",
      "  batch 4 loss: 1.2185471057891846\n",
      "  batch 5 loss: 1.270754337310791\n",
      "  batch 6 loss: 1.4230185747146606\n",
      "  batch 7 loss: 1.1421676874160767\n",
      "  batch 8 loss: 1.2897870540618896\n",
      "  batch 9 loss: 1.2010754346847534\n",
      "  batch 10 loss: 1.6161525249481201\n",
      "  batch 11 loss: 1.2698686122894287\n",
      "  batch 12 loss: 1.500831961631775\n",
      "  batch 13 loss: 1.4247474670410156\n",
      "  batch 14 loss: 1.2735694646835327\n",
      "  batch 15 loss: 1.6117223501205444\n",
      "LOSS train 1.6117223501205444 valid 1.5068094730377197\n",
      "ACCURACY train: 0.5 valid 0.0\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.5704452991485596\n",
      "  batch 2 loss: 1.1520178318023682\n",
      "  batch 3 loss: 1.3706042766571045\n",
      "  batch 4 loss: 1.3701430559158325\n",
      "  batch 5 loss: 1.3783477544784546\n",
      "  batch 6 loss: 1.35860276222229\n",
      "  batch 7 loss: 1.2531098127365112\n",
      "  batch 8 loss: 1.1140540838241577\n",
      "  batch 9 loss: 1.4110629558563232\n",
      "  batch 10 loss: 1.4350886344909668\n",
      "  batch 11 loss: 1.185893177986145\n",
      "  batch 12 loss: 1.180963397026062\n",
      "  batch 13 loss: 1.3332127332687378\n",
      "  batch 14 loss: 1.3831478357315063\n",
      "  batch 15 loss: 1.1811224222183228\n",
      "LOSS train 1.1811224222183228 valid 1.4846609830856323\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.3457002639770508\n",
      "  batch 2 loss: 1.4576407670974731\n",
      "  batch 3 loss: 1.2274491786956787\n",
      "  batch 4 loss: 1.38358473777771\n",
      "  batch 5 loss: 1.4388110637664795\n",
      "  batch 6 loss: 1.3169691562652588\n",
      "  batch 7 loss: 1.0309793949127197\n",
      "  batch 8 loss: 1.0953890085220337\n",
      "  batch 9 loss: 1.4534722566604614\n",
      "  batch 10 loss: 1.2742739915847778\n",
      "  batch 11 loss: 1.2932898998260498\n",
      "  batch 12 loss: 1.3144781589508057\n",
      "  batch 13 loss: 1.3432917594909668\n",
      "  batch 14 loss: 1.2914875745773315\n",
      "  batch 15 loss: 1.1822913885116577\n",
      "LOSS train 1.1822913885116577 valid 1.4958866834640503\n",
      "ACCURACY train: 0.375 valid 0.5\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.3815468549728394\n",
      "  batch 2 loss: 1.2044109106063843\n",
      "  batch 3 loss: 0.9579212069511414\n",
      "  batch 4 loss: 1.1988370418548584\n",
      "  batch 5 loss: 1.235445499420166\n",
      "  batch 6 loss: 1.366072416305542\n",
      "  batch 7 loss: 1.218887448310852\n",
      "  batch 8 loss: 1.3195960521697998\n",
      "  batch 9 loss: 1.3233997821807861\n",
      "  batch 10 loss: 1.3356719017028809\n",
      "  batch 11 loss: 1.458638072013855\n",
      "  batch 12 loss: 1.4842902421951294\n",
      "  batch 13 loss: 1.4214566946029663\n",
      "  batch 14 loss: 1.1184343099594116\n",
      "  batch 15 loss: 1.563530445098877\n",
      "LOSS train 1.563530445098877 valid 1.4672152996063232\n",
      "ACCURACY train: 0.375 valid 0.0\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.16594398021698\n",
      "  batch 2 loss: 1.2585598230361938\n",
      "  batch 3 loss: 1.120225191116333\n",
      "  batch 4 loss: 1.44140625\n",
      "  batch 5 loss: 1.1143909692764282\n",
      "  batch 6 loss: 1.1519715785980225\n",
      "  batch 7 loss: 1.3067163228988647\n",
      "  batch 8 loss: 1.506962776184082\n",
      "  batch 9 loss: 1.3154025077819824\n",
      "  batch 10 loss: 1.381471037864685\n",
      "  batch 11 loss: 1.2401856184005737\n",
      "  batch 12 loss: 1.2401230335235596\n",
      "  batch 13 loss: 1.358950138092041\n",
      "  batch 14 loss: 1.4408210515975952\n",
      "  batch 15 loss: 1.1546368598937988\n",
      "LOSS train 1.1546368598937988 valid 1.4347103834152222\n",
      "ACCURACY train: 0.375 valid 0.0\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.2838448286056519\n",
      "  batch 2 loss: 1.461343765258789\n",
      "  batch 3 loss: 1.3477048873901367\n",
      "  batch 4 loss: 1.4418976306915283\n",
      "  batch 5 loss: 1.2863565683364868\n",
      "  batch 6 loss: 1.4009034633636475\n",
      "  batch 7 loss: 1.2322710752487183\n",
      "  batch 8 loss: 1.2086851596832275\n",
      "  batch 9 loss: 1.224217414855957\n",
      "  batch 10 loss: 1.5196082592010498\n",
      "  batch 11 loss: 1.042102336883545\n",
      "  batch 12 loss: 1.124243140220642\n",
      "  batch 13 loss: 1.1004633903503418\n",
      "  batch 14 loss: 1.2918355464935303\n",
      "  batch 15 loss: 1.4475860595703125\n",
      "LOSS train 1.4475860595703125 valid 1.3849526643753052\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.2503217458724976\n",
      "  batch 2 loss: 1.0747865438461304\n",
      "  batch 3 loss: 1.2982584238052368\n",
      "  batch 4 loss: 1.2967146635055542\n",
      "  batch 5 loss: 1.1156816482543945\n",
      "  batch 6 loss: 1.2619860172271729\n",
      "  batch 7 loss: 1.2419800758361816\n",
      "  batch 8 loss: 1.445556640625\n",
      "  batch 9 loss: 1.3175616264343262\n",
      "  batch 10 loss: 1.4092150926589966\n",
      "  batch 11 loss: 1.2709327936172485\n",
      "  batch 12 loss: 1.200649619102478\n",
      "  batch 13 loss: 1.295759677886963\n",
      "  batch 14 loss: 1.2984851598739624\n",
      "  batch 15 loss: 1.3699325323104858\n",
      "LOSS train 1.3699325323104858 valid 1.4148842096328735\n",
      "ACCURACY train: 0.5 valid 0.0\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 0.9242945313453674\n",
      "  batch 2 loss: 1.4376882314682007\n",
      "  batch 3 loss: 1.3441364765167236\n",
      "  batch 4 loss: 0.9157543182373047\n",
      "  batch 5 loss: 1.4438378810882568\n",
      "  batch 6 loss: 1.2731996774673462\n",
      "  batch 7 loss: 1.2387282848358154\n",
      "  batch 8 loss: 1.2985455989837646\n",
      "  batch 9 loss: 1.3650572299957275\n",
      "  batch 10 loss: 1.0216180086135864\n",
      "  batch 11 loss: 1.165677785873413\n",
      "  batch 12 loss: 1.4602948427200317\n",
      "  batch 13 loss: 1.362402319908142\n",
      "  batch 14 loss: 1.263034462928772\n",
      "  batch 15 loss: 1.336583137512207\n",
      "LOSS train 1.336583137512207 valid 1.3992624282836914\n",
      "ACCURACY train: 0.5 valid 1.0\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.2646981477737427\n",
      "  batch 2 loss: 1.3419108390808105\n",
      "  batch 3 loss: 1.3097853660583496\n",
      "  batch 4 loss: 1.1953415870666504\n",
      "  batch 5 loss: 1.159199595451355\n",
      "  batch 6 loss: 1.179972767829895\n",
      "  batch 7 loss: 1.38951575756073\n",
      "  batch 8 loss: 1.0914525985717773\n",
      "  batch 9 loss: 1.2238225936889648\n",
      "  batch 10 loss: 1.0813730955123901\n",
      "  batch 11 loss: 1.2871909141540527\n",
      "  batch 12 loss: 1.335040807723999\n",
      "  batch 13 loss: 1.2060120105743408\n",
      "  batch 14 loss: 1.2051304578781128\n",
      "  batch 15 loss: 1.7121011018753052\n",
      "LOSS train 1.7121011018753052 valid 1.3936381340026855\n",
      "ACCURACY train: 0.375 valid 0.0\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 1.1592087745666504\n",
      "  batch 2 loss: 1.130795955657959\n",
      "  batch 3 loss: 1.0689014196395874\n",
      "  batch 4 loss: 1.4320603609085083\n",
      "  batch 5 loss: 1.4084787368774414\n",
      "  batch 6 loss: 1.052058219909668\n",
      "  batch 7 loss: 1.1629928350448608\n",
      "  batch 8 loss: 1.1643497943878174\n",
      "  batch 9 loss: 1.3444406986236572\n",
      "  batch 10 loss: 1.0674784183502197\n",
      "  batch 11 loss: 1.4048408269882202\n",
      "  batch 12 loss: 1.5012203454971313\n",
      "  batch 13 loss: 1.3154656887054443\n",
      "  batch 14 loss: 1.412556529045105\n",
      "  batch 15 loss: 1.0325151681900024\n",
      "LOSS train 1.0325151681900024 valid 1.4529166221618652\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 1.2718039751052856\n",
      "  batch 2 loss: 1.184619665145874\n",
      "  batch 3 loss: 1.3148726224899292\n",
      "  batch 4 loss: 1.1338365077972412\n",
      "  batch 5 loss: 1.224417805671692\n",
      "  batch 6 loss: 1.2417327165603638\n",
      "  batch 7 loss: 1.0580586194992065\n",
      "  batch 8 loss: 1.2598609924316406\n",
      "  batch 9 loss: 1.384461760520935\n",
      "  batch 10 loss: 1.1877883672714233\n",
      "  batch 11 loss: 1.3964420557022095\n",
      "  batch 12 loss: 1.2192139625549316\n",
      "  batch 13 loss: 0.9909467101097107\n",
      "  batch 14 loss: 1.2498270273208618\n",
      "  batch 15 loss: 1.3306959867477417\n",
      "LOSS train 1.3306959867477417 valid 1.389045238494873\n",
      "ACCURACY train: 0.5 valid 1.0\n",
      "EPOCH 13:\n",
      "  batch 1 loss: 1.1559557914733887\n",
      "  batch 2 loss: 1.2183855772018433\n",
      "  batch 3 loss: 1.36381196975708\n",
      "  batch 4 loss: 1.455590844154358\n",
      "  batch 5 loss: 1.3979045152664185\n",
      "  batch 6 loss: 1.2834206819534302\n",
      "  batch 7 loss: 1.1787785291671753\n",
      "  batch 8 loss: 1.2942965030670166\n",
      "  batch 9 loss: 1.1083931922912598\n",
      "  batch 10 loss: 1.13751220703125\n",
      "  batch 11 loss: 0.9535536766052246\n",
      "  batch 12 loss: 1.1685022115707397\n",
      "  batch 13 loss: 1.1594700813293457\n",
      "  batch 14 loss: 1.3393170833587646\n",
      "  batch 15 loss: 1.4014811515808105\n",
      "LOSS train 1.4014811515808105 valid 1.3861082792282104\n",
      "ACCURACY train: 0.375 valid 0.5\n",
      "EPOCH 14:\n",
      "  batch 1 loss: 1.0239882469177246\n",
      "  batch 2 loss: 1.2099648714065552\n",
      "  batch 3 loss: 1.4004122018814087\n",
      "  batch 4 loss: 1.4470499753952026\n",
      "  batch 5 loss: 1.5492987632751465\n",
      "  batch 6 loss: 1.2308781147003174\n",
      "  batch 7 loss: 1.2548503875732422\n",
      "  batch 8 loss: 1.09928560256958\n",
      "  batch 9 loss: 1.134604573249817\n",
      "  batch 10 loss: 1.0362443923950195\n",
      "  batch 11 loss: 1.150726556777954\n",
      "  batch 12 loss: 1.25436532497406\n",
      "  batch 13 loss: 1.2835721969604492\n",
      "  batch 14 loss: 1.1784452199935913\n",
      "  batch 15 loss: 0.9739921689033508\n",
      "LOSS train 0.9739921689033508 valid 1.3707926273345947\n",
      "ACCURACY train: 0.625 valid 1.0\n",
      "EPOCH 15:\n",
      "  batch 1 loss: 1.188645839691162\n",
      "  batch 2 loss: 1.491084337234497\n",
      "  batch 3 loss: 0.9776946306228638\n",
      "  batch 4 loss: 1.0133987665176392\n",
      "  batch 5 loss: 1.1752104759216309\n",
      "  batch 6 loss: 1.1472676992416382\n",
      "  batch 7 loss: 1.3819535970687866\n",
      "  batch 8 loss: 1.3385069370269775\n",
      "  batch 9 loss: 1.2335608005523682\n",
      "  batch 10 loss: 1.163893222808838\n",
      "  batch 11 loss: 1.1501343250274658\n",
      "  batch 12 loss: 1.2282403707504272\n",
      "  batch 13 loss: 1.4401909112930298\n",
      "  batch 14 loss: 1.1325815916061401\n",
      "  batch 15 loss: 1.3155111074447632\n",
      "LOSS train 1.3155111074447632 valid 1.4222230911254883\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 16:\n",
      "  batch 1 loss: 1.1323264837265015\n",
      "  batch 2 loss: 1.1218092441558838\n",
      "  batch 3 loss: 1.0386961698532104\n",
      "  batch 4 loss: 1.024586796760559\n",
      "  batch 5 loss: 1.3957090377807617\n",
      "  batch 6 loss: 1.4170470237731934\n",
      "  batch 7 loss: 1.1246941089630127\n",
      "  batch 8 loss: 1.1749427318572998\n",
      "  batch 9 loss: 1.2471280097961426\n",
      "  batch 10 loss: 1.1538809537887573\n",
      "  batch 11 loss: 1.375185251235962\n",
      "  batch 12 loss: 1.1832382678985596\n",
      "  batch 13 loss: 1.3990404605865479\n",
      "  batch 14 loss: 1.1601828336715698\n",
      "  batch 15 loss: 1.4227927923202515\n",
      "LOSS train 1.4227927923202515 valid 1.4180115461349487\n",
      "ACCURACY train: 0.625 valid 1.0\n",
      "EPOCH 17:\n",
      "  batch 1 loss: 1.1712820529937744\n",
      "  batch 2 loss: 1.3594961166381836\n",
      "  batch 3 loss: 1.0463709831237793\n",
      "  batch 4 loss: 1.5025513172149658\n",
      "  batch 5 loss: 1.517187476158142\n",
      "  batch 6 loss: 1.1302473545074463\n",
      "  batch 7 loss: 1.1205999851226807\n",
      "  batch 8 loss: 1.222294569015503\n",
      "  batch 9 loss: 1.038496732711792\n",
      "  batch 10 loss: 1.1363086700439453\n",
      "  batch 11 loss: 1.163197636604309\n",
      "  batch 12 loss: 1.0658800601959229\n",
      "  batch 13 loss: 1.0022149085998535\n",
      "  batch 14 loss: 1.3104265928268433\n",
      "  batch 15 loss: 1.4183934926986694\n",
      "LOSS train 1.4183934926986694 valid 1.3836259841918945\n",
      "ACCURACY train: 0.625 valid 0.5\n",
      "EPOCH 18:\n",
      "  batch 1 loss: 1.3718968629837036\n",
      "  batch 2 loss: 1.2441270351409912\n",
      "  batch 3 loss: 0.9841774702072144\n",
      "  batch 4 loss: 1.4839699268341064\n",
      "  batch 5 loss: 1.124337911605835\n",
      "  batch 6 loss: 1.0616106986999512\n",
      "  batch 7 loss: 1.0551488399505615\n",
      "  batch 8 loss: 1.2724826335906982\n",
      "  batch 9 loss: 1.238956093788147\n",
      "  batch 10 loss: 1.4470241069793701\n",
      "  batch 11 loss: 0.9675167798995972\n",
      "  batch 12 loss: 1.0462983846664429\n",
      "  batch 13 loss: 1.3877824544906616\n",
      "  batch 14 loss: 1.0840181112289429\n",
      "  batch 15 loss: 1.3717589378356934\n",
      "LOSS train 1.3717589378356934 valid 1.3808128833770752\n",
      "ACCURACY train: 0.75 valid 1.0\n",
      "EPOCH 19:\n",
      "  batch 1 loss: 1.4102845191955566\n",
      "  batch 2 loss: 1.2000253200531006\n",
      "  batch 3 loss: 1.1949338912963867\n",
      "  batch 4 loss: 1.1157169342041016\n",
      "  batch 5 loss: 1.3975012302398682\n",
      "  batch 6 loss: 1.1668589115142822\n",
      "  batch 7 loss: 0.9986361265182495\n",
      "  batch 8 loss: 0.9739987850189209\n",
      "  batch 9 loss: 1.2729008197784424\n",
      "  batch 10 loss: 1.3788460493087769\n",
      "  batch 11 loss: 1.3019702434539795\n",
      "  batch 12 loss: 1.3771196603775024\n",
      "  batch 13 loss: 0.9704757928848267\n",
      "  batch 14 loss: 1.0143150091171265\n",
      "  batch 15 loss: 1.231239914894104\n",
      "LOSS train 1.231239914894104 valid 1.4146100282669067\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 20:\n",
      "  batch 1 loss: 1.2911499738693237\n",
      "  batch 2 loss: 0.9706881046295166\n",
      "  batch 3 loss: 1.3012644052505493\n",
      "  batch 4 loss: 1.4779959917068481\n",
      "  batch 5 loss: 0.9226275682449341\n",
      "  batch 6 loss: 1.0135972499847412\n",
      "  batch 7 loss: 1.0818380117416382\n",
      "  batch 8 loss: 1.3959380388259888\n",
      "  batch 9 loss: 1.1767057180404663\n",
      "  batch 10 loss: 1.0353134870529175\n",
      "  batch 11 loss: 1.4623221158981323\n",
      "  batch 12 loss: 1.041198968887329\n",
      "  batch 13 loss: 1.3433910608291626\n",
      "  batch 14 loss: 1.3302996158599854\n",
      "  batch 15 loss: 1.1210333108901978\n",
      "LOSS train 1.1210333108901978 valid 1.428316354751587\n",
      "ACCURACY train: 0.5 valid 0.0\n",
      "EPOCH 21:\n",
      "  batch 1 loss: 1.0153770446777344\n",
      "  batch 2 loss: 1.2042052745819092\n",
      "  batch 3 loss: 1.1003371477127075\n",
      "  batch 4 loss: 1.1180938482284546\n",
      "  batch 5 loss: 1.1896933317184448\n",
      "  batch 6 loss: 1.3736017942428589\n",
      "  batch 7 loss: 1.2239092588424683\n",
      "  batch 8 loss: 1.1332645416259766\n",
      "  batch 9 loss: 1.4040093421936035\n",
      "  batch 10 loss: 1.1528122425079346\n",
      "  batch 11 loss: 1.3189616203308105\n",
      "  batch 12 loss: 1.3395451307296753\n",
      "  batch 13 loss: 0.9908425211906433\n",
      "  batch 14 loss: 1.1742974519729614\n",
      "  batch 15 loss: 1.3783174753189087\n",
      "LOSS train 1.3783174753189087 valid 1.408821940422058\n",
      "ACCURACY train: 0.375 valid 0.0\n",
      "EPOCH 22:\n",
      "  batch 1 loss: 1.2659189701080322\n",
      "  batch 2 loss: 0.9469705820083618\n",
      "  batch 3 loss: 1.0437114238739014\n",
      "  batch 4 loss: 1.2123193740844727\n",
      "  batch 5 loss: 1.2779712677001953\n",
      "  batch 6 loss: 1.2212371826171875\n",
      "  batch 7 loss: 1.2953531742095947\n",
      "  batch 8 loss: 1.0659043788909912\n",
      "  batch 9 loss: 1.2531282901763916\n",
      "  batch 10 loss: 1.036675214767456\n",
      "  batch 11 loss: 1.152484655380249\n",
      "  batch 12 loss: 1.4516658782958984\n",
      "  batch 13 loss: 1.347245216369629\n",
      "  batch 14 loss: 1.072264313697815\n",
      "  batch 15 loss: 1.0699769258499146\n",
      "LOSS train 1.0699769258499146 valid 1.4043385982513428\n",
      "ACCURACY train: 0.625 valid 0.0\n",
      "EPOCH 23:\n",
      "  batch 1 loss: 1.379257321357727\n",
      "  batch 2 loss: 1.235527753829956\n",
      "  batch 3 loss: 1.1048754453659058\n",
      "  batch 4 loss: 1.4593533277511597\n",
      "  batch 5 loss: 1.1136447191238403\n",
      "  batch 6 loss: 1.194178581237793\n",
      "  batch 7 loss: 1.3714566230773926\n",
      "  batch 8 loss: 1.1970300674438477\n",
      "  batch 9 loss: 1.0369220972061157\n",
      "  batch 10 loss: 1.251570701599121\n",
      "  batch 11 loss: 1.022383213043213\n",
      "  batch 12 loss: 1.083027720451355\n",
      "  batch 13 loss: 1.0178508758544922\n",
      "  batch 14 loss: 1.0217576026916504\n",
      "  batch 15 loss: 0.8560457229614258\n",
      "LOSS train 0.8560457229614258 valid 1.3905550241470337\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 24:\n",
      "  batch 1 loss: 1.0255260467529297\n",
      "  batch 2 loss: 1.3348692655563354\n",
      "  batch 3 loss: 1.0078959465026855\n",
      "  batch 4 loss: 1.1610171794891357\n",
      "  batch 5 loss: 1.4154891967773438\n",
      "  batch 6 loss: 1.2472726106643677\n",
      "  batch 7 loss: 1.2141729593276978\n",
      "  batch 8 loss: 1.1330655813217163\n",
      "  batch 9 loss: 1.3624178171157837\n",
      "  batch 10 loss: 1.0839836597442627\n",
      "  batch 11 loss: 0.9403353333473206\n",
      "  batch 12 loss: 1.24082350730896\n",
      "  batch 13 loss: 1.0597612857818604\n",
      "  batch 14 loss: 1.0580061674118042\n",
      "  batch 15 loss: 1.1022156476974487\n",
      "LOSS train 1.1022156476974487 valid 1.3866413831710815\n",
      "ACCURACY train: 0.75 valid 0.5\n",
      "EPOCH 25:\n",
      "  batch 1 loss: 1.0839207172393799\n",
      "  batch 2 loss: 1.037131905555725\n",
      "  batch 3 loss: 1.508816123008728\n",
      "  batch 4 loss: 0.9709643125534058\n",
      "  batch 5 loss: 0.8129411339759827\n",
      "  batch 6 loss: 1.3155224323272705\n",
      "  batch 7 loss: 0.8811258673667908\n",
      "  batch 8 loss: 1.3837262392044067\n",
      "  batch 9 loss: 1.0907132625579834\n",
      "  batch 10 loss: 1.3442044258117676\n",
      "  batch 11 loss: 1.0777589082717896\n",
      "  batch 12 loss: 1.237453818321228\n",
      "  batch 13 loss: 1.2297859191894531\n",
      "  batch 14 loss: 1.0749671459197998\n",
      "  batch 15 loss: 1.237513780593872\n",
      "LOSS train 1.237513780593872 valid 1.423304557800293\n",
      "ACCURACY train: 0.25 valid 0.0\n",
      "EPOCH 26:\n",
      "  batch 1 loss: 1.1870590448379517\n",
      "  batch 2 loss: 1.1903194189071655\n",
      "  batch 3 loss: 1.3041014671325684\n",
      "  batch 4 loss: 1.157619595527649\n",
      "  batch 5 loss: 1.2338054180145264\n",
      "  batch 6 loss: 1.0581835508346558\n",
      "  batch 7 loss: 1.1329635381698608\n",
      "  batch 8 loss: 1.2668726444244385\n",
      "  batch 9 loss: 1.1704736948013306\n",
      "  batch 10 loss: 0.9089714884757996\n",
      "  batch 11 loss: 1.270721673965454\n",
      "  batch 12 loss: 1.074789047241211\n",
      "  batch 13 loss: 1.2815767526626587\n",
      "  batch 14 loss: 1.0647238492965698\n",
      "  batch 15 loss: 1.4207695722579956\n",
      "LOSS train 1.4207695722579956 valid 1.3854546546936035\n",
      "ACCURACY train: 0.5 valid 0.0\n",
      "EPOCH 27:\n",
      "  batch 1 loss: 1.0399457216262817\n",
      "  batch 2 loss: 1.0342750549316406\n",
      "  batch 3 loss: 1.1860599517822266\n",
      "  batch 4 loss: 1.2074604034423828\n",
      "  batch 5 loss: 0.9369505047798157\n",
      "  batch 6 loss: 1.1143114566802979\n",
      "  batch 7 loss: 1.4375213384628296\n",
      "  batch 8 loss: 0.9970645308494568\n",
      "  batch 9 loss: 1.068069577217102\n",
      "  batch 10 loss: 1.1547685861587524\n",
      "  batch 11 loss: 0.9606599807739258\n",
      "  batch 12 loss: 1.3976181745529175\n",
      "  batch 13 loss: 1.2227554321289062\n",
      "  batch 14 loss: 1.3626872301101685\n",
      "  batch 15 loss: 0.9075586795806885\n",
      "LOSS train 0.9075586795806885 valid 1.4361193180084229\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 28:\n",
      "  batch 1 loss: 1.1455634832382202\n",
      "  batch 2 loss: 1.1944587230682373\n",
      "  batch 3 loss: 1.2051674127578735\n",
      "  batch 4 loss: 1.1816734075546265\n",
      "  batch 5 loss: 1.2760071754455566\n",
      "  batch 6 loss: 1.2978988885879517\n",
      "  batch 7 loss: 1.2349811792373657\n",
      "  batch 8 loss: 1.2054412364959717\n",
      "  batch 9 loss: 1.1465017795562744\n",
      "  batch 10 loss: 1.1466715335845947\n",
      "  batch 11 loss: 1.1383144855499268\n",
      "  batch 12 loss: 0.8990862965583801\n",
      "  batch 13 loss: 1.2120290994644165\n",
      "  batch 14 loss: 1.3177040815353394\n",
      "  batch 15 loss: 1.048218846321106\n",
      "LOSS train 1.048218846321106 valid 1.4121203422546387\n",
      "ACCURACY train: 0.75 valid 0.0\n",
      "EPOCH 29:\n",
      "  batch 1 loss: 1.1406400203704834\n",
      "  batch 2 loss: 1.2613160610198975\n",
      "  batch 3 loss: 0.9899964332580566\n",
      "  batch 4 loss: 1.0253545045852661\n",
      "  batch 5 loss: 1.060535192489624\n",
      "  batch 6 loss: 1.111364722251892\n",
      "  batch 7 loss: 1.224037528038025\n",
      "  batch 8 loss: 0.7908543348312378\n",
      "  batch 9 loss: 1.007018804550171\n",
      "  batch 10 loss: 1.0793107748031616\n",
      "  batch 11 loss: 1.347111463546753\n",
      "  batch 12 loss: 1.3265786170959473\n",
      "  batch 13 loss: 1.2080614566802979\n",
      "  batch 14 loss: 1.2659481763839722\n",
      "  batch 15 loss: 1.4134525060653687\n",
      "LOSS train 1.4134525060653687 valid 1.4095494747161865\n",
      "ACCURACY train: 0.625 valid 1.0\n",
      "EPOCH 30:\n",
      "  batch 1 loss: 0.7926351428031921\n",
      "  batch 2 loss: 1.1697208881378174\n",
      "  batch 3 loss: 1.1207469701766968\n",
      "  batch 4 loss: 1.2074129581451416\n",
      "  batch 5 loss: 1.4383105039596558\n",
      "  batch 6 loss: 0.8861955404281616\n",
      "  batch 7 loss: 0.9068811535835266\n",
      "  batch 8 loss: 1.1803592443466187\n",
      "  batch 9 loss: 1.2450037002563477\n",
      "  batch 10 loss: 1.1701881885528564\n",
      "  batch 11 loss: 1.0470417737960815\n",
      "  batch 12 loss: 1.3385584354400635\n",
      "  batch 13 loss: 1.1344373226165771\n",
      "  batch 14 loss: 1.0767627954483032\n",
      "  batch 15 loss: 1.3749603033065796\n",
      "LOSS train 1.3749603033065796 valid 1.418819785118103\n",
      "ACCURACY train: 0.625 valid 0.0\n",
      "EPOCH 31:\n",
      "  batch 1 loss: 1.1186249256134033\n",
      "  batch 2 loss: 1.1201542615890503\n",
      "  batch 3 loss: 0.9990435838699341\n",
      "  batch 4 loss: 1.2505775690078735\n",
      "  batch 5 loss: 1.236903429031372\n",
      "  batch 6 loss: 1.158484935760498\n",
      "  batch 7 loss: 0.9044461250305176\n",
      "  batch 8 loss: 0.9800087809562683\n",
      "  batch 9 loss: 1.1597306728363037\n",
      "  batch 10 loss: 1.3243165016174316\n",
      "  batch 11 loss: 1.0566600561141968\n",
      "  batch 12 loss: 1.2846649885177612\n",
      "  batch 13 loss: 1.2372040748596191\n",
      "  batch 14 loss: 1.179122805595398\n",
      "  batch 15 loss: 0.8493015766143799\n",
      "LOSS train 0.8493015766143799 valid 1.3962438106536865\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 32:\n",
      "  batch 1 loss: 0.8571510910987854\n",
      "  batch 2 loss: 1.1702958345413208\n",
      "  batch 3 loss: 0.9955528974533081\n",
      "  batch 4 loss: 1.0583630800247192\n",
      "  batch 5 loss: 1.0429378747940063\n",
      "  batch 6 loss: 0.865358293056488\n",
      "  batch 7 loss: 1.0214093923568726\n",
      "  batch 8 loss: 1.1146823167800903\n",
      "  batch 9 loss: 1.1786999702453613\n",
      "  batch 10 loss: 1.101141333580017\n",
      "  batch 11 loss: 1.3938056230545044\n",
      "  batch 12 loss: 1.2473822832107544\n",
      "  batch 13 loss: 1.3400832414627075\n",
      "  batch 14 loss: 1.1913914680480957\n",
      "  batch 15 loss: 1.5782699584960938\n",
      "LOSS train 1.5782699584960938 valid 1.379905343055725\n",
      "ACCURACY train: 1.0 valid 1.0\n",
      "EPOCH 33:\n",
      "  batch 1 loss: 1.0053449869155884\n",
      "  batch 2 loss: 1.0346906185150146\n",
      "  batch 3 loss: 1.0254886150360107\n",
      "  batch 4 loss: 1.1085801124572754\n",
      "  batch 5 loss: 1.13352632522583\n",
      "  batch 6 loss: 1.2494984865188599\n",
      "  batch 7 loss: 1.064785122871399\n",
      "  batch 8 loss: 0.9279886484146118\n",
      "  batch 9 loss: 1.4243574142456055\n",
      "  batch 10 loss: 1.088097095489502\n",
      "  batch 11 loss: 1.0207223892211914\n",
      "  batch 12 loss: 1.241530418395996\n",
      "  batch 13 loss: 1.381247639656067\n",
      "  batch 14 loss: 1.118718147277832\n",
      "  batch 15 loss: 1.3981165885925293\n",
      "LOSS train 1.3981165885925293 valid 1.4334784746170044\n",
      "ACCURACY train: 0.5 valid 0.0\n",
      "EPOCH 34:\n",
      "  batch 1 loss: 1.218902587890625\n",
      "  batch 2 loss: 1.0650991201400757\n",
      "  batch 3 loss: 0.9772972464561462\n",
      "  batch 4 loss: 0.9768717288970947\n",
      "  batch 5 loss: 0.9534648656845093\n",
      "  batch 6 loss: 0.916989803314209\n",
      "  batch 7 loss: 1.0047624111175537\n",
      "  batch 8 loss: 1.2416925430297852\n",
      "  batch 9 loss: 1.0814696550369263\n",
      "  batch 10 loss: 1.2788496017456055\n",
      "  batch 11 loss: 1.1782655715942383\n",
      "  batch 12 loss: 1.2338982820510864\n",
      "  batch 13 loss: 1.1655213832855225\n",
      "  batch 14 loss: 1.2682735919952393\n",
      "  batch 15 loss: 1.2646241188049316\n",
      "LOSS train 1.2646241188049316 valid 1.4075531959533691\n",
      "ACCURACY train: 0.5 valid 1.0\n",
      "EPOCH 35:\n",
      "  batch 1 loss: 1.1913011074066162\n",
      "  batch 2 loss: 1.3045744895935059\n",
      "  batch 3 loss: 1.234933853149414\n",
      "  batch 4 loss: 1.1459202766418457\n",
      "  batch 5 loss: 1.1407039165496826\n",
      "  batch 6 loss: 1.2405396699905396\n",
      "  batch 7 loss: 0.9932941198348999\n",
      "  batch 8 loss: 1.0938565731048584\n",
      "  batch 9 loss: 0.8518621325492859\n",
      "  batch 10 loss: 1.1366956233978271\n",
      "  batch 11 loss: 1.2997461557388306\n",
      "  batch 12 loss: 0.7986928820610046\n",
      "  batch 13 loss: 0.9981147050857544\n",
      "  batch 14 loss: 1.1677528619766235\n",
      "  batch 15 loss: 1.0816701650619507\n",
      "LOSS train 1.0816701650619507 valid 1.4158170223236084\n",
      "ACCURACY train: nan valid 0.5\n",
      "EPOCH 36:\n",
      "  batch 1 loss: 0.887550413608551\n",
      "  batch 2 loss: 1.020183801651001\n",
      "  batch 3 loss: 1.2023035287857056\n",
      "  batch 4 loss: 1.2015498876571655\n",
      "  batch 5 loss: 1.025608777999878\n",
      "  batch 6 loss: 1.2722617387771606\n",
      "  batch 7 loss: 0.9843817949295044\n",
      "  batch 8 loss: 1.1172763109207153\n",
      "  batch 9 loss: 1.0508348941802979\n",
      "  batch 10 loss: 1.3843194246292114\n",
      "  batch 11 loss: 0.9772046208381653\n",
      "  batch 12 loss: 1.0123251676559448\n",
      "  batch 13 loss: 1.1550503969192505\n",
      "  batch 14 loss: 1.174351453781128\n",
      "  batch 15 loss: 1.4344897270202637\n",
      "LOSS train 1.4344897270202637 valid 1.4157919883728027\n",
      "ACCURACY train: 0.875 valid 1.0\n",
      "EPOCH 37:\n",
      "  batch 1 loss: 1.1612656116485596\n",
      "  batch 2 loss: 0.9653739929199219\n",
      "  batch 3 loss: 1.515291452407837\n",
      "  batch 4 loss: 1.0261448621749878\n",
      "  batch 5 loss: 0.9386336803436279\n",
      "  batch 6 loss: 0.9173430800437927\n",
      "  batch 7 loss: 1.1087483167648315\n",
      "  batch 8 loss: 1.2215476036071777\n",
      "  batch 9 loss: 0.8457502126693726\n",
      "  batch 10 loss: 1.1444659233093262\n",
      "  batch 11 loss: 1.00117826461792\n",
      "  batch 12 loss: 1.1567440032958984\n",
      "  batch 13 loss: 1.3223950862884521\n",
      "  batch 14 loss: 1.1710461378097534\n",
      "  batch 15 loss: 0.9593914151191711\n",
      "LOSS train 0.9593914151191711 valid 1.3925890922546387\n",
      "ACCURACY train: 0.875 valid 0.5\n",
      "EPOCH 38:\n",
      "  batch 1 loss: 1.2781916856765747\n",
      "  batch 2 loss: 1.162309169769287\n",
      "  batch 3 loss: 0.9871582984924316\n",
      "  batch 4 loss: 1.1493698358535767\n",
      "  batch 5 loss: 1.1710784435272217\n",
      "  batch 6 loss: 1.3650418519973755\n",
      "  batch 7 loss: 0.9493999481201172\n",
      "  batch 8 loss: 1.1474566459655762\n",
      "  batch 9 loss: 0.933436393737793\n",
      "  batch 10 loss: 1.1658408641815186\n",
      "  batch 11 loss: 1.1717733144760132\n",
      "  batch 12 loss: 1.3082842826843262\n",
      "  batch 13 loss: 0.8249517679214478\n",
      "  batch 14 loss: 1.2879321575164795\n",
      "  batch 15 loss: 1.1825696229934692\n",
      "LOSS train 1.1825696229934692 valid 1.384660005569458\n",
      "ACCURACY train: 0.5 valid 1.0\n",
      "EPOCH 39:\n",
      "  batch 1 loss: 0.9693872928619385\n",
      "  batch 2 loss: 1.1139860153198242\n",
      "  batch 3 loss: 1.015044927597046\n",
      "  batch 4 loss: 1.1571731567382812\n",
      "  batch 5 loss: 1.281597375869751\n",
      "  batch 6 loss: 1.5059962272644043\n",
      "  batch 7 loss: 1.1424287557601929\n",
      "  batch 8 loss: 1.1646286249160767\n",
      "  batch 9 loss: 1.2997912168502808\n",
      "  batch 10 loss: 0.8878207802772522\n",
      "  batch 11 loss: 0.9679102301597595\n",
      "  batch 12 loss: 1.0660784244537354\n",
      "  batch 13 loss: 1.1648234128952026\n",
      "  batch 14 loss: 0.9203373789787292\n",
      "  batch 15 loss: 1.1419323682785034\n",
      "LOSS train 1.1419323682785034 valid 1.401401400566101\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 40:\n",
      "  batch 1 loss: 1.2083139419555664\n",
      "  batch 2 loss: 0.9316962957382202\n",
      "  batch 3 loss: 1.1891549825668335\n",
      "  batch 4 loss: 1.226737141609192\n",
      "  batch 5 loss: 1.1952474117279053\n",
      "  batch 6 loss: 1.1692804098129272\n",
      "  batch 7 loss: 0.9517937302589417\n",
      "  batch 8 loss: 1.1666479110717773\n",
      "  batch 9 loss: 1.0098789930343628\n",
      "  batch 10 loss: 1.2180507183074951\n",
      "  batch 11 loss: 1.1223413944244385\n",
      "  batch 12 loss: 0.9103472232818604\n",
      "  batch 13 loss: 1.1775381565093994\n",
      "  batch 14 loss: 1.1032938957214355\n",
      "  batch 15 loss: 1.3290448188781738\n",
      "LOSS train 1.3290448188781738 valid 1.3904120922088623\n",
      "ACCURACY train: 0.5 valid 0.5\n",
      "EPOCH 41:\n",
      "  batch 1 loss: 1.1254639625549316\n",
      "  batch 2 loss: 1.3330713510513306\n",
      "  batch 3 loss: 0.92662513256073\n",
      "  batch 4 loss: 0.8698470592498779\n",
      "  batch 5 loss: 1.0251847505569458\n",
      "  batch 6 loss: 1.2676805257797241\n",
      "  batch 7 loss: 1.218362808227539\n",
      "  batch 8 loss: 0.9305230379104614\n",
      "  batch 9 loss: 1.1366498470306396\n",
      "  batch 10 loss: 1.1177505254745483\n",
      "  batch 11 loss: 0.9904465675354004\n",
      "  batch 12 loss: 1.1999231576919556\n",
      "  batch 13 loss: 1.0397692918777466\n",
      "  batch 14 loss: 1.2312370538711548\n",
      "  batch 15 loss: 1.0398149490356445\n",
      "LOSS train 1.0398149490356445 valid 1.360721468925476\n",
      "ACCURACY train: 0.5 valid 0.0\n",
      "EPOCH 42:\n",
      "  batch 1 loss: 0.9280899167060852\n",
      "  batch 2 loss: 0.9404715895652771\n",
      "  batch 3 loss: 0.8058491945266724\n",
      "  batch 4 loss: 1.0697870254516602\n",
      "  batch 5 loss: 1.1998748779296875\n",
      "  batch 6 loss: 1.277281641960144\n",
      "  batch 7 loss: 0.9855477809906006\n",
      "  batch 8 loss: 1.4437079429626465\n",
      "  batch 9 loss: 1.1779553890228271\n",
      "  batch 10 loss: 1.1479835510253906\n",
      "  batch 11 loss: 1.086089849472046\n",
      "  batch 12 loss: 1.2991079092025757\n",
      "  batch 13 loss: 1.2581658363342285\n",
      "  batch 14 loss: 0.8761546015739441\n",
      "  batch 15 loss: 1.1824911832809448\n",
      "LOSS train 1.1824911832809448 valid 1.3918193578720093\n",
      "ACCURACY train: 0.625 valid 0.0\n",
      "EPOCH 43:\n",
      "  batch 1 loss: 0.8903282880783081\n",
      "  batch 2 loss: 1.144696593284607\n",
      "  batch 3 loss: 0.7870659232139587\n",
      "  batch 4 loss: 1.355026125907898\n",
      "  batch 5 loss: 0.9804667830467224\n",
      "  batch 6 loss: 1.2748113870620728\n",
      "  batch 7 loss: 1.0015312433242798\n",
      "  batch 8 loss: 1.2876123189926147\n",
      "  batch 9 loss: 1.118404507637024\n",
      "  batch 10 loss: 1.0162960290908813\n",
      "  batch 11 loss: 1.2005558013916016\n",
      "  batch 12 loss: 1.2407535314559937\n",
      "  batch 13 loss: 1.2960638999938965\n",
      "  batch 14 loss: 1.0112628936767578\n",
      "  batch 15 loss: 1.240822672843933\n",
      "LOSS train 1.240822672843933 valid 1.4238344430923462\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 44:\n",
      "  batch 1 loss: 1.2736330032348633\n",
      "  batch 2 loss: 1.1983087062835693\n",
      "  batch 3 loss: 0.9994508624076843\n",
      "  batch 4 loss: 1.046237587928772\n",
      "  batch 5 loss: 0.9284268021583557\n",
      "  batch 6 loss: 1.2380884885787964\n",
      "  batch 7 loss: 0.8858729600906372\n",
      "  batch 8 loss: 1.2471351623535156\n",
      "  batch 9 loss: 1.061790943145752\n",
      "  batch 10 loss: 1.0445524454116821\n",
      "  batch 11 loss: 1.1388285160064697\n",
      "  batch 12 loss: 0.9164341688156128\n",
      "  batch 13 loss: 1.2798351049423218\n",
      "  batch 14 loss: 1.3821226358413696\n",
      "  batch 15 loss: 0.7437307834625244\n",
      "LOSS train 0.7437307834625244 valid 1.4205797910690308\n",
      "ACCURACY train: 0.375 valid 0.5\n",
      "EPOCH 45:\n",
      "  batch 1 loss: 1.129868984222412\n",
      "  batch 2 loss: 1.1084566116333008\n",
      "  batch 3 loss: 1.1217167377471924\n",
      "  batch 4 loss: 1.1452795267105103\n",
      "  batch 5 loss: 1.068017601966858\n",
      "  batch 6 loss: 1.149043321609497\n",
      "  batch 7 loss: 1.2357690334320068\n",
      "  batch 8 loss: 1.136195182800293\n",
      "  batch 9 loss: 0.9615654945373535\n",
      "  batch 10 loss: 1.0888190269470215\n",
      "  batch 11 loss: 1.2492165565490723\n",
      "  batch 12 loss: 1.0238268375396729\n",
      "  batch 13 loss: 1.011695146560669\n",
      "  batch 14 loss: 1.1273894309997559\n",
      "  batch 15 loss: 0.7969048619270325\n",
      "LOSS train 0.7969048619270325 valid 1.4042980670928955\n",
      "ACCURACY train: 0.625 valid 0.0\n",
      "EPOCH 46:\n",
      "  batch 1 loss: 1.01188063621521\n",
      "  batch 2 loss: 0.8230284452438354\n",
      "  batch 3 loss: 1.015328049659729\n",
      "  batch 4 loss: 1.1265696287155151\n",
      "  batch 5 loss: 1.0755219459533691\n",
      "  batch 6 loss: 1.3401011228561401\n",
      "  batch 7 loss: 1.1518068313598633\n",
      "  batch 8 loss: 1.0101078748703003\n",
      "  batch 9 loss: 1.3845449686050415\n",
      "  batch 10 loss: 0.9865719676017761\n",
      "  batch 11 loss: 1.116626262664795\n",
      "  batch 12 loss: 1.027458906173706\n",
      "  batch 13 loss: 1.2511606216430664\n",
      "  batch 14 loss: 1.1254631280899048\n",
      "  batch 15 loss: 0.7440125346183777\n",
      "LOSS train 0.7440125346183777 valid 1.38395357131958\n",
      "ACCURACY train: 0.75 valid 0.5\n",
      "EPOCH 47:\n",
      "  batch 1 loss: 1.1524487733840942\n",
      "  batch 2 loss: 1.0432775020599365\n",
      "  batch 3 loss: 0.9013878107070923\n",
      "  batch 4 loss: 1.13904869556427\n",
      "  batch 5 loss: 1.1100062131881714\n",
      "  batch 6 loss: 1.051423192024231\n",
      "  batch 7 loss: 1.0029090642929077\n",
      "  batch 8 loss: 1.0762264728546143\n",
      "  batch 9 loss: 1.253860354423523\n",
      "  batch 10 loss: 0.9136356115341187\n",
      "  batch 11 loss: 1.124915599822998\n",
      "  batch 12 loss: 1.1145730018615723\n",
      "  batch 13 loss: 1.3280588388442993\n",
      "  batch 14 loss: 1.0201787948608398\n",
      "  batch 15 loss: 1.064360499382019\n",
      "LOSS train 1.064360499382019 valid 1.3799784183502197\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 48:\n",
      "  batch 1 loss: 1.008934736251831\n",
      "  batch 2 loss: 0.8787580132484436\n",
      "  batch 3 loss: 1.19626784324646\n",
      "  batch 4 loss: 1.0562576055526733\n",
      "  batch 5 loss: 1.070711612701416\n",
      "  batch 6 loss: 1.1272399425506592\n",
      "  batch 7 loss: 1.261217713356018\n",
      "  batch 8 loss: 1.2987526655197144\n",
      "  batch 9 loss: 0.9526779651641846\n",
      "  batch 10 loss: 1.3025201559066772\n",
      "  batch 11 loss: 1.0453500747680664\n",
      "  batch 12 loss: 1.2609292268753052\n",
      "  batch 13 loss: 0.8686566948890686\n",
      "  batch 14 loss: 1.1101919412612915\n",
      "  batch 15 loss: 0.8658850193023682\n",
      "LOSS train 0.8658850193023682 valid 1.3996875286102295\n",
      "ACCURACY train: 0.75 valid 0.0\n",
      "EPOCH 49:\n",
      "  batch 1 loss: 1.0273092985153198\n",
      "  batch 2 loss: 1.3216626644134521\n",
      "  batch 3 loss: 1.3183579444885254\n",
      "  batch 4 loss: 1.0194718837738037\n",
      "  batch 5 loss: 1.3017686605453491\n",
      "  batch 6 loss: 1.0357555150985718\n",
      "  batch 7 loss: 0.9823355674743652\n",
      "  batch 8 loss: 1.0273030996322632\n",
      "  batch 9 loss: 1.168613076210022\n",
      "  batch 10 loss: 0.9207668304443359\n",
      "  batch 11 loss: 0.945185124874115\n",
      "  batch 12 loss: 1.1330397129058838\n",
      "  batch 13 loss: 1.0134128332138062\n",
      "  batch 14 loss: 1.3346866369247437\n",
      "  batch 15 loss: 0.7513754963874817\n",
      "LOSS train 0.7513754963874817 valid 1.3937796354293823\n",
      "ACCURACY train: 0.75 valid 1.0\n",
      "EPOCH 50:\n",
      "  batch 1 loss: 1.1267783641815186\n",
      "  batch 2 loss: 1.038056492805481\n",
      "  batch 3 loss: 0.9437862038612366\n",
      "  batch 4 loss: 1.338377594947815\n",
      "  batch 5 loss: 1.0344268083572388\n",
      "  batch 6 loss: 0.8979947566986084\n",
      "  batch 7 loss: 1.1307768821716309\n",
      "  batch 8 loss: 1.320968508720398\n",
      "  batch 9 loss: 1.1599218845367432\n",
      "  batch 10 loss: 1.08130943775177\n",
      "  batch 11 loss: 0.8749026656150818\n",
      "  batch 12 loss: 1.0953397750854492\n",
      "  batch 13 loss: 1.0386979579925537\n",
      "  batch 14 loss: 1.1397020816802979\n",
      "  batch 15 loss: 1.2625871896743774\n",
      "LOSS train 1.2625871896743774 valid 1.368385672569275\n",
      "ACCURACY train: 0.875 valid 0.0\n",
      "EPOCH 51:\n",
      "  batch 1 loss: 1.3153892755508423\n",
      "  batch 2 loss: 1.1343715190887451\n",
      "  batch 3 loss: 1.1663203239440918\n",
      "  batch 4 loss: 1.1487147808074951\n",
      "  batch 5 loss: 1.3377635478973389\n",
      "  batch 6 loss: 1.2397394180297852\n",
      "  batch 7 loss: 1.088895320892334\n",
      "  batch 8 loss: 0.8960980176925659\n",
      "  batch 9 loss: 0.9586617946624756\n",
      "  batch 10 loss: 1.01482355594635\n",
      "  batch 11 loss: 1.030078649520874\n",
      "  batch 12 loss: 0.9045592546463013\n",
      "  batch 13 loss: 1.0024279356002808\n",
      "  batch 14 loss: 1.0659234523773193\n",
      "  batch 15 loss: 1.123216152191162\n",
      "LOSS train 1.123216152191162 valid 1.359208106994629\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 52:\n",
      "  batch 1 loss: 1.1195769309997559\n",
      "  batch 2 loss: 0.8889257907867432\n",
      "  batch 3 loss: 1.240220069885254\n",
      "  batch 4 loss: 1.175085425376892\n",
      "  batch 5 loss: 1.2969908714294434\n",
      "  batch 6 loss: 1.269797444343567\n",
      "  batch 7 loss: 0.9637317657470703\n",
      "  batch 8 loss: 0.949117124080658\n",
      "  batch 9 loss: 1.0379440784454346\n",
      "  batch 10 loss: 1.098101258277893\n",
      "  batch 11 loss: 1.2001521587371826\n",
      "  batch 12 loss: 0.8991928100585938\n",
      "  batch 13 loss: 1.029368281364441\n",
      "  batch 14 loss: 1.0699087381362915\n",
      "  batch 15 loss: 0.7895867228507996\n",
      "LOSS train 0.7895867228507996 valid 1.3483084440231323\n",
      "ACCURACY train: 0.625 valid 1.0\n",
      "EPOCH 53:\n",
      "  batch 1 loss: 0.820075273513794\n",
      "  batch 2 loss: 0.8840246200561523\n",
      "  batch 3 loss: 1.0871816873550415\n",
      "  batch 4 loss: 1.137342095375061\n",
      "  batch 5 loss: 1.1006076335906982\n",
      "  batch 6 loss: 1.0163816213607788\n",
      "  batch 7 loss: 1.1786963939666748\n",
      "  batch 8 loss: 1.1376380920410156\n",
      "  batch 9 loss: 0.9490160346031189\n",
      "  batch 10 loss: 1.1634145975112915\n",
      "  batch 11 loss: 1.1072320938110352\n",
      "  batch 12 loss: 0.9736250638961792\n",
      "  batch 13 loss: 1.2629570960998535\n",
      "  batch 14 loss: 1.2485240697860718\n",
      "  batch 15 loss: 1.0740514993667603\n",
      "LOSS train 1.0740514993667603 valid 1.3845956325531006\n",
      "ACCURACY train: 0.75 valid 0.5\n",
      "EPOCH 54:\n",
      "  batch 1 loss: 0.9986678957939148\n",
      "  batch 2 loss: 1.1382216215133667\n",
      "  batch 3 loss: 1.2166084051132202\n",
      "  batch 4 loss: 0.9134282469749451\n",
      "  batch 5 loss: 1.253994345664978\n",
      "  batch 6 loss: 0.9294533729553223\n",
      "  batch 7 loss: 1.1387227773666382\n",
      "  batch 8 loss: 1.0774691104888916\n",
      "  batch 9 loss: 1.1090331077575684\n",
      "  batch 10 loss: 1.244072437286377\n",
      "  batch 11 loss: 0.8087174296379089\n",
      "  batch 12 loss: 1.1299375295639038\n",
      "  batch 13 loss: 1.1659808158874512\n",
      "  batch 14 loss: 1.024958610534668\n",
      "  batch 15 loss: 1.0535494089126587\n",
      "LOSS train 1.0535494089126587 valid 1.3379323482513428\n",
      "ACCURACY train: 1.0 valid 1.0\n",
      "EPOCH 55:\n",
      "  batch 1 loss: 1.0253288745880127\n",
      "  batch 2 loss: 1.129183292388916\n",
      "  batch 3 loss: 0.9254723787307739\n",
      "  batch 4 loss: 1.0028586387634277\n",
      "  batch 5 loss: 1.245166540145874\n",
      "  batch 6 loss: 0.9979159832000732\n",
      "  batch 7 loss: 1.1415454149246216\n",
      "  batch 8 loss: 1.1974101066589355\n",
      "  batch 9 loss: 0.9997878074645996\n",
      "  batch 10 loss: 1.037213921546936\n",
      "  batch 11 loss: 1.1616662740707397\n",
      "  batch 12 loss: 1.121275544166565\n",
      "  batch 13 loss: 0.940895676612854\n",
      "  batch 14 loss: 1.2348377704620361\n",
      "  batch 15 loss: 0.7902224659919739\n",
      "LOSS train 0.7902224659919739 valid 1.3677752017974854\n",
      "ACCURACY train: nan valid 0.5\n",
      "EPOCH 56:\n",
      "  batch 1 loss: 1.047744870185852\n",
      "  batch 2 loss: 0.793580174446106\n",
      "  batch 3 loss: 1.1389446258544922\n",
      "  batch 4 loss: 1.1937936544418335\n",
      "  batch 5 loss: 1.241510272026062\n",
      "  batch 6 loss: 1.164533257484436\n",
      "  batch 7 loss: 0.9268450736999512\n",
      "  batch 8 loss: 1.2642568349838257\n",
      "  batch 9 loss: 0.7805647850036621\n",
      "  batch 10 loss: 1.0737398862838745\n",
      "  batch 11 loss: 0.8464325070381165\n",
      "  batch 12 loss: 1.092171549797058\n",
      "  batch 13 loss: 1.252855896949768\n",
      "  batch 14 loss: 1.2500829696655273\n",
      "  batch 15 loss: 1.1140828132629395\n",
      "LOSS train 1.1140828132629395 valid 1.346264123916626\n",
      "ACCURACY train: 0.75 valid 1.0\n",
      "EPOCH 57:\n",
      "  batch 1 loss: 1.1164064407348633\n",
      "  batch 2 loss: 1.0438735485076904\n",
      "  batch 3 loss: 1.1502538919448853\n",
      "  batch 4 loss: 1.3662664890289307\n",
      "  batch 5 loss: 0.8934535980224609\n",
      "  batch 6 loss: 1.2477140426635742\n",
      "  batch 7 loss: 0.8646403551101685\n",
      "  batch 8 loss: 1.1193820238113403\n",
      "  batch 9 loss: 1.2010127305984497\n",
      "  batch 10 loss: 0.876988410949707\n",
      "  batch 11 loss: 1.0061488151550293\n",
      "  batch 12 loss: 1.0847864151000977\n",
      "  batch 13 loss: 1.0452001094818115\n",
      "  batch 14 loss: 1.1034666299819946\n",
      "  batch 15 loss: 0.7475369572639465\n",
      "LOSS train 0.7475369572639465 valid 1.3484036922454834\n",
      "ACCURACY train: 0.5 valid 1.0\n",
      "EPOCH 58:\n",
      "  batch 1 loss: 1.1160496473312378\n",
      "  batch 2 loss: 1.2042407989501953\n",
      "  batch 3 loss: 0.972511887550354\n",
      "  batch 4 loss: 1.0361276865005493\n",
      "  batch 5 loss: 1.4577960968017578\n",
      "  batch 6 loss: 1.1105583906173706\n",
      "  batch 7 loss: 1.005857229232788\n",
      "  batch 8 loss: 1.0596073865890503\n",
      "  batch 9 loss: 1.1052013635635376\n",
      "  batch 10 loss: 0.9906436800956726\n",
      "  batch 11 loss: 1.129648208618164\n",
      "  batch 12 loss: 1.0178416967391968\n",
      "  batch 13 loss: 0.938542366027832\n",
      "  batch 14 loss: 0.8889821171760559\n",
      "  batch 15 loss: 1.1267656087875366\n",
      "LOSS train 1.1267656087875366 valid 1.3441332578659058\n",
      "ACCURACY train: 0.75 valid 1.0\n",
      "EPOCH 59:\n",
      "  batch 1 loss: 1.0659599304199219\n",
      "  batch 2 loss: 0.8870012164115906\n",
      "  batch 3 loss: 1.0222017765045166\n",
      "  batch 4 loss: 1.2929017543792725\n",
      "  batch 5 loss: 1.204580545425415\n",
      "  batch 6 loss: 1.0224542617797852\n",
      "  batch 7 loss: 1.0128247737884521\n",
      "  batch 8 loss: 1.2151631116867065\n",
      "  batch 9 loss: 0.955371618270874\n",
      "  batch 10 loss: 1.1436418294906616\n",
      "  batch 11 loss: 1.1492539644241333\n",
      "  batch 12 loss: 0.8403193354606628\n",
      "  batch 13 loss: 1.2299256324768066\n",
      "  batch 14 loss: 0.9714112281799316\n",
      "  batch 15 loss: 1.1285606622695923\n",
      "LOSS train 1.1285606622695923 valid 1.3569769859313965\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 60:\n",
      "  batch 1 loss: 1.2062034606933594\n",
      "  batch 2 loss: 1.0273327827453613\n",
      "  batch 3 loss: 0.8414458632469177\n",
      "  batch 4 loss: 1.0023467540740967\n",
      "  batch 5 loss: 0.9510712027549744\n",
      "  batch 6 loss: 1.142496943473816\n",
      "  batch 7 loss: 1.4697548151016235\n",
      "  batch 8 loss: 1.1328812837600708\n",
      "  batch 9 loss: 0.9323546886444092\n",
      "  batch 10 loss: 1.2052597999572754\n",
      "  batch 11 loss: 0.9329115152359009\n",
      "  batch 12 loss: 1.031481385231018\n",
      "  batch 13 loss: 0.9747990369796753\n",
      "  batch 14 loss: 1.1621506214141846\n",
      "  batch 15 loss: 0.9926632046699524\n",
      "LOSS train 0.9926632046699524 valid 1.3406181335449219\n",
      "ACCURACY train: 0.5 valid 0.0\n",
      "EPOCH 61:\n",
      "  batch 1 loss: 1.1230871677398682\n",
      "  batch 2 loss: 0.8863811492919922\n",
      "  batch 3 loss: 0.9329637289047241\n",
      "  batch 4 loss: 1.3491768836975098\n",
      "  batch 5 loss: 0.9888572692871094\n",
      "  batch 6 loss: 0.9613795280456543\n",
      "  batch 7 loss: 1.0191627740859985\n",
      "  batch 8 loss: 1.1423110961914062\n",
      "  batch 9 loss: 1.0611882209777832\n",
      "  batch 10 loss: 1.3199330568313599\n",
      "  batch 11 loss: 1.1019972562789917\n",
      "  batch 12 loss: 1.006032943725586\n",
      "  batch 13 loss: 1.0748683214187622\n",
      "  batch 14 loss: 1.0116544961929321\n",
      "  batch 15 loss: 1.0547116994857788\n",
      "LOSS train 1.0547116994857788 valid 1.3678550720214844\n",
      "ACCURACY train: 0.875 valid 0.0\n",
      "EPOCH 62:\n",
      "  batch 1 loss: 1.0279159545898438\n",
      "  batch 2 loss: 0.9868980050086975\n",
      "  batch 3 loss: 1.235748052597046\n",
      "  batch 4 loss: 1.1209161281585693\n",
      "  batch 5 loss: 1.0638734102249146\n",
      "  batch 6 loss: 0.8941665887832642\n",
      "  batch 7 loss: 0.913445234298706\n",
      "  batch 8 loss: 1.1462492942810059\n",
      "  batch 9 loss: 1.0328898429870605\n",
      "  batch 10 loss: 1.0051484107971191\n",
      "  batch 11 loss: 1.2646006345748901\n",
      "  batch 12 loss: 1.1770989894866943\n",
      "  batch 13 loss: 1.1241375207901\n",
      "  batch 14 loss: 0.9904760122299194\n",
      "  batch 15 loss: 1.4379148483276367\n",
      "LOSS train 1.4379148483276367 valid 1.3510054349899292\n",
      "ACCURACY train: 0.5 valid 0.0\n",
      "EPOCH 63:\n",
      "  batch 1 loss: 1.010019302368164\n",
      "  batch 2 loss: 1.0224483013153076\n",
      "  batch 3 loss: 1.0016683340072632\n",
      "  batch 4 loss: 1.1300336122512817\n",
      "  batch 5 loss: 1.1811491250991821\n",
      "  batch 6 loss: 1.0448987483978271\n",
      "  batch 7 loss: 1.0221225023269653\n",
      "  batch 8 loss: 1.0496160984039307\n",
      "  batch 9 loss: 1.140972375869751\n",
      "  batch 10 loss: 0.975095808506012\n",
      "  batch 11 loss: 1.1319631338119507\n",
      "  batch 12 loss: 1.0366809368133545\n",
      "  batch 13 loss: 1.1877386569976807\n",
      "  batch 14 loss: 1.1237876415252686\n",
      "  batch 15 loss: 1.1312389373779297\n",
      "LOSS train 1.1312389373779297 valid 1.3578544855117798\n",
      "ACCURACY train: nan valid 1.0\n",
      "EPOCH 64:\n",
      "  batch 1 loss: 1.0705357789993286\n",
      "  batch 2 loss: 1.2045097351074219\n",
      "  batch 3 loss: 0.8876852989196777\n",
      "  batch 4 loss: 1.2263110876083374\n",
      "  batch 5 loss: 1.320609211921692\n",
      "  batch 6 loss: 1.0669238567352295\n",
      "  batch 7 loss: 1.1494203805923462\n",
      "  batch 8 loss: 1.1386029720306396\n",
      "  batch 9 loss: 1.043792486190796\n",
      "  batch 10 loss: 1.0255182981491089\n",
      "  batch 11 loss: 0.9877792596817017\n",
      "  batch 12 loss: 0.902409553527832\n",
      "  batch 13 loss: 1.0025229454040527\n",
      "  batch 14 loss: 0.8982802629470825\n",
      "  batch 15 loss: 1.0757745504379272\n",
      "LOSS train 1.0757745504379272 valid 1.3581985235214233\n",
      "ACCURACY train: 0.75 valid 0.5\n",
      "EPOCH 65:\n",
      "  batch 1 loss: 1.1148490905761719\n",
      "  batch 2 loss: 0.8802757263183594\n",
      "  batch 3 loss: 1.236854076385498\n",
      "  batch 4 loss: 0.8869332671165466\n",
      "  batch 5 loss: 0.9010541439056396\n",
      "  batch 6 loss: 0.854171633720398\n",
      "  batch 7 loss: 0.9317916035652161\n",
      "  batch 8 loss: 1.1553139686584473\n",
      "  batch 9 loss: 1.0643872022628784\n",
      "  batch 10 loss: 1.5526609420776367\n",
      "  batch 11 loss: 1.2687709331512451\n",
      "  batch 12 loss: 1.2299573421478271\n",
      "  batch 13 loss: 0.8987638354301453\n",
      "  batch 14 loss: 0.752636194229126\n",
      "  batch 15 loss: 1.4086227416992188\n",
      "LOSS train 1.4086227416992188 valid 1.350327730178833\n",
      "ACCURACY train: 0.875 valid 0.0\n",
      "EPOCH 66:\n",
      "  batch 1 loss: 0.8142556548118591\n",
      "  batch 2 loss: 1.258897066116333\n",
      "  batch 3 loss: 0.8208461403846741\n",
      "  batch 4 loss: 1.2391247749328613\n",
      "  batch 5 loss: 0.7586172819137573\n",
      "  batch 6 loss: 1.1480202674865723\n",
      "  batch 7 loss: 1.231570839881897\n",
      "  batch 8 loss: 1.233283519744873\n",
      "  batch 9 loss: 1.1375458240509033\n",
      "  batch 10 loss: 1.135921597480774\n",
      "  batch 11 loss: 1.1526765823364258\n",
      "  batch 12 loss: 0.8572469353675842\n",
      "  batch 13 loss: 1.1165707111358643\n",
      "  batch 14 loss: 1.0292218923568726\n",
      "  batch 15 loss: 0.820594310760498\n",
      "LOSS train 0.820594310760498 valid 1.3527495861053467\n",
      "ACCURACY train: 0.625 valid 0.0\n",
      "EPOCH 67:\n",
      "  batch 1 loss: 1.1312599182128906\n",
      "  batch 2 loss: 1.1259496212005615\n",
      "  batch 3 loss: 1.0099709033966064\n",
      "  batch 4 loss: 1.0098767280578613\n",
      "  batch 5 loss: 1.1197543144226074\n",
      "  batch 6 loss: 1.162126064300537\n",
      "  batch 7 loss: 1.2299288511276245\n",
      "  batch 8 loss: 1.1734914779663086\n",
      "  batch 9 loss: 0.8932229280471802\n",
      "  batch 10 loss: 1.1871085166931152\n",
      "  batch 11 loss: 0.7807895541191101\n",
      "  batch 12 loss: 1.0229426622390747\n",
      "  batch 13 loss: 1.0074337720870972\n",
      "  batch 14 loss: 1.1296086311340332\n",
      "  batch 15 loss: 0.7500166893005371\n",
      "LOSS train 0.7500166893005371 valid 1.3578405380249023\n",
      "ACCURACY train: nan valid 1.0\n",
      "EPOCH 68:\n",
      "  batch 1 loss: 0.915718674659729\n",
      "  batch 2 loss: 1.0474406480789185\n",
      "  batch 3 loss: 0.9948967099189758\n",
      "  batch 4 loss: 1.2761280536651611\n",
      "  batch 5 loss: 1.1710158586502075\n",
      "  batch 6 loss: 1.1310398578643799\n",
      "  batch 7 loss: 1.2107915878295898\n",
      "  batch 8 loss: 0.9308885335922241\n",
      "  batch 9 loss: 1.1196050643920898\n",
      "  batch 10 loss: 0.890161395072937\n",
      "  batch 11 loss: 0.9882793426513672\n",
      "  batch 12 loss: 1.01466965675354\n",
      "  batch 13 loss: 1.0380100011825562\n",
      "  batch 14 loss: 0.9970765113830566\n",
      "  batch 15 loss: 1.4997235536575317\n",
      "LOSS train 1.4997235536575317 valid 1.354575276374817\n",
      "ACCURACY train: 0.875 valid 0.0\n",
      "EPOCH 69:\n",
      "  batch 1 loss: 1.2026569843292236\n",
      "  batch 2 loss: 1.054973840713501\n",
      "  batch 3 loss: 1.001497745513916\n",
      "  batch 4 loss: 0.8748953938484192\n",
      "  batch 5 loss: 1.237993836402893\n",
      "  batch 6 loss: 0.9134660363197327\n",
      "  batch 7 loss: 0.8721456527709961\n",
      "  batch 8 loss: 1.1414989233016968\n",
      "  batch 9 loss: 1.1483430862426758\n",
      "  batch 10 loss: 1.0477354526519775\n",
      "  batch 11 loss: 1.0275484323501587\n",
      "  batch 12 loss: 1.1094151735305786\n",
      "  batch 13 loss: 1.1139436960220337\n",
      "  batch 14 loss: 1.0678291320800781\n",
      "  batch 15 loss: 1.393828272819519\n",
      "LOSS train 1.393828272819519 valid 1.3381575345993042\n",
      "ACCURACY train: 0.875 valid 1.0\n",
      "EPOCH 70:\n",
      "  batch 1 loss: 1.141146183013916\n",
      "  batch 2 loss: 0.9778204560279846\n",
      "  batch 3 loss: 0.8862922787666321\n",
      "  batch 4 loss: 1.2216224670410156\n",
      "  batch 5 loss: 1.0289404392242432\n",
      "  batch 6 loss: 1.116424322128296\n",
      "  batch 7 loss: 1.1571438312530518\n",
      "  batch 8 loss: 1.1653555631637573\n",
      "  batch 9 loss: 0.9316695332527161\n",
      "  batch 10 loss: 0.9321802854537964\n",
      "  batch 11 loss: 1.0184884071350098\n",
      "  batch 12 loss: 0.9493106603622437\n",
      "  batch 13 loss: 1.1266040802001953\n",
      "  batch 14 loss: 1.1604080200195312\n",
      "  batch 15 loss: 1.1014776229858398\n",
      "LOSS train 1.1014776229858398 valid 1.3300879001617432\n",
      "ACCURACY train: 0.75 valid 0.0\n",
      "EPOCH 71:\n",
      "  batch 1 loss: 1.038644552230835\n",
      "  batch 2 loss: 1.1320513486862183\n",
      "  batch 3 loss: 0.8907129168510437\n",
      "  batch 4 loss: 1.117315649986267\n",
      "  batch 5 loss: 1.182424545288086\n",
      "  batch 6 loss: 1.0898789167404175\n",
      "  batch 7 loss: 0.7694210410118103\n",
      "  batch 8 loss: 1.1213653087615967\n",
      "  batch 9 loss: 1.3527127504348755\n",
      "  batch 10 loss: 0.9837139248847961\n",
      "  batch 11 loss: 1.0790210962295532\n",
      "  batch 12 loss: 1.141188144683838\n",
      "  batch 13 loss: 1.073277473449707\n",
      "  batch 14 loss: 0.8907036185264587\n",
      "  batch 15 loss: 1.1047823429107666\n",
      "LOSS train 1.1047823429107666 valid 1.3471503257751465\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 72:\n",
      "  batch 1 loss: 1.0017744302749634\n",
      "  batch 2 loss: 1.116042971611023\n",
      "  batch 3 loss: 1.1039254665374756\n",
      "  batch 4 loss: 0.96342933177948\n",
      "  batch 5 loss: 1.1350817680358887\n",
      "  batch 6 loss: 1.4771865606307983\n",
      "  batch 7 loss: 1.101989984512329\n",
      "  batch 8 loss: 1.0010974407196045\n",
      "  batch 9 loss: 0.8922918438911438\n",
      "  batch 10 loss: 1.0449634790420532\n",
      "  batch 11 loss: 0.920230507850647\n",
      "  batch 12 loss: 0.9130538702011108\n",
      "  batch 13 loss: 1.0852575302124023\n",
      "  batch 14 loss: 0.9864739179611206\n",
      "  batch 15 loss: 1.409852385520935\n",
      "LOSS train 1.409852385520935 valid 1.339444637298584\n",
      "ACCURACY train: 0.75 valid 0.0\n",
      "EPOCH 73:\n",
      "  batch 1 loss: 1.1033962965011597\n",
      "  batch 2 loss: 0.7831178307533264\n",
      "  batch 3 loss: 1.0173397064208984\n",
      "  batch 4 loss: 1.117379903793335\n",
      "  batch 5 loss: 1.2505334615707397\n",
      "  batch 6 loss: 1.1046628952026367\n",
      "  batch 7 loss: 0.897635817527771\n",
      "  batch 8 loss: 1.1239362955093384\n",
      "  batch 9 loss: 0.7843798995018005\n",
      "  batch 10 loss: 1.1424570083618164\n",
      "  batch 11 loss: 1.051321029663086\n",
      "  batch 12 loss: 0.9107769131660461\n",
      "  batch 13 loss: 1.3923814296722412\n",
      "  batch 14 loss: 1.1045184135437012\n",
      "  batch 15 loss: 1.076254963874817\n",
      "LOSS train 1.076254963874817 valid 1.359695553779602\n",
      "ACCURACY train: 0.625 valid 0.5\n",
      "EPOCH 74:\n",
      "  batch 1 loss: 0.9237727522850037\n",
      "  batch 2 loss: 0.7738884687423706\n",
      "  batch 3 loss: 1.0236499309539795\n",
      "  batch 4 loss: 0.9947761297225952\n",
      "  batch 5 loss: 1.0500494241714478\n",
      "  batch 6 loss: 0.9097283482551575\n",
      "  batch 7 loss: 1.1423511505126953\n",
      "  batch 8 loss: 1.1072051525115967\n",
      "  batch 9 loss: 1.1361974477767944\n",
      "  batch 10 loss: 1.2433477640151978\n",
      "  batch 11 loss: 1.1144232749938965\n",
      "  batch 12 loss: 1.020106554031372\n",
      "  batch 13 loss: 1.3874168395996094\n",
      "  batch 14 loss: 1.051835060119629\n",
      "  batch 15 loss: 0.8335149884223938\n",
      "LOSS train 0.8335149884223938 valid 1.334656000137329\n",
      "ACCURACY train: 0.625 valid 1.0\n",
      "EPOCH 75:\n",
      "  batch 1 loss: 1.1164647340774536\n",
      "  batch 2 loss: 1.0056854486465454\n",
      "  batch 3 loss: 1.3553277254104614\n",
      "  batch 4 loss: 1.1561013460159302\n",
      "  batch 5 loss: 1.2346056699752808\n",
      "  batch 6 loss: 1.1189426183700562\n",
      "  batch 7 loss: 1.1308302879333496\n",
      "  batch 8 loss: 1.0005708932876587\n",
      "  batch 9 loss: 0.9907972812652588\n",
      "  batch 10 loss: 0.8768683671951294\n",
      "  batch 11 loss: 0.8531389832496643\n",
      "  batch 12 loss: 1.0415928363800049\n",
      "  batch 13 loss: 0.9073085188865662\n",
      "  batch 14 loss: 0.8975054621696472\n",
      "  batch 15 loss: 1.1681177616119385\n",
      "LOSS train 1.1681177616119385 valid 1.3384270668029785\n",
      "ACCURACY train: nan valid 0.5\n",
      "EPOCH 76:\n",
      "  batch 1 loss: 1.1794887781143188\n",
      "  batch 2 loss: 0.9126719236373901\n",
      "  batch 3 loss: 1.2469661235809326\n",
      "  batch 4 loss: 0.9175983667373657\n",
      "  batch 5 loss: 1.3218960762023926\n",
      "  batch 6 loss: 1.2467330694198608\n",
      "  batch 7 loss: 1.0263532400131226\n",
      "  batch 8 loss: 0.9934982657432556\n",
      "  batch 9 loss: 1.165596604347229\n",
      "  batch 10 loss: 0.7504314184188843\n",
      "  batch 11 loss: 1.0270003080368042\n",
      "  batch 12 loss: 0.8986420631408691\n",
      "  batch 13 loss: 1.05845046043396\n",
      "  batch 14 loss: 1.1229571104049683\n",
      "  batch 15 loss: 0.7761702537536621\n",
      "LOSS train 0.7761702537536621 valid 1.3468108177185059\n",
      "ACCURACY train: 0.625 valid 1.0\n",
      "EPOCH 77:\n",
      "  batch 1 loss: 1.135519027709961\n",
      "  batch 2 loss: 1.1388417482376099\n",
      "  batch 3 loss: 1.2220749855041504\n",
      "  batch 4 loss: 0.9041861891746521\n",
      "  batch 5 loss: 1.0216401815414429\n",
      "  batch 6 loss: 1.0090404748916626\n",
      "  batch 7 loss: 1.155529260635376\n",
      "  batch 8 loss: 1.112276554107666\n",
      "  batch 9 loss: 1.0241138935089111\n",
      "  batch 10 loss: 1.2737621068954468\n",
      "  batch 11 loss: 0.8546000719070435\n",
      "  batch 12 loss: 1.0183736085891724\n",
      "  batch 13 loss: 0.9873751401901245\n",
      "  batch 14 loss: 1.0262665748596191\n",
      "  batch 15 loss: 0.7819008231163025\n",
      "LOSS train 0.7819008231163025 valid 1.3392385244369507\n",
      "ACCURACY train: 0.75 valid 0.5\n",
      "EPOCH 78:\n",
      "  batch 1 loss: 0.9859987497329712\n",
      "  batch 2 loss: 1.2753511667251587\n",
      "  batch 3 loss: 1.013230323791504\n",
      "  batch 4 loss: 1.0626453161239624\n",
      "  batch 5 loss: 1.0885207653045654\n",
      "  batch 6 loss: 1.2742271423339844\n",
      "  batch 7 loss: 0.7858695983886719\n",
      "  batch 8 loss: 0.7927067875862122\n",
      "  batch 9 loss: 1.119014859199524\n",
      "  batch 10 loss: 1.2113776206970215\n",
      "  batch 11 loss: 0.9959045052528381\n",
      "  batch 12 loss: 0.9263802766799927\n",
      "  batch 13 loss: 1.1682318449020386\n",
      "  batch 14 loss: 0.9472707509994507\n",
      "  batch 15 loss: 1.3784459829330444\n",
      "LOSS train 1.3784459829330444 valid 1.3393526077270508\n",
      "ACCURACY train: 0.75 valid 1.0\n",
      "EPOCH 79:\n",
      "  batch 1 loss: 1.2247061729431152\n",
      "  batch 2 loss: 1.2250163555145264\n",
      "  batch 3 loss: 1.0504413843154907\n",
      "  batch 4 loss: 0.9500831365585327\n",
      "  batch 5 loss: 1.1046146154403687\n",
      "  batch 6 loss: 0.9327281713485718\n",
      "  batch 7 loss: 0.997637927532196\n",
      "  batch 8 loss: 1.0168377161026\n",
      "  batch 9 loss: 1.1299515962600708\n",
      "  batch 10 loss: 1.0712313652038574\n",
      "  batch 11 loss: 0.8710921406745911\n",
      "  batch 12 loss: 1.0108792781829834\n",
      "  batch 13 loss: 1.1199334859848022\n",
      "  batch 14 loss: 1.0259721279144287\n",
      "  batch 15 loss: 1.1070083379745483\n",
      "LOSS train 1.1070083379745483 valid 1.3566899299621582\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 80:\n",
      "  batch 1 loss: 0.7961866855621338\n",
      "  batch 2 loss: 1.1573094129562378\n",
      "  batch 3 loss: 1.2406115531921387\n",
      "  batch 4 loss: 1.245430827140808\n",
      "  batch 5 loss: 1.0211111307144165\n",
      "  batch 6 loss: 1.2447047233581543\n",
      "  batch 7 loss: 0.8940423727035522\n",
      "  batch 8 loss: 1.0435020923614502\n",
      "  batch 9 loss: 1.3249300718307495\n",
      "  batch 10 loss: 1.2224602699279785\n",
      "  batch 11 loss: 0.8278924226760864\n",
      "  batch 12 loss: 0.8927277326583862\n",
      "  batch 13 loss: 0.9964989423751831\n",
      "  batch 14 loss: 0.9943671822547913\n",
      "  batch 15 loss: 0.7596322894096375\n",
      "LOSS train 0.7596322894096375 valid 1.3491005897521973\n",
      "ACCURACY train: 1.0 valid 0.0\n",
      "EPOCH 81:\n",
      "  batch 1 loss: 0.9834349155426025\n",
      "  batch 2 loss: 1.0145864486694336\n",
      "  batch 3 loss: 1.0347418785095215\n",
      "  batch 4 loss: 1.2761948108673096\n",
      "  batch 5 loss: 0.9902987480163574\n",
      "  batch 6 loss: 1.0163179636001587\n",
      "  batch 7 loss: 1.1504896879196167\n",
      "  batch 8 loss: 1.1336867809295654\n",
      "  batch 9 loss: 1.3918684720993042\n",
      "  batch 10 loss: 1.0039039850234985\n",
      "  batch 11 loss: 1.0000557899475098\n",
      "  batch 12 loss: 0.9539462924003601\n",
      "  batch 13 loss: 1.0377436876296997\n",
      "  batch 14 loss: 0.8708075881004333\n",
      "  batch 15 loss: 0.7851800322532654\n",
      "LOSS train 0.7851800322532654 valid 1.333279013633728\n",
      "ACCURACY train: 0.75 valid 0.0\n",
      "EPOCH 82:\n",
      "  batch 1 loss: 1.1171125173568726\n",
      "  batch 2 loss: 0.8823437690734863\n",
      "  batch 3 loss: 1.140586018562317\n",
      "  batch 4 loss: 1.1190298795700073\n",
      "  batch 5 loss: 0.9056896567344666\n",
      "  batch 6 loss: 1.0067089796066284\n",
      "  batch 7 loss: 1.1247490644454956\n",
      "  batch 8 loss: 0.991036593914032\n",
      "  batch 9 loss: 1.051758885383606\n",
      "  batch 10 loss: 1.0036340951919556\n",
      "  batch 11 loss: 1.365405797958374\n",
      "  batch 12 loss: 0.8985638618469238\n",
      "  batch 13 loss: 1.048818826675415\n",
      "  batch 14 loss: 1.0206775665283203\n",
      "  batch 15 loss: 1.0965944528579712\n",
      "LOSS train 1.0965944528579712 valid 1.3493144512176514\n",
      "ACCURACY train: 0.375 valid 0.0\n",
      "EPOCH 83:\n",
      "  batch 1 loss: 0.9993904232978821\n",
      "  batch 2 loss: 1.0060642957687378\n",
      "  batch 3 loss: 1.0016977787017822\n",
      "  batch 4 loss: 1.0040433406829834\n",
      "  batch 5 loss: 1.1756858825683594\n",
      "  batch 6 loss: 1.000757098197937\n",
      "  batch 7 loss: 0.9839057326316833\n",
      "  batch 8 loss: 0.900881826877594\n",
      "  batch 9 loss: 0.9965877532958984\n",
      "  batch 10 loss: 1.007077693939209\n",
      "  batch 11 loss: 1.1174014806747437\n",
      "  batch 12 loss: 1.0621209144592285\n",
      "  batch 13 loss: 1.1512778997421265\n",
      "  batch 14 loss: 1.236388921737671\n",
      "  batch 15 loss: 1.091692566871643\n",
      "LOSS train 1.091692566871643 valid 1.3376959562301636\n",
      "ACCURACY train: nan valid 1.0\n",
      "EPOCH 84:\n",
      "  batch 1 loss: 0.9979981780052185\n",
      "  batch 2 loss: 1.208589792251587\n",
      "  batch 3 loss: 1.3254798650741577\n",
      "  batch 4 loss: 1.1456005573272705\n",
      "  batch 5 loss: 1.0024735927581787\n",
      "  batch 6 loss: 1.027269959449768\n",
      "  batch 7 loss: 1.0578547716140747\n",
      "  batch 8 loss: 0.9942343235015869\n",
      "  batch 9 loss: 1.009650468826294\n",
      "  batch 10 loss: 0.9298536777496338\n",
      "  batch 11 loss: 0.913313627243042\n",
      "  batch 12 loss: 1.0380560159683228\n",
      "  batch 13 loss: 0.8947504162788391\n",
      "  batch 14 loss: 1.136115312576294\n",
      "  batch 15 loss: 1.0807989835739136\n",
      "LOSS train 1.0807989835739136 valid 1.334341287612915\n",
      "ACCURACY train: 0.75 valid 0.5\n",
      "EPOCH 85:\n",
      "  batch 1 loss: 1.1282501220703125\n",
      "  batch 2 loss: 1.0232057571411133\n",
      "  batch 3 loss: 0.9004682302474976\n",
      "  batch 4 loss: 1.0281251668930054\n",
      "  batch 5 loss: 1.0068472623825073\n",
      "  batch 6 loss: 1.1356395483016968\n",
      "  batch 7 loss: 1.0022510290145874\n",
      "  batch 8 loss: 0.7647338509559631\n",
      "  batch 9 loss: 1.3732249736785889\n",
      "  batch 10 loss: 0.9931786060333252\n",
      "  batch 11 loss: 1.2445372343063354\n",
      "  batch 12 loss: 1.106683373451233\n",
      "  batch 13 loss: 1.0207122564315796\n",
      "  batch 14 loss: 0.9001439809799194\n",
      "  batch 15 loss: 1.1331887245178223\n",
      "LOSS train 1.1331887245178223 valid 1.3438389301300049\n",
      "ACCURACY train: 0.625 valid 0.0\n",
      "EPOCH 86:\n",
      "  batch 1 loss: 1.2185735702514648\n",
      "  batch 2 loss: 0.8758354187011719\n",
      "  batch 3 loss: 0.9050008654594421\n",
      "  batch 4 loss: 0.9941740036010742\n",
      "  batch 5 loss: 1.0093941688537598\n",
      "  batch 6 loss: 1.0686901807785034\n",
      "  batch 7 loss: 1.046368956565857\n",
      "  batch 8 loss: 1.3486050367355347\n",
      "  batch 9 loss: 0.9851836562156677\n",
      "  batch 10 loss: 1.0206987857818604\n",
      "  batch 11 loss: 0.9900960326194763\n",
      "  batch 12 loss: 1.1221528053283691\n",
      "  batch 13 loss: 1.0634996891021729\n",
      "  batch 14 loss: 1.0415949821472168\n",
      "  batch 15 loss: 1.0522459745407104\n",
      "LOSS train 1.0522459745407104 valid 1.3339673280715942\n",
      "ACCURACY train: 0.75 valid 0.5\n",
      "EPOCH 87:\n",
      "  batch 1 loss: 1.2309730052947998\n",
      "  batch 2 loss: 1.00373113155365\n",
      "  batch 3 loss: 0.8762887716293335\n",
      "  batch 4 loss: 1.0212411880493164\n",
      "  batch 5 loss: 0.7662208080291748\n",
      "  batch 6 loss: 1.1437876224517822\n",
      "  batch 7 loss: 1.0033005475997925\n",
      "  batch 8 loss: 1.2628456354141235\n",
      "  batch 9 loss: 0.8992832899093628\n",
      "  batch 10 loss: 0.9189209342002869\n",
      "  batch 11 loss: 0.901473343372345\n",
      "  batch 12 loss: 1.2651004791259766\n",
      "  batch 13 loss: 0.9819740653038025\n",
      "  batch 14 loss: 1.461665153503418\n",
      "  batch 15 loss: 0.7657942175865173\n",
      "LOSS train 0.7657942175865173 valid 1.358539342880249\n",
      "ACCURACY train: nan valid 1.0\n",
      "EPOCH 88:\n",
      "  batch 1 loss: 1.0813250541687012\n",
      "  batch 2 loss: 1.134016752243042\n",
      "  batch 3 loss: 0.984660267829895\n",
      "  batch 4 loss: 0.8680904507637024\n",
      "  batch 5 loss: 0.7743287682533264\n",
      "  batch 6 loss: 1.3599746227264404\n",
      "  batch 7 loss: 1.1133103370666504\n",
      "  batch 8 loss: 1.0049962997436523\n",
      "  batch 9 loss: 0.8703305721282959\n",
      "  batch 10 loss: 1.0532466173171997\n",
      "  batch 11 loss: 1.2069467306137085\n",
      "  batch 12 loss: 1.2175865173339844\n",
      "  batch 13 loss: 0.9908034801483154\n",
      "  batch 14 loss: 1.0413438081741333\n",
      "  batch 15 loss: 1.1616358757019043\n",
      "LOSS train 1.1616358757019043 valid 1.3160282373428345\n",
      "ACCURACY train: 0.625 valid 0.0\n",
      "EPOCH 89:\n",
      "  batch 1 loss: 1.2639377117156982\n",
      "  batch 2 loss: 0.8129851818084717\n",
      "  batch 3 loss: 0.8986125588417053\n",
      "  batch 4 loss: 1.1058590412139893\n",
      "  batch 5 loss: 0.8326970338821411\n",
      "  batch 6 loss: 1.1297847032546997\n",
      "  batch 7 loss: 1.0092132091522217\n",
      "  batch 8 loss: 1.0415427684783936\n",
      "  batch 9 loss: 1.380457878112793\n",
      "  batch 10 loss: 0.9987528920173645\n",
      "  batch 11 loss: 0.9077931642532349\n",
      "  batch 12 loss: 1.1359310150146484\n",
      "  batch 13 loss: 1.331710696220398\n",
      "  batch 14 loss: 1.136082649230957\n",
      "  batch 15 loss: 0.7750574946403503\n",
      "LOSS train 0.7750574946403503 valid 1.325943112373352\n",
      "ACCURACY train: 0.625 valid 1.0\n",
      "EPOCH 90:\n",
      "  batch 1 loss: 1.018127202987671\n",
      "  batch 2 loss: 0.8864974975585938\n",
      "  batch 3 loss: 1.117514729499817\n",
      "  batch 4 loss: 1.2537916898727417\n",
      "  batch 5 loss: 0.9949067234992981\n",
      "  batch 6 loss: 1.001517415046692\n",
      "  batch 7 loss: 1.0511248111724854\n",
      "  batch 8 loss: 0.8208328485488892\n",
      "  batch 9 loss: 1.1040383577346802\n",
      "  batch 10 loss: 0.8980559706687927\n",
      "  batch 11 loss: 0.884641170501709\n",
      "  batch 12 loss: 1.0029138326644897\n",
      "  batch 13 loss: 1.3657761812210083\n",
      "  batch 14 loss: 1.210381269454956\n",
      "  batch 15 loss: 1.2929635047912598\n",
      "LOSS train 1.2929635047912598 valid 1.334141492843628\n",
      "ACCURACY train: 0.875 valid 0.0\n",
      "EPOCH 91:\n",
      "  batch 1 loss: 1.1358985900878906\n",
      "  batch 2 loss: 1.2457315921783447\n",
      "  batch 3 loss: 0.7586730122566223\n",
      "  batch 4 loss: 1.1146703958511353\n",
      "  batch 5 loss: 1.3618580102920532\n",
      "  batch 6 loss: 1.0125526189804077\n",
      "  batch 7 loss: 1.0549322366714478\n",
      "  batch 8 loss: 1.0765701532363892\n",
      "  batch 9 loss: 1.1403390169143677\n",
      "  batch 10 loss: 0.76918625831604\n",
      "  batch 11 loss: 0.944911539554596\n",
      "  batch 12 loss: 0.9941985607147217\n",
      "  batch 13 loss: 1.108681559562683\n",
      "  batch 14 loss: 1.3584115505218506\n",
      "  batch 15 loss: 0.7450053095817566\n",
      "LOSS train 0.7450053095817566 valid 1.3067095279693604\n",
      "ACCURACY train: nan valid 0.0\n",
      "EPOCH 92:\n",
      "  batch 1 loss: 0.9169515371322632\n",
      "  batch 2 loss: 1.1017907857894897\n",
      "  batch 3 loss: 0.8951902389526367\n",
      "  batch 4 loss: 0.9985129833221436\n",
      "  batch 5 loss: 0.9102151393890381\n",
      "  batch 6 loss: 1.148995041847229\n",
      "  batch 7 loss: 1.270249843597412\n",
      "  batch 8 loss: 0.9922395348548889\n",
      "  batch 9 loss: 1.2174357175827026\n",
      "  batch 10 loss: 1.1179980039596558\n",
      "  batch 11 loss: 0.9832800030708313\n",
      "  batch 12 loss: 0.8700912594795227\n",
      "  batch 13 loss: 1.2918978929519653\n",
      "  batch 14 loss: 0.9874914884567261\n",
      "  batch 15 loss: 0.8213790059089661\n",
      "LOSS train 0.8213790059089661 valid 1.3681721687316895\n",
      "ACCURACY train: 0.875 valid 0.0\n",
      "EPOCH 93:\n",
      "  batch 1 loss: 0.8346104025840759\n",
      "  batch 2 loss: 1.1702263355255127\n",
      "  batch 3 loss: 0.8649636507034302\n",
      "  batch 4 loss: 1.278939127922058\n",
      "  batch 5 loss: 1.1163814067840576\n",
      "  batch 6 loss: 1.2504557371139526\n",
      "  batch 7 loss: 1.24687659740448\n",
      "  batch 8 loss: 1.1423025131225586\n",
      "  batch 9 loss: 1.0457541942596436\n",
      "  batch 10 loss: 0.9406864643096924\n",
      "  batch 11 loss: 0.9083824157714844\n",
      "  batch 12 loss: 0.9951537847518921\n",
      "  batch 13 loss: 0.889906108379364\n",
      "  batch 14 loss: 0.9625334739685059\n",
      "  batch 15 loss: 1.3598085641860962\n",
      "LOSS train 1.3598085641860962 valid 1.307463526725769\n",
      "ACCURACY train: 0.5 valid 0.5\n",
      "EPOCH 94:\n",
      "  batch 1 loss: 0.8981334567070007\n",
      "  batch 2 loss: 1.0357913970947266\n",
      "  batch 3 loss: 1.0124430656433105\n",
      "  batch 4 loss: 1.0263354778289795\n",
      "  batch 5 loss: 0.9599746465682983\n",
      "  batch 6 loss: 1.3099939823150635\n",
      "  batch 7 loss: 1.0165091753005981\n",
      "  batch 8 loss: 1.0036301612854004\n",
      "  batch 9 loss: 1.026760220527649\n",
      "  batch 10 loss: 1.1904783248901367\n",
      "  batch 11 loss: 0.8725461959838867\n",
      "  batch 12 loss: 1.141724705696106\n",
      "  batch 13 loss: 1.0753718614578247\n",
      "  batch 14 loss: 1.1553308963775635\n",
      "  batch 15 loss: 0.9533320069313049\n",
      "LOSS train 0.9533320069313049 valid 1.3184467554092407\n",
      "ACCURACY train: 0.875 valid 1.0\n",
      "EPOCH 95:\n",
      "  batch 1 loss: 1.1365275382995605\n",
      "  batch 2 loss: 0.8795914053916931\n",
      "  batch 3 loss: 1.3417603969573975\n",
      "  batch 4 loss: 1.0284138917922974\n",
      "  batch 5 loss: 0.7595861554145813\n",
      "  batch 6 loss: 1.0698806047439575\n",
      "  batch 7 loss: 1.2334084510803223\n",
      "  batch 8 loss: 1.0334420204162598\n",
      "  batch 9 loss: 1.1508092880249023\n",
      "  batch 10 loss: 0.8777422308921814\n",
      "  batch 11 loss: 0.8739924430847168\n",
      "  batch 12 loss: 0.9275103807449341\n",
      "  batch 13 loss: 1.2057229280471802\n",
      "  batch 14 loss: 1.1130492687225342\n",
      "  batch 15 loss: 1.150309443473816\n",
      "LOSS train 1.150309443473816 valid 1.3117538690567017\n",
      "ACCURACY train: nan valid 0.5\n",
      "EPOCH 96:\n",
      "  batch 1 loss: 0.9005736708641052\n",
      "  batch 2 loss: 0.7867341637611389\n",
      "  batch 3 loss: 1.135912299156189\n",
      "  batch 4 loss: 1.2423449754714966\n",
      "  batch 5 loss: 1.0085055828094482\n",
      "  batch 6 loss: 1.2506201267242432\n",
      "  batch 7 loss: 1.1486682891845703\n",
      "  batch 8 loss: 0.9148221015930176\n",
      "  batch 9 loss: 1.0454509258270264\n",
      "  batch 10 loss: 1.1837486028671265\n",
      "  batch 11 loss: 1.1511509418487549\n",
      "  batch 12 loss: 0.9887458086013794\n",
      "  batch 13 loss: 1.0471806526184082\n",
      "  batch 14 loss: 0.8750807046890259\n",
      "  batch 15 loss: 0.8909702301025391\n",
      "LOSS train 0.8909702301025391 valid 1.321879267692566\n",
      "ACCURACY train: 0.875 valid 1.0\n",
      "EPOCH 97:\n",
      "  batch 1 loss: 0.9908335208892822\n",
      "  batch 2 loss: 0.8935897946357727\n",
      "  batch 3 loss: 1.1983906030654907\n",
      "  batch 4 loss: 0.8950809836387634\n",
      "  batch 5 loss: 0.897689163684845\n",
      "  batch 6 loss: 1.086986780166626\n",
      "  batch 7 loss: 0.9787542819976807\n",
      "  batch 8 loss: 1.006590485572815\n",
      "  batch 9 loss: 1.0134832859039307\n",
      "  batch 10 loss: 1.0100162029266357\n",
      "  batch 11 loss: 1.1128541231155396\n",
      "  batch 12 loss: 1.014750361442566\n",
      "  batch 13 loss: 1.25563645362854\n",
      "  batch 14 loss: 1.1547942161560059\n",
      "  batch 15 loss: 1.0804669857025146\n",
      "LOSS train 1.0804669857025146 valid 1.3179419040679932\n",
      "ACCURACY train: 0.75 valid 1.0\n",
      "EPOCH 98:\n",
      "  batch 1 loss: 0.986687421798706\n",
      "  batch 2 loss: 1.0261590480804443\n",
      "  batch 3 loss: 1.3681353330612183\n",
      "  batch 4 loss: 1.0187073945999146\n",
      "  batch 5 loss: 1.0666192770004272\n",
      "  batch 6 loss: 1.243186116218567\n",
      "  batch 7 loss: 1.1066819429397583\n",
      "  batch 8 loss: 0.7968842387199402\n",
      "  batch 9 loss: 0.9421711564064026\n",
      "  batch 10 loss: 1.1121667623519897\n",
      "  batch 11 loss: 0.9296127557754517\n",
      "  batch 12 loss: 0.9271304607391357\n",
      "  batch 13 loss: 0.8898684978485107\n",
      "  batch 14 loss: 1.1578177213668823\n",
      "  batch 15 loss: 0.8207514882087708\n",
      "LOSS train 0.8207514882087708 valid 1.3343796730041504\n",
      "ACCURACY train: 0.875 valid 1.0\n",
      "EPOCH 99:\n",
      "  batch 1 loss: 1.0110136270523071\n",
      "  batch 2 loss: 1.008226990699768\n",
      "  batch 3 loss: 0.8860244750976562\n",
      "  batch 4 loss: 1.0278037786483765\n",
      "  batch 5 loss: 1.1005439758300781\n",
      "  batch 6 loss: 0.8877721428871155\n",
      "  batch 7 loss: 1.0589640140533447\n",
      "  batch 8 loss: 1.01565420627594\n",
      "  batch 9 loss: 0.9002349376678467\n",
      "  batch 10 loss: 1.2681444883346558\n",
      "  batch 11 loss: 0.9945751428604126\n",
      "  batch 12 loss: 0.9932227730751038\n",
      "  batch 13 loss: 1.1470874547958374\n",
      "  batch 14 loss: 1.0387976169586182\n",
      "  batch 15 loss: 1.4007965326309204\n",
      "LOSS train 1.4007965326309204 valid 1.3354579210281372\n",
      "ACCURACY train: nan valid 0.0\n"
     ]
    }
   ],
   "source": [
    "train_test_loop(\"mlp_adam\", model, writer, train_dataloader, test_dataloader, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c7d518-4170-4937-b701-f38bf0c7e2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
